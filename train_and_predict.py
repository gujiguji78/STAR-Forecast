# train_and_predict.py - STAR-Forecast å®Œæ•´ç«¯åˆ°ç«¯è®­ç»ƒå’Œé¢„æµ‹

import sys
import os
import torch
import numpy as np
import pandas as pd
from pathlib import Path
import json
from datetime import datetime
from typing import Dict, Any, List, Tuple
import warnings

warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

print("ğŸš€ STAR-Forecast å®Œæ•´ç«¯åˆ°ç«¯è®­ç»ƒå’Œé¢„æµ‹")
print("=" * 80)
print("ğŸŒŸ æ•´åˆ: ISTR(ç¥ç») + AutoGen(ç¬¦å·) + Agent Lightning(å¼ºåŒ–) + è®°å¿†é“¶è¡Œ")
print("=" * 80)


class STARForecastTrainer:
    """STAR-Forecast å®Œæ•´è®­ç»ƒå’Œé¢„æµ‹å™¨"""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self.get_default_config()
        self.device = self.setup_device()
        self.initialize_components()
        self.setup_directories()

    def get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # æ•°æ®é…ç½®
            'dataset': 'ETTh1',
            'data_path': 'data/raw/ETTh1.csv',
            'seq_len': 96,
            'pred_len': 24,
            'batch_size': 32,
            'train_ratio': 0.7,
            'val_ratio': 0.15,
            'test_ratio': 0.15,

            # ISTRæ¨¡å‹é…ç½®
            'input_dim': 7,
            'hidden_dim': 64,
            'num_blocks': 3,
            'trainable_ratio': 0.01,
            'laplacian_weight': 0.01,

            # è®­ç»ƒé…ç½®
            'epochs': 50,
            'learning_rate': 1e-3,
            'patience': 10,

            # AutoGené…ç½®
            'use_autogen': True,
            'agent_count': 3,
            'debate_rounds': 2,

            # Agent Lightningé…ç½®
            'use_lightning': True,
            'reinforcement_epochs': 5,

            # ç³»ç»Ÿé…ç½®
            'save_checkpoints': True,
            'checkpoint_freq': 5,
            'log_interval': 10,
            'seed': 42
        }

    def setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        else:
            device = torch.device("cpu")
            print("â„¹ï¸  ä½¿ç”¨CPU")
        return device

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            "results",
            "results/checkpoints",
            "results/predictions",
            "results/logs",
            "results/memory"
        ]

        for dir_path in directories:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

        print("ğŸ“ ç›®å½•ç»“æ„å·²å‡†å¤‡")

    def initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        print("\nğŸ”§ åˆå§‹åŒ–æ¡†æ¶ç»„ä»¶...")

        # 1. åˆå§‹åŒ–ISTRæ¨¡å‹
        print("1ï¸âƒ£ åˆå§‹åŒ–ISTRç¥ç»ç½‘ç»œ...")
        try:
            from models.istr import ISTRPredictor

            self.istr_model = ISTRPredictor(
                input_dim=self.config['input_dim'],
                hidden_dim=self.config['hidden_dim'],
                pred_len=self.config['pred_len'],
                num_blocks=self.config['num_blocks'],
                trainable_ratio=self.config['trainable_ratio'],
                laplacian_weight=self.config['laplacian_weight']
            ).to(self.device)

            print(f"   âœ… ISTRæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            print(f"      æ€»å‚æ•°: {self.istr_model.total_params:,}")
            print(f"      å¯è®­ç»ƒå‚æ•°: {self.istr_model.trainable_params:,}")

        except Exception as e:
            print(f"   âŒ ISTRæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

        # 2. åˆå§‹åŒ–AutoGenç³»ç»Ÿ
        if self.config['use_autogen']:
            print("2ï¸âƒ£ åˆå§‹åŒ–AutoGenå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ...")
            try:
                from agents.autogen_system import AutoGenDebateSystem
                from dataclasses import dataclass

                @dataclass
                class DebateConfig:
                    agent_count: int = self.config['agent_count']
                    debate_rounds: int = self.config['debate_rounds']
                    temperature: float = 0.7
                    use_memory: bool = True

                self.autogen_system = AutoGenDebateSystem(config=DebateConfig())
                print(f"   âœ… AutoGenç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

            except Exception as e:
                print(f"   âš ï¸  AutoGenç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.config['use_autogen'] = False

        # 3. åˆå§‹åŒ–è®°å¿†é“¶è¡Œ
        print("3ï¸âƒ£ åˆå§‹åŒ–è®°å¿†é“¶è¡Œ...")
        try:
            from agents.memory_bank import MemoryBank

            self.memory_bank = MemoryBank(
                persistence_path="results/memory/memory_bank.json",
                max_memory_items=1000
            )
            print(f"   âœ… è®°å¿†é“¶è¡Œåˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            print(f"   âŒ è®°å¿†é“¶è¡Œåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

        # 4. åˆå§‹åŒ–Agent Lightning
        if self.config['use_lightning']:
            print("4ï¸âƒ£ åˆå§‹åŒ–Agent Lightningå¼ºåŒ–å­¦ä¹ ...")
            try:
                from training.lightning_client import LightningTrainer

                self.lightning_trainer = LightningTrainer(
                    model=self.istr_model,
                    learning_rate=self.config['learning_rate'] * 0.1,  # å¼ºåŒ–å­¦ä¹ ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
                    batch_size=self.config['batch_size'],
                    enable_reinforcement=True
                )
                print(f"   âœ… Agent Lightningåˆå§‹åŒ–æˆåŠŸ")

            except Exception as e:
                print(f"   âš ï¸  Agent Lightningåˆå§‹åŒ–å¤±è´¥: {e}")
                self.config['use_lightning'] = False

        print("\nâœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ!")

    def load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("\nğŸ“Š åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")

        # åŠ è½½CSVæ•°æ®
        data_path = Path(self.config['data_path'])
        if not data_path.exists():
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            # å°è¯•ä»ETTh1.csvåŠ è½½
            data_path = Path("data/raw/ETTh1.csv")
            if not data_path.exists():
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

        df = pd.read_csv(data_path)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")

        # æå–ç‰¹å¾
        feature_columns = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
        if all(col in df.columns for col in feature_columns):
            features = df[feature_columns].values.astype(np.float32)
        else:
            # ä½¿ç”¨å‰7åˆ—ä½œä¸ºç‰¹å¾
            features = df.iloc[:, 1:8].values.astype(np.float32)

        print(f"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {features.shape}")

        # æ•°æ®æ ‡å‡†åŒ–
        self.data_mean = features.mean(axis=0)
        self.data_std = features.std(axis=0) + 1e-8
        features_normalized = (features - self.data_mean) / self.data_std

        # åˆ›å»ºåºåˆ—
        seq_len = self.config['seq_len']
        pred_len = self.config['pred_len']

        X, y = self.create_sequences(features_normalized, seq_len, pred_len)
        print(f"   åˆ›å»ºåºåˆ—å®Œæˆ: X={X.shape}, y={y.shape}")

        # åˆ’åˆ†æ•°æ®é›†
        n_samples = len(X)
        train_size = int(n_samples * self.config['train_ratio'])
        val_size = int(n_samples * self.config['val_ratio'])

        indices = np.arange(n_samples)
        np.random.shuffle(indices)

        train_idx = indices[:train_size]
        val_idx = indices[train_size:train_size + val_size]
        test_idx = indices[train_size + val_size:]

        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        X_test, y_test = X[test_idx], y[test_idx]

        print(f"   æ•°æ®é›†åˆ’åˆ†:")
        print(f"     è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"     éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"     æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_t = torch.FloatTensor(X_train).to(self.device)
        y_train_t = torch.FloatTensor(y_train).unsqueeze(-1).to(self.device)  # [batch, pred_len, 1]
        X_val_t = torch.FloatTensor(X_val).to(self.device)
        y_val_t = torch.FloatTensor(y_val).unsqueeze(-1).to(self.device)
        X_test_t = torch.FloatTensor(X_test).to(self.device)
        y_test_t = torch.FloatTensor(y_test).unsqueeze(-1).to(self.device)

        return {
            'train': (X_train_t, y_train_t),
            'val': (X_val_t, y_val_t),
            'test': (X_test_t, y_test_t),
            'mean': self.data_mean,
            'std': self.data_std,
            'original': features
        }

    def create_sequences(self, data: np.ndarray, seq_len: int, pred_len: int) -> Tuple[np.ndarray, np.ndarray]:
        """åˆ›å»ºæ—¶é—´åºåˆ—æ ·æœ¬"""
        n_samples = len(data) - seq_len - pred_len
        X, y = [], []

        for i in range(n_samples):
            X.append(data[i:i + seq_len])
            y.append(data[i + seq_len:i + seq_len + pred_len, -1])  # åªé¢„æµ‹OTåˆ—

        return np.array(X), np.array(y)

    def train_istr_model(self, train_data, val_data):
        """è®­ç»ƒISTRæ¨¡å‹"""
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒISTRæ¨¡å‹...")

        X_train, y_train = train_data
        X_val, y_val = val_data

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        from torch.utils.data import TensorDataset, DataLoader

        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)

        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False
        )

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = torch.nn.MSELoss()
        optimizer = torch.optim.Adam(
            filter(lambda p: p.requires_grad, self.istr_model.parameters()),
            lr=self.config['learning_rate']
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )

        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        patience_counter = 0

        train_losses = []
        val_losses = []

        for epoch in range(self.config['epochs']):
            # è®­ç»ƒé˜¶æ®µ
            self.istr_model.train()
            train_loss = 0.0

            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­
                predictions = self.istr_model(batch_X)

                # è®¡ç®—æŸå¤±
                mse_loss = criterion(predictions, batch_y)
                reg_loss = self.istr_model.compute_regularization_loss(predictions)
                loss = mse_loss + reg_loss

                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.istr_model.parameters(), max_norm=1.0)
                optimizer.step()

                train_loss += loss.item()

            avg_train_loss = train_loss / len(train_loader)
            train_losses.append(avg_train_loss)

            # éªŒè¯é˜¶æ®µ
            self.istr_model.eval()
            val_loss = 0.0

            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    predictions = self.istr_model(batch_X)
                    mse_loss = criterion(predictions, batch_y)
                    reg_loss = self.istr_model.compute_regularization_loss(predictions)
                    loss = mse_loss + reg_loss
                    val_loss += loss.item()

            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_val_loss)

            # æ‰“å°è¿›åº¦
            if (epoch + 1) % self.config['log_interval'] == 0:
                print(f"   Epoch {epoch + 1}/{self.config['epochs']}: "
                      f"Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0

                if self.config['save_checkpoints']:
                    self.save_checkpoint(epoch, avg_val_loss, "best")
                    print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {avg_val_loss:.6f})")
            else:
                patience_counter += 1

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if self.config['save_checkpoints'] and (epoch + 1) % self.config['checkpoint_freq'] == 0:
                self.save_checkpoint(epoch, avg_val_loss, f"epoch_{epoch + 1}")

            # æ—©åœ
            if patience_counter >= self.config['patience']:
                print(f"   â¹ï¸  æ—©åœè§¦å‘äºç¬¬ {epoch + 1} è½®")
                break

        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history(train_losses, val_losses)

        print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")

        return best_val_loss

    def save_checkpoint(self, epoch: int, val_loss: float, name: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path("results/checkpoints")
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        checkpoint_path = checkpoint_dir / f"istr_{name}.pt"

        torch.save({
            'epoch': epoch,
            'model_state_dict': self.istr_model.state_dict(),
            'val_loss': val_loss,
            'config': self.config,
            'data_stats': {
                'mean': self.data_mean.tolist(),
                'std': self.data_std.tolist()
            }
        }, checkpoint_path)

    def save_training_history(self, train_losses: List[float], val_losses: List[float]):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_dir = Path("results/logs")
        history_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        history_path = history_dir / f"training_history_{timestamp}.json"

        history = {
            'timestamp': timestamp,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'config': self.config
        }

        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

    def run_autogen_debate(self, historical_data: np.ndarray, predictions: np.ndarray) -> Dict[str, Any]:
        """è¿è¡ŒAutoGenè¾©è®º"""
        if not self.config['use_autogen']:
            return None

        print("\nğŸ¤– å¯åŠ¨AutoGenå¤šæ™ºèƒ½ä½“è¾©è®º...")

        try:
            # å‡†å¤‡è¾©è®ºä¸Šä¸‹æ–‡
            context = {
                "data_description": f"{self.config['dataset']} æ—¶é—´åºåˆ—æ•°æ®",
                "historical_stats": {
                    "mean": float(historical_data.mean()),
                    "std": float(historical_data.std()),
                    "min": float(historical_data.min()),
                    "max": float(historical_data.max())
                },
                "prediction_stats": {
                    "mean": float(predictions.mean()),
                    "std": float(predictions.std()),
                    "min": float(predictions.min()),
                    "max": float(predictions.max())
                },
                "model_info": {
                    "type": "ISTR",
                    "trainable_params": self.istr_model.trainable_params,
                    "trainable_ratio": self.config['trainable_ratio']
                }
            }

            # å¯åŠ¨è¾©è®º
            debate_result = self.autogen_system.start_debate(
                topic="æ—¶é—´åºåˆ—é¢„æµ‹åˆ†æä¸ä¼˜åŒ–",
                context=context,
                question="å½“å‰çš„é¢„æµ‹ç»“æœæ˜¯å¦åˆç†ï¼Ÿæœ‰å“ªäº›æ”¹è¿›å»ºè®®ï¼Ÿ"
            )

            print(f"   âœ… è¾©è®ºå®Œæˆ")
            print(f"     å…±è¯†: {debate_result.consensus[:100]}...")
            print(f"     å»ºè®®æ•°é‡: {len(debate_result.recommendations)}")

            # å­˜å‚¨åˆ°è®°å¿†é“¶è¡Œ
            if hasattr(self, 'memory_bank'):
                self.memory_bank.store_experience({
                    "type": "debate",
                    "timestamp": datetime.now().isoformat(),
                    "topic": "é¢„æµ‹ä¼˜åŒ–",
                    "consensus": debate_result.consensus,
                    "recommendations": debate_result.recommendations,
                    "context": context
                })

            return {
                'consensus': debate_result.consensus,
                'recommendations': debate_result.recommendations,
                'insights': debate_result.get_consensus_insights() if hasattr(debate_result,
                                                                              'get_consensus_insights') else {}
            }

        except Exception as e:
            print(f"   âš ï¸  è¾©è®ºå¤±è´¥: {e}")
            return None

    def apply_debate_insights(self, predictions: np.ndarray, debate_result: Dict[str, Any]) -> np.ndarray:
        """åº”ç”¨è¾©è®ºè§è§£ä¼˜åŒ–é¢„æµ‹"""
        if not debate_result or 'insights' not in debate_result:
            return predictions

        insights = debate_result['insights']
        optimized = predictions.copy()

        print("\nğŸ”§ åº”ç”¨è¾©è®ºè§è§£ä¼˜åŒ–é¢„æµ‹...")

        # åº”ç”¨è¶‹åŠ¿è°ƒæ•´
        if "adjust_trend" in insights:
            adjustment = insights["adjust_trend"]
            optimized = optimized * (1 + adjustment)
            print(f"   åº”ç”¨è¶‹åŠ¿è°ƒæ•´: {adjustment * 100:.1f}%")

        # åº”ç”¨å¹³æ»‘
        if "smooth_variance" in insights and insights["smooth_variance"]:
            try:
                from scipy.ndimage import gaussian_filter1d
                optimized = gaussian_filter1d(optimized, sigma=1)
                print(f"   åº”ç”¨æ–¹å·®å¹³æ»‘")
            except:
                pass

        return optimized

    def run_reinforcement_learning(self, experiences: List[Dict[str, Any]]) -> float:
        """è¿è¡Œå¼ºåŒ–å­¦ä¹ """
        if not self.config['use_lightning'] or not experiences:
            return 0.0

        print("\nâš¡ å¯åŠ¨Agent Lightningå¼ºåŒ–å­¦ä¹ ...")

        try:
            if len(experiences) < 10:
                print(f"   âš ï¸  ç»éªŒæ•°æ®ä¸è¶³ ({len(experiences)}ä¸ª)ï¼Œè·³è¿‡å¼ºåŒ–å­¦ä¹ ")
                return 0.0

            improvement = self.lightning_trainer.reinforce(
                experiences=experiences,
                target_metric="mse",
                n_epochs=self.config['reinforcement_epochs']
            )

            print(f"   âœ… å¼ºåŒ–å­¦ä¹ å®Œæˆï¼Œæ”¹è¿›: {improvement:.6f}")

            # å­˜å‚¨åˆ°è®°å¿†é“¶è¡Œ
            if hasattr(self, 'memory_bank'):
                self.memory_bank.store_experience({
                    "type": "reinforcement",
                    "timestamp": datetime.now().isoformat(),
                    "improvement": improvement,
                    "experience_count": len(experiences),
                    "epochs": self.config['reinforcement_epochs']
                })

            return improvement

        except Exception as e:
            print(f"   âš ï¸  å¼ºåŒ–å­¦ä¹ å¤±è´¥: {e}")
            return 0.0

    def evaluate_predictions(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        mse = np.mean((predictions - targets) ** 2)
        mae = np.mean(np.abs(predictions - targets))
        rmse = np.sqrt(mse)

        return {
            'mse': float(mse),
            'mae': float(mae),
            'rmse': float(rmse)
        }

    def run_prediction_pipeline(self, test_data, n_samples: int = 5):
        """è¿è¡Œå®Œæ•´é¢„æµ‹ç®¡é“"""
        print("\n" + "=" * 80)
        print("ğŸ”® è¿è¡Œå®Œæ•´é¢„æµ‹ç®¡é“")
        print("=" * 80)

        X_test, y_test = test_data

        # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡
        n_samples = min(n_samples, len(X_test))
        print(f"   å¤„ç† {n_samples} ä¸ªæµ‹è¯•æ ·æœ¬")

        all_results = []
        valuable_experiences = []

        for i in range(n_samples):
            print(f"\nğŸ“Š æ ·æœ¬ {i + 1}/{n_samples}")
            print("-" * 40)

            # 1. ISTRåŸºç¡€é¢„æµ‹
            print("1ï¸âƒ£ ISTRåŸºç¡€é¢„æµ‹...")
            sample_input = X_test[i:i + 1].cpu().numpy()  # [1, seq_len, features]
            sample_target = y_test[i:i + 1].cpu().numpy().squeeze()  # [pred_len]

            # åæ ‡å‡†åŒ–è¾“å…¥ç”¨äºè¾©è®ºä¸Šä¸‹æ–‡
            historical_data_denorm = sample_input.squeeze() * self.data_std + self.data_mean

            # æ¨¡å‹é¢„æµ‹
            self.istr_model.eval()
            with torch.no_grad():
                input_tensor = torch.FloatTensor(sample_input).to(self.device)
                prediction_tensor = self.istr_model(input_tensor)
                base_prediction = prediction_tensor.cpu().numpy().squeeze()  # [pred_len]

            # åæ ‡å‡†åŒ–é¢„æµ‹å’Œç›®æ ‡
            base_pred_denorm = base_prediction * self.data_std[-1] + self.data_mean[-1]
            target_denorm = sample_target * self.data_std[-1] + self.data_mean[-1]

            # è¯„ä¼°åŸºç¡€é¢„æµ‹
            base_metrics = self.evaluate_predictions(base_pred_denorm, target_denorm)
            print(f"   åŸºç¡€é¢„æµ‹MSE: {base_metrics['mse']:.6f}")

            # 2. AutoGenè¾©è®ºä¼˜åŒ–
            print("2ï¸âƒ£ AutoGenè¾©è®ºä¼˜åŒ–...")
            debate_result = self.run_autogen_debate(
                historical_data_denorm[:, -1],  # åªä½¿ç”¨OTåˆ—çš„å†å²æ•°æ®
                base_pred_denorm
            )

            # 3. åº”ç”¨è¾©è®ºè§è§£
            print("3ï¸âƒ£ åº”ç”¨è¾©è®ºè§è§£...")
            optimized_prediction = self.apply_debate_insights(base_pred_denorm, debate_result)

            # è¯„ä¼°ä¼˜åŒ–åé¢„æµ‹
            optimized_metrics = self.evaluate_predictions(optimized_prediction, target_denorm)
            improvement = base_metrics['mse'] - optimized_metrics['mse']

            print(f"   ä¼˜åŒ–åé¢„æµ‹MSE: {optimized_metrics['mse']:.6f}")
            print(f"   æ”¹è¿›: {improvement:.6f} ({improvement / base_metrics['mse'] * 100:.1f}%)")

            # 4. æ”¶é›†ç»éªŒç”¨äºå¼ºåŒ–å­¦ä¹ 
            if improvement > 0:  # åªæœ‰æ”¹è¿›çš„ç»éªŒæ‰æ”¶é›†
                experience = {
                    "state": sample_input.squeeze(),  # [seq_len, features]
                    "action": base_prediction,  # åŸºç¡€é¢„æµ‹
                    "reward": -optimized_metrics['mse'],  # è´ŸMSEä½œä¸ºå¥–åŠ±
                    "next_state": sample_input.squeeze(),  # ç®€åŒ–ï¼Œä½¿ç”¨ç›¸åŒçŠ¶æ€
                    "improvement": improvement
                }
                valuable_experiences.append(experience)

            # 5. å­˜å‚¨ç»“æœ
            sample_result = {
                "sample_id": i,
                "base_prediction": base_pred_denorm.tolist(),
                "optimized_prediction": optimized_prediction.tolist(),
                "true_values": target_denorm.tolist(),
                "base_metrics": base_metrics,
                "optimized_metrics": optimized_metrics,
                "improvement": float(improvement),
                "debate_consensus": debate_result['consensus'] if debate_result else None,
                "recommendations": debate_result['recommendations'] if debate_result else []
            }

            all_results.append(sample_result)

            # å­˜å‚¨åˆ°è®°å¿†é“¶è¡Œ
            if hasattr(self, 'memory_bank'):
                self.memory_bank.store_experience({
                    "type": "prediction",
                    "timestamp": datetime.now().isoformat(),
                    "sample_id": i,
                    "base_mse": base_metrics['mse'],
                    "optimized_mse": optimized_metrics['mse'],
                    "improvement": improvement,
                    "debate_used": debate_result is not None
                })

        # 6. Agent Lightningå¼ºåŒ–å­¦ä¹ 
        print(f"\n" + "=" * 80)
        print("âš¡ é˜¶æ®µ4: Agent Lightningå¼ºåŒ–å­¦ä¹ ")
        print("=" * 80)

        rl_improvement = self.run_reinforcement_learning(valuable_experiences)

        # 7. æ±‡æ€»ç»“æœ
        print(f"\n" + "=" * 80)
        print("ğŸ“Š æœ€ç»ˆç»“æœæ±‡æ€»")
        print("=" * 80)

        if all_results:
            base_mses = [r["base_metrics"]["mse"] for r in all_results]
            optimized_mses = [r["optimized_metrics"]["mse"] for r in all_results]

            avg_base_mse = np.mean(base_mses)
            avg_optimized_mse = np.mean(optimized_mses)
            avg_improvement = avg_base_mse - avg_optimized_mse

            print(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
            print(f"   å¹³å‡åŸºç¡€MSE: {avg_base_mse:.6f}")
            print(f"   å¹³å‡ä¼˜åŒ–MSE: {avg_optimized_mse:.6f}")
            print(f"   å¹³å‡æ”¹è¿›: {avg_improvement:.6f} ({avg_improvement / avg_base_mse * 100:.1f}%)")
            print(f"   å¼ºåŒ–å­¦ä¹ æ”¹è¿›: {rl_improvement:.6f}")

            print(f"\nğŸ”§ ç»„ä»¶ä½¿ç”¨æƒ…å†µ:")
            print(f"   ISTRæ¨¡å‹: âœ… å·²ä½¿ç”¨")
            print(f"   AutoGenç³»ç»Ÿ: {'âœ…' if self.config['use_autogen'] else 'âŒ'}")
            print(f"   Agent Lightning: {'âœ…' if self.config['use_lightning'] else 'âŒ'}")
            print(f"   è®°å¿†é“¶è¡Œ: âœ… å·²ä½¿ç”¨ ({len(self.memory_bank) if hasattr(self, 'memory_bank') else 0} æ¡è®°å¿†)")

            # ä¿å­˜é¢„æµ‹ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = Path("results/predictions")
            results_file = results_dir / f"prediction_results_{timestamp}.json"

            final_results = {
                "timestamp": timestamp,
                "dataset": self.config['dataset'],
                "samples_processed": n_samples,
                "average_base_mse": float(avg_base_mse),
                "average_optimized_mse": float(avg_optimized_mse),
                "average_improvement": float(avg_improvement),
                "rl_improvement": float(rl_improvement),
                "config": self.config,
                "sample_results": all_results
            }

            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {results_file}")

            # ä¿å­˜è®°å¿†é“¶è¡Œ
            if hasattr(self, 'memory_bank'):
                memory_file = results_dir / f"memory_bank_{timestamp}.json"
                self.memory_bank.save(memory_file)
                print(f"ğŸ’¾ è®°å¿†é“¶è¡Œä¿å­˜åˆ°: {memory_file}")

            return final_results

        return None

    def run(self, train_model: bool = True, test_samples: int = 5):
        """è¿è¡Œå®Œæ•´è®­ç»ƒå’Œé¢„æµ‹æµç¨‹"""
        try:
            # 1. åŠ è½½æ•°æ®
            print("\n" + "=" * 80)
            print("ğŸ“Š æ­¥éª¤1: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
            print("=" * 80)
            data_dict = self.load_and_preprocess_data()

            # 2. è®­ç»ƒæ¨¡å‹
            if train_model:
                print("\n" + "=" * 80)
                print("ğŸ¯ æ­¥éª¤2: è®­ç»ƒISTRæ¨¡å‹")
                print("=" * 80)
                self.train_istr_model(data_dict['train'], data_dict['val'])

            # 3. åŠ è½½æœ€ä½³æ¨¡å‹
            print("\n" + "=" * 80)
            print("ğŸ“‚ æ­¥éª¤3: åŠ è½½æœ€ä½³æ¨¡å‹")
            print("=" * 80)
            self.load_best_checkpoint()

            # 4. è¿è¡Œé¢„æµ‹ç®¡é“
            print("\n" + "=" * 80)
            print("ğŸ”® æ­¥éª¤4: å®Œæ•´é¢„æµ‹ç®¡é“")
            print("=" * 80)
            results = self.run_prediction_pipeline(data_dict['test'], test_samples)

            print("\n" + "=" * 80)
            print("ğŸ‰ STAR-Forecast å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸ!")
            print("=" * 80)

            return results

        except Exception as e:
            print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def load_best_checkpoint(self):
        """åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path("results/checkpoints")
        if not checkpoint_dir.exists():
            print("   â„¹ï¸  æ²¡æœ‰æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨åˆå§‹æ¨¡å‹")
            return

        checkpoint_files = list(checkpoint_dir.glob("istr_best*.pt"))
        if not checkpoint_files:
            checkpoint_files = list(checkpoint_dir.glob("*.pt"))

        if checkpoint_files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            checkpoint_path = checkpoint_files[0]

            print(f"   ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path.name}")

            try:
                checkpoint = torch.load(checkpoint_path, map_location=self.device)
                self.istr_model.load_state_dict(checkpoint['model_state_dict'])

                # æ›´æ–°æ•°æ®ç»Ÿè®¡
                if 'data_stats' in checkpoint:
                    self.data_mean = np.array(checkpoint['data_stats']['mean'])
                    self.data_std = np.array(checkpoint['data_stats']['std'])

                print(f"   âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ (Val Loss: {checkpoint.get('val_loss', 'N/A'):.6f})")

            except Exception as e:
                print(f"   âš ï¸  æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
        else:
            print("   â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='STAR-Forecast å®Œæ•´è®­ç»ƒå’Œé¢„æµ‹')
    parser.add_argument('--mode', type=str, default='full',
                        choices=['train', 'predict', 'full'],
                        help='è¿è¡Œæ¨¡å¼: train=ä»…è®­ç»ƒ, predict=ä»…é¢„æµ‹, full=å®Œæ•´æµç¨‹')
    parser.add_argument('--dataset', type=str, default='ETTh1',
                        help='æ•°æ®é›†åç§°')
    parser.add_argument('--epochs', type=int, default=50,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--samples', type=int, default=5,
                        help='æµ‹è¯•æ ·æœ¬æ•°é‡')
    parser.add_argument('--no-autogen', action='store_true',
                        help='ç¦ç”¨AutoGenç³»ç»Ÿ')
    parser.add_argument('--no-lightning', action='store_true',
                        help='ç¦ç”¨Agent Lightning')
    parser.add_argument('--config', type=str, default=None,
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    print(f"ğŸš€ STAR-Forecast å¯åŠ¨")
    print(f"   æ¨¡å¼: {args.mode}")
    print(f"   æ•°æ®é›†: {args.dataset}")
    print(f"   æ ·æœ¬æ•°: {args.samples}")
    print(f"   AutoGen: {'ç¦ç”¨' if args.no_autogen else 'å¯ç”¨'}")
    print(f"   Agent Lightning: {'ç¦ç”¨' if args.no_lightning else 'å¯ç”¨'}")

    # åˆ›å»ºé…ç½®
    config = {
        'dataset': args.dataset,
        'data_path': f"data/raw/{args.dataset}.csv",
        'epochs': args.epochs,
        'use_autogen': not args.no_autogen,
        'use_lightning': not args.no_lightning
    }

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = STARForecastTrainer(config)

    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == 'train':
        # ä»…è®­ç»ƒ
        data_dict = trainer.load_and_preprocess_data()
        trainer.train_istr_model(data_dict['train'], data_dict['val'])

    elif args.mode == 'predict':
        # ä»…é¢„æµ‹ï¼ˆéœ€è¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
        data_dict = trainer.load_and_preprocess_data()
        trainer.load_best_checkpoint()
        trainer.run_prediction_pipeline(data_dict['test'], args.samples)

    else:  # full
        # å®Œæ•´æµç¨‹
        trainer.run(train_model=True, test_samples=args.samples)

    print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ!")


if __name__ == "__main__":
    main()