"""
è¾©è®ºå¼æ™ºèƒ½ä½“ç³»ç»Ÿ - æ”¯æŒå¤šæ™ºèƒ½ä½“è¾©è®ºã€æŠ•ç¥¨å’Œå…±è¯†å½¢æˆ
æ¯”æ™®é€šAutoGenæ›´åŠ å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´å’Œæ·±åº¦è¾©è®º
"""
import asyncio
import random
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
from datetime import datetime
import uuid
import logging
from collections import defaultdict

from .autogen_system import AutoGenMultiAgentSystem, ConversationResult


class DebatePhase(Enum):
    """è¾©è®ºé˜¶æ®µ"""
    OPENING = "opening"  # å¼€ç¯‡é™ˆè¿°
    REBUTTAL = "rebuttal"  # åé©³é˜¶æ®µ
    CROSS_EXAMINATION = "cross"  # äº¤å‰è´¨è¯¢
    CLOSING = "closing"  # ç»“æ¡ˆé™ˆè¯
    VOTING = "voting"  # æŠ•ç¥¨é˜¶æ®µ


class DebateRole(Enum):
    """è¾©è®ºè§’è‰²"""
    PROPOSITION = "proposition"  # æ­£æ–¹
    OPPOSITION = "opposition"  # åæ–¹
    MODERATOR = "moderator"  # ä¸»æŒäºº
    JUDGE = "judge"  # è¯„å§”


@dataclass
class DebateArgument:
    """è¾©è®ºè®ºç‚¹"""
    argument_id: str
    speaker: str
    role: DebateRole
    phase: DebatePhase
    content: str
    claims: List[str] = field(default_factory=list)  # ä¸»å¼ åˆ—è¡¨
    evidence: Dict[str, Any] = field(default_factory=dict)  # è¯æ®
    fallacies: List[str] = field(default_factory=list)  # é€»è¾‘è°¬è¯¯æ ‡è®°
    strength: float = 0.0  # è®ºç‚¹å¼ºåº¦
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class DebateRound:
    """è¾©è®ºè½®æ¬¡"""
    round_id: str
    phase: DebatePhase
    arguments: List[DebateArgument] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    summary: Optional[str] = None


@dataclass
class DebateResult:
    """è¾©è®ºç»“æœ"""
    debate_id: str
    topic: str
    rounds: List[DebateRound]
    final_vote: Dict[str, int] = field(default_factory=dict)  # æ”¯æŒæ–¹ -> ç¥¨æ•°
    consensus: Optional[Dict[str, Any]] = None
    winner: Optional[str] = None
    reasoning: str = ""
    metrics: Dict[str, Any] = field(default_factory=dict)


class LogicalFallacyDetector:
    """é€»è¾‘è°¬è¯¯æ£€æµ‹å™¨"""

    FALLACIES = {
        'ad_hominem': ['äººèº«æ”»å‡»', 'æ”»å‡»äººæ ¼', 'è´¬ä½å¯¹æ–¹'],
        'straw_man': ['ç¨»è‰äºº', 'æ›²è§£è®ºç‚¹', 'æ­ªæ›²ç«‹åœº'],
        'false_cause': ['è™šå‡å› æœ', 'ç›¸å…³å½“å› æœ', 'å› æœé¢ å€’'],
        'slippery_slope': ['æ»‘å¡è°¬è¯¯', 'æç«¯æ¨è®º', 'è¿é”ååº”'],
        'appeal_to_emotion': ['è¯‰è¯¸æƒ…æ„Ÿ', 'æƒ…ç»ªåŒ–è®ºè¯', 'ç…½æƒ…'],
        'black_or_white': ['éé»‘å³ç™½', 'äºŒå…ƒå¯¹ç«‹', 'æ’é™¤ä¸­é—´'],
        'bandwagon': ['ä»ä¼—è°¬è¯¯', 'å¤§å®¶éƒ½è¿™æ ·', 'æµè¡Œå³æ­£ç¡®'],
        'appeal_to_authority': ['è¯‰è¯¸æƒå¨', 'ä¸“å®¶è¯´', 'æƒå¨è®ºæ–­'],
        'hasty_generalization': ['è‰ç‡æ¦‚æ‹¬', 'ä»¥åæ¦‚å…¨', 'æ ·æœ¬ä¸è¶³'],
        'red_herring': ['è½¬ç§»è¯é¢˜', 'å·æ¢æ¦‚å¿µ', 'ç¦»é¢˜ä¸‡é‡Œ']
    }

    @classmethod
    def detect(cls, text: str) -> List[str]:
        """æ£€æµ‹æ–‡æœ¬ä¸­çš„é€»è¾‘è°¬è¯¯"""
        detected = []
        text_lower = text.lower()

        for fallacy, keywords in cls.FALLACIES.items():
            for keyword in keywords:
                if keyword in text_lower:
                    detected.append(fallacy)
                    break

        return detected


class ArgumentStrengthAnalyzer:
    """è®ºç‚¹å¼ºåº¦åˆ†æå™¨"""

    @classmethod
    def analyze(cls, argument: str, evidence: Dict[str, Any] = None) -> float:
        """åˆ†æè®ºç‚¹å¼ºåº¦"""
        strength = 0.0

        # 1. é•¿åº¦åˆ†æï¼ˆé€‚ä¸­çš„é•¿åº¦æ›´å¥½ï¼‰
        length_score = min(len(argument) / 1000, 1.0)  # ä¸è¶…è¿‡1000å­—
        strength += length_score * 0.2

        # 2. è¯æ®æ”¯æŒ
        if evidence:
            evidence_score = cls._evaluate_evidence(evidence)
            strength += evidence_score * 0.3

        # 3. é€»è¾‘ç»“æ„
        logic_score = cls._evaluate_logic(argument)
        strength += logic_score * 0.3

        # 4. æ¸…æ™°åº¦
        clarity_score = cls._evaluate_clarity(argument)
        strength += clarity_score * 0.2

        # 5. å‡å»è°¬è¯¯æƒ©ç½š
        fallacies = LogicalFallacyDetector.detect(argument)
        if fallacies:
            strength -= len(fallacies) * 0.1

        return max(0.0, min(1.0, strength))

    @classmethod
    def _evaluate_evidence(cls, evidence: Dict[str, Any]) -> float:
        """è¯„ä¼°è¯æ®è´¨é‡"""
        score = 0.0

        if 'data' in evidence and isinstance(evidence['data'], (list, dict)):
            score += 0.3

        if 'sources' in evidence and evidence['sources']:
            score += 0.3

        if 'statistics' in evidence:
            score += 0.2

        if 'examples' in evidence:
            score += 0.2

        return min(1.0, score)

    @classmethod
    def _evaluate_logic(cls, argument: str) -> float:
        """è¯„ä¼°é€»è¾‘ç»“æ„"""
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯
        connectors = ['å› æ­¤', 'æ‰€ä»¥', 'å› ä¸º', 'ç”±äº', 'å¯¼è‡´', 'ç»“æœ',
                      'ç”±æ­¤å¯è§', 'ç»¼ä¸Šæ‰€è¿°', 'æ€»è€Œè¨€ä¹‹', 'é¦–å…ˆ', 'å…¶æ¬¡']

        connector_count = sum(1 for c in connectors if c in argument)
        logic_score = min(connector_count / 5, 1.0) * 0.7

        # æ£€æŸ¥ç»“æ„
        if 'ä¸»å¼ ' in argument and 'ç†ç”±' in argument:
            logic_score += 0.3

        return min(1.0, logic_score)

    @classmethod
    def _evaluate_clarity(cls, argument: str) -> float:
        """è¯„ä¼°æ¸…æ™°åº¦"""
        # ç®€å•å¯å‘å¼ï¼šæ£€æŸ¥å¥å­é•¿åº¦å’Œæ ‡ç‚¹
        sentences = argument.replace('ã€‚', '.').replace('ï¼', '!').replace('ï¼Ÿ', '?').split('.')
        avg_sentence_len = sum(len(s.strip()) for s in sentences) / max(len(sentences), 1)

        # ç†æƒ³å¥å­é•¿åº¦ï¼š20-50å­—
        if 20 <= avg_sentence_len <= 50:
            clarity = 1.0
        elif avg_sentence_len < 10:
            clarity = 0.3
        elif avg_sentence_len > 100:
            clarity = 0.3
        else:
            clarity = 0.7

        # æ£€æŸ¥ä¸“ä¸šæœ¯è¯­è¿‡å¤š
        jargon = ['æ³›åŒ–è¯¯å·®', 'æ¢¯åº¦æ¶ˆå¤±', 'è¿‡æ‹Ÿåˆ', 'æ­£åˆ™åŒ–', 'æ³¨æ„åŠ›æœºåˆ¶']
        jargon_count = sum(1 for j in jargon if j in argument)
        if jargon_count > 5:
            clarity *= 0.5

        return clarity


class DebateSystem:
    """
    è¾©è®ºå¼æ™ºèƒ½ä½“ç³»ç»Ÿ

    ç‰¹ç‚¹ï¼š
    1. æ­£å¼è¾©è®ºç»“æ„ï¼ˆå¼€ç¯‡ã€åé©³ã€è´¨è¯¢ã€ç»“æ¡ˆï¼‰
    2. é€»è¾‘è°¬è¯¯æ£€æµ‹
    3. è®ºç‚¹å¼ºåº¦åˆ†æ
    4. è¯„å§”æŠ•ç¥¨æœºåˆ¶
    5. å…±è¯†å½¢æˆè¿‡ç¨‹
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # è¾©è®ºå‚æ•°
        self.debate_params = config.get('debate', {
            'max_rounds': 4,
            'time_limit_per_speaker': 120,  # ç§’
            'min_arguments_per_side': 2,
            'require_evidence': True,
            'voting_threshold': 0.6  # 60%æŠ•ç¥¨é€šè¿‡
        })

        # æ™ºèƒ½ä½“æ± 
        self.agents = {}
        self.initialize_agents()

        # è¾©è®ºå†å²
        self.debate_history: Dict[str, DebateResult] = {}

        # è¯„å§”ç³»ç»Ÿ
        self.judges = self._create_judges()

        self.logger.info("âœ… è¾©è®ºå¼æ™ºèƒ½ä½“ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def initialize_agents(self):
        """åˆå§‹åŒ–è¾©è®ºæ™ºèƒ½ä½“"""
        # åˆ›å»ºä¸åŒç±»å‹çš„è¾©è®ºè€…
        self.agents = {
            'pro_analyst': self._create_agent(
                name="ProAnalyst",
                role=DebateRole.PROPOSITION,
                style="ä¸¥è°¨çš„åˆ†æå¸ˆï¼Œå–„äºç”¨æ•°æ®å’Œé€»è¾‘æ”¯æŒè®ºç‚¹",
                bias="å€¾å‘äºä¼˜åŒ–å’Œæ”¹è¿›ç°æœ‰æ–¹æ¡ˆ"
            ),
            'con_critic': self._create_agent(
                name="ConCritic",
                role=DebateRole.OPPOSITION,
                style="ä¸¥æ ¼çš„æ‰¹è¯„å®¶ï¼Œä¸“æ³¨äºå‘ç°é—®é¢˜å’Œé£é™©",
                bias="å€¾å‘äºä¿å®ˆå’Œè°¨æ…çš„æ–¹æ¡ˆ"
            ),
            'balanced_architect': self._create_agent(
                name="BalancedArchitect",
                role=DebateRole.PROPOSITION,
                style="å¹³è¡¡çš„å»ºç­‘å¸ˆï¼Œå¯»æ±‚æŠ˜ä¸­å’Œåˆ›æ–°",
                bias="å€¾å‘äºèåˆä¸åŒè§‚ç‚¹çš„æ–¹æ¡ˆ"
            ),
            'radical_innovator': self._create_agent(
                name="RadicalInnovator",
                role=DebateRole.PROPOSITION,
                style="æ¿€è¿›çš„åˆ›æ–°è€…ï¼Œè¿½æ±‚çªç ´æ€§æ”¹å˜",
                bias="å€¾å‘äºé¢ è¦†æ€§æ–¹æ¡ˆ"
            )
        }

        # åˆ›å»ºä¸»æŒäºº
        self.moderator = self._create_moderator()

    def _create_agent(self, name: str, role: DebateRole, style: str, bias: str) -> Dict[str, Any]:
        """åˆ›å»ºè¾©è®ºæ™ºèƒ½ä½“"""
        # æ ¹æ®è§’è‰²è®¾ç½®ç³»ç»Ÿæç¤ºè¯
        if role == DebateRole.PROPOSITION:
            system_prompt = f"""ä½ æ˜¯ä¸€ä½è¾©è®ºçš„æ­£æ–¹ä»£è¡¨ï¼Œ{style}ã€‚

è¾©è®ºé£æ ¼ï¼š{style}
å›ºæœ‰åè§ï¼š{bias}

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. æå‡ºå¼ºæœ‰åŠ›çš„æ”¯æŒè®ºç‚¹
2. æä¾›æ•°æ®å’Œè¯æ®æ”¯æŒ
3. é¢„æµ‹å¹¶åé©³å¯èƒ½çš„åå¯¹æ„è§
4. åœ¨åé©³é˜¶æ®µæœ‰æ•ˆå›åº”åæ–¹
5. åœ¨è´¨è¯¢é˜¶æ®µæ¸…æ™°å›ç­”é—®é¢˜

è¾©è®ºæŠ€å·§ï¼š
- ä½¿ç”¨ä¸‰æ®µè®ºï¼šä¸»å¼ ã€ç†ç”±ã€ç»“è®º
- æä¾›å…·ä½“è¯æ®å’Œä¾‹å­
- é¿å…é€»è¾‘è°¬è¯¯
- ä¿æŒç†æ€§å’Œä¸“ä¸šæ€§
- æ”»å‡»è®ºç‚¹è€Œéä¸ªäºº

è¾“å‡ºæ ¼å¼ï¼š
ä¸»å¼ ï¼š[ä½ çš„æ ¸å¿ƒä¸»å¼ ]
ç†ç”±ï¼š[æ”¯æŒä¸»å¼ çš„ç†ç”±]
è¯æ®ï¼š[ç›¸å…³è¯æ®å’Œæ•°æ®]
é¢„æµ‹åé©³ï¼š[é¢„æµ‹çš„åæ–¹è®ºç‚¹]
å›åº”ç­–ç•¥ï¼š[å¦‚ä½•å›åº”]
"""
        else:  # OPPOSITION
            system_prompt = f"""ä½ æ˜¯ä¸€ä½è¾©è®ºçš„åæ–¹ä»£è¡¨ï¼Œ{style}ã€‚

è¾©è®ºé£æ ¼ï¼š{style}
å›ºæœ‰åè§ï¼š{bias}

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. æŒ‡å‡ºæ­£æ–¹è®ºç‚¹çš„ç¼ºé™·å’Œé£é™©
2. æå‡ºæ›¿ä»£æ–¹æ¡ˆå’Œæ›´å¥½é€‰æ‹©
3. è´¨ç–‘è¯æ®çš„æœ‰æ•ˆæ€§å’Œç›¸å…³æ€§
4. åœ¨åé©³é˜¶æ®µæœ‰æ•ˆå‰Šå¼±æ­£æ–¹è®ºç‚¹
5. åœ¨è´¨è¯¢é˜¶æ®µæå‡ºå°–é”é—®é¢˜

è¾©è®ºæŠ€å·§ï¼š
- æŒ‡å‡ºé€»è¾‘è°¬è¯¯å’Œå‡è®¾é—®é¢˜
- å¼ºè°ƒè¢«å¿½ç•¥çš„é£é™©å’Œæˆæœ¬
- æä¾›å¯¹æ¯”å’Œæ›¿ä»£æ–¹æ¡ˆ
- ä¿æŒå»ºè®¾æ€§æ‰¹è¯„æ€åº¦
- èšç„¦è®®é¢˜æœ¬èº«

è¾“å‡ºæ ¼å¼ï¼š
åå¯¹ç‚¹ï¼š[ä¸»è¦åå¯¹æ„è§]
ç†ç”±ï¼š[åå¯¹çš„ç†ç”±]
é£é™©ï¼š[è¯†åˆ«å‡ºçš„é£é™©]
æ›¿ä»£æ–¹æ¡ˆï¼š[æ›´å¥½çš„é€‰æ‹©]
è´¨ç–‘é—®é¢˜ï¼š[è´¨è¯¢é˜¶æ®µçš„é—®é¢˜]
"""

        return {
            'name': name,
            'role': role,
            'style': style,
            'bias': bias,
            'system_prompt': system_prompt,
            'arguments': [],
            'performance': {
                'arguments_made': 0,
                'argument_strength_avg': 0.0,
                'fallacies_detected': 0,
                'rebuttals_successful': 0
            }
        }

    def _create_moderator(self) -> Dict[str, Any]:
        """åˆ›å»ºä¸»æŒäºº"""
        return {
            'name': 'Moderator',
            'role': DebateRole.MODERATOR,
            'system_prompt': """ä½ æ˜¯è¾©è®ºçš„ä¸»æŒäººï¼Œè´Ÿè´£ï¼š
1. æ§åˆ¶è¾©è®ºæµç¨‹å’Œæ—¶é—´
2. ç¡®ä¿éµå®ˆè¾©è®ºè§„åˆ™
3. ç»´æŒç§©åºå’Œå°Šé‡æ°›å›´
4. æå‡ºæ¾„æ¸…æ€§é—®é¢˜
5. æ€»ç»“å„æ–¹è§‚ç‚¹

ä¸»æŒåŸåˆ™ï¼š
- ä¿æŒä¸­ç«‹å’Œå…¬æ­£
- ç¡®ä¿æ¯ä¸ªäººéƒ½æœ‰å‘è¨€æœºä¼š
- åŠæ—¶åˆ¶æ­¢äººèº«æ”»å‡»
- èšç„¦æ ¸å¿ƒè®®é¢˜
- ä¿ƒè¿›æœ‰å»ºè®¾æ€§çš„è®¨è®º
""",
            'rules': self._get_debate_rules()
        }

    def _create_judges(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºè¯„å§”"""
        return [
            {
                'name': 'TechnicalJudge',
                'specialty': 'æŠ€æœ¯å¯è¡Œæ€§',
                'criteria': ['åˆ›æ–°æ€§', 'å¯è¡Œæ€§', 'æ•ˆç‡', 'å¯æ‰©å±•æ€§'],
                'weight': 0.4
            },
            {
                'name': 'RiskJudge',
                'specialty': 'é£é™©è¯„ä¼°',
                'criteria': ['å®‰å…¨æ€§', 'ç¨³å®šæ€§', 'é£é™©æ§åˆ¶', 'å®¹é”™æ€§'],
                'weight': 0.3
            },
            {
                'name': 'PracticalJudge',
                'specialty': 'å®è·µåº”ç”¨',
                'criteria': ['æ˜“ç”¨æ€§', 'æˆæœ¬æ•ˆç›Š', 'éƒ¨ç½²éš¾åº¦', 'ç»´æŠ¤æ€§'],
                'weight': 0.3
            }
        ]

    def _get_debate_rules(self) -> List[str]:
        """è·å–è¾©è®ºè§„åˆ™"""
        return [
            "æ¯ä½è¾©æ‰‹æœ‰2åˆ†é’Ÿå‘è¨€æ—¶é—´",
            "å‘è¨€é¡ºåºï¼šæ­£æ–¹â†’åæ–¹â†’æ­£æ–¹â†’åæ–¹",
            "è´¨è¯¢é˜¶æ®µï¼šæ¯ä½è¾©æ‰‹å¯ä»¥æé—®1åˆ†é’Ÿ",
            "ç¦æ­¢äººèº«æ”»å‡»å’Œæƒ…ç»ªåŒ–è¨€è®º",
            "å¿…é¡»æä¾›è¯æ®æ”¯æŒè®ºç‚¹",
            "å¿…é¡»å›åº”å¯¹æ–¹çš„ç›´æ¥è´¨ç–‘",
            "ç»“è®ºå¿…é¡»åŸºäºå·²æå‡ºçš„è®ºæ®"
        ]

    async def conduct_debate(self, topic: str, context: Dict[str, Any]) -> DebateResult:
        """
        æ‰§è¡Œè¾©è®º

        Args:
            topic: è¾©è®ºè®®é¢˜
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            è¾©è®ºç»“æœ
        """
        debate_id = str(uuid.uuid4())
        self.logger.info(f"âš–ï¸ å¼€å§‹è¾©è®º: {topic}")

        # å‡†å¤‡è¾©è®º
        prepared_topic = self._prepare_debate_topic(topic, context)

        # è¾©è®ºæµç¨‹
        rounds = []

        # é˜¶æ®µ1ï¼šå¼€ç¯‡é™ˆè¿°
        opening_round = await self._conduct_round(
            debate_id, DebatePhase.OPENING, prepared_topic, context
        )
        rounds.append(opening_round)

        # é˜¶æ®µ2ï¼šåé©³é˜¶æ®µ
        rebuttal_round = await self._conduct_round(
            debate_id, DebatePhase.REBUTTAL, prepared_topic, context,
            previous_round=opening_round
        )
        rounds.append(rebuttal_round)

        # é˜¶æ®µ3ï¼šäº¤å‰è´¨è¯¢
        cross_round = await self._conduct_round(
            debate_id, DebatePhase.CROSS_EXAMINATION, prepared_topic, context,
            previous_round=rebuttal_round
        )
        rounds.append(cross_round)

        # é˜¶æ®µ4ï¼šç»“æ¡ˆé™ˆè¯
        closing_round = await self._conduct_round(
            debate_id, DebatePhase.CLOSING, prepared_topic, context,
            previous_round=cross_round
        )
        rounds.append(closing_round)

        # é˜¶æ®µ5ï¼šæŠ•ç¥¨å’Œåˆ¤å†³
        voting_result = await self._conduct_voting(rounds, context)

        # æ„å»ºè¾©è®ºç»“æœ
        debate_result = DebateResult(
            debate_id=debate_id,
            topic=topic,
            rounds=rounds,
            final_vote=voting_result['votes'],
            winner=voting_result['winner'],
            consensus=voting_result['consensus'],
            reasoning=voting_result['reasoning'],
            metrics=self._calculate_debate_metrics(rounds)
        )

        # ä¿å­˜åˆ°å†å²
        self.debate_history[debate_id] = debate_result

        self.logger.info(f"âœ… è¾©è®ºå®Œæˆ: {debate_id}, èƒœæ–¹: {debate_result.winner}")

        return debate_result

    def _prepare_debate_topic(self, topic: str, context: Dict[str, Any]) -> str:
        """å‡†å¤‡è¾©è®ºè®®é¢˜"""
        features = context.get('features', {})
        metrics = context.get('metrics', {})
        current_params = context.get('current_params', {})

        prepared = f"""
è¾©è®ºè®®é¢˜ï¼š{topic}

èƒŒæ™¯ä¿¡æ¯ï¼š
- æ¨¡å‹æ€§èƒ½ï¼šMSE={metrics.get('mse', 0):.4f}, MAE={metrics.get('mae', 0):.4f}
- å½“å‰å‚æ•°ï¼š{json.dumps(current_params, indent=2, ensure_ascii=False)}
- æ•°æ®ç‰¹å¾ï¼š{json.dumps(features.get('statistics', {}), indent=2, ensure_ascii=False)}

è¾©è®ºç„¦ç‚¹ï¼š
1. å½“å‰è°ƒæ•´æ–¹æ¡ˆçš„æŠ€æœ¯å¯è¡Œæ€§
2. å¯èƒ½çš„é£é™©å’Œæ”¶ç›Šæƒè¡¡
3. æ›¿ä»£æ–¹æ¡ˆçš„æ¯”è¾ƒä¼˜åŠ¿
4. å®æ–½çš„ä¼˜å…ˆé¡ºåºå’Œç­–ç•¥

è¯·åŸºäºä»¥ä¸ŠèƒŒæ™¯è¿›è¡Œè¾©è®ºã€‚
"""
        return prepared

    async def _conduct_round(self, debate_id: str, phase: DebatePhase,
                             topic: str, context: Dict[str, Any],
                             previous_round: DebateRound = None) -> DebateRound:
        """æ‰§è¡Œä¸€ä¸ªè¾©è®ºè½®æ¬¡"""
        round_id = f"{debate_id}_{phase.value}"
        self.logger.info(f"  ğŸ—£ï¸  {phase.value}é˜¶æ®µå¼€å§‹")

        round_args = []

        # æ ¹æ®é˜¶æ®µç¡®å®šå‘è¨€é¡ºåº
        if phase == DebatePhase.OPENING:
            speakers = ['ProAnalyst', 'ConCritic', 'BalancedArchitect', 'RadicalInnovator']
        elif phase == DebatePhase.REBUTTAL:
            speakers = ['ConCritic', 'ProAnalyst', 'RadicalInnovator', 'BalancedArchitect']
        elif phase == DebatePhase.CROSS_EXAMINATION:
            speakers = self._create_cross_examination_pairs()
        elif phase == DebatePhase.CLOSING:
            speakers = ['ProAnalyst', 'ConCritic']  # ä¸»è¦è¾©æ‰‹ç»“æ¡ˆ

        for speaker_name in speakers:
            # è·å–æ™ºèƒ½ä½“
            agent = self._get_agent_by_name(speaker_name)
            if not agent:
                continue

            # ç”Ÿæˆè®ºç‚¹
            argument = await self._generate_argument(
                agent, phase, topic, context, previous_round
            )

            # åˆ†æè®ºç‚¹
            argument.strength = ArgumentStrengthAnalyzer.analyze(
                argument.content, argument.evidence
            )
            argument.fallacies = LogicalFallacyDetector.detect(argument.content)

            # æ›´æ–°æ™ºèƒ½ä½“è¡¨ç°
            self._update_agent_performance(agent['name'], argument)

            round_args.append(argument)

            self.logger.debug(f"    {speaker_name}: å¼ºåº¦={argument.strength:.2f}, "
                              f"è°¬è¯¯={len(argument.fallacies)}")

        # ç”Ÿæˆè½®æ¬¡æ€»ç»“
        summary = self._generate_round_summary(phase, round_args)

        round_result = DebateRound(
            round_id=round_id,
            phase=phase,
            arguments=round_args,
            summary=summary
        )

        return round_result

    def _create_cross_examination_pairs(self) -> List[str]:
        """åˆ›å»ºäº¤å‰è´¨è¯¢å¯¹"""
        # æ­£æ–¹é—®åæ–¹ï¼Œåæ–¹é—®æ­£æ–¹
        pairs = []
        pro_agents = [name for name, agent in self.agents.items()
                      if agent['role'] == DebateRole.PROPOSITION]
        con_agents = [name for name, agent in self.agents.items()
                      if agent['role'] == DebateRole.OPPOSITION]

        # åˆ›å»ºé…å¯¹
        for pro, con in zip(pro_agents, con_agents):
            pairs.extend([pro, con])

        return pairs

    def _get_agent_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        """é€šè¿‡åç§°è·å–æ™ºèƒ½ä½“"""
        for key, agent in self.agents.items():
            if agent['name'] == name:
                return agent
        return None

    async def _generate_argument(self, agent: Dict[str, Any], phase: DebatePhase,
                                 topic: str, context: Dict[str, Any],
                                 previous_round: DebateRound = None) -> DebateArgument:
        """ç”Ÿæˆè®ºç‚¹"""
        # æ ¹æ®é˜¶æ®µå‡†å¤‡æç¤ºè¯
        prompt = self._build_argument_prompt(agent, phase, topic, context, previous_round)

        # æ¨¡æ‹ŸAPIè°ƒç”¨ï¼ˆå®é™…åº”è°ƒç”¨LLM APIï¼‰
        # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”
        content = await self._simulate_llm_call(prompt, agent)

        # è§£æè®ºç‚¹å†…å®¹
        claims, evidence = self._parse_argument_content(content)

        argument = DebateArgument(
            argument_id=str(uuid.uuid4()),
            speaker=agent['name'],
            role=agent['role'],
            phase=phase,
            content=content,
            claims=claims,
            evidence=evidence
        )

        return argument

    def _build_argument_prompt(self, agent: Dict[str, Any], phase: DebatePhase,
                               topic: str, context: Dict[str, Any],
                               previous_round: DebateRound = None) -> str:
        """æ„å»ºè®ºç‚¹æç¤ºè¯"""
        base_prompt = agent['system_prompt']

        phase_instructions = {
            DebatePhase.OPENING: "è¯·è¿›è¡Œå¼€ç¯‡é™ˆè¿°ï¼Œæå‡ºä½ çš„æ ¸å¿ƒè§‚ç‚¹ã€‚",
            DebatePhase.REBUTTAL: "è¯·åé©³å¯¹æ–¹çš„è§‚ç‚¹ï¼ŒæŒ‡å‡ºå…¶ç¼ºé™·ã€‚",
            DebatePhase.CROSS_EXAMINATION: "è¯·å‘å¯¹æ–¹æé—®ï¼Œæˆ–å›ç­”å¯¹æ–¹çš„é—®é¢˜ã€‚",
            DebatePhase.CLOSING: "è¯·è¿›è¡Œç»“æ¡ˆé™ˆè¯ï¼Œæ€»ç»“ä½ çš„ç«‹åœºã€‚"
        }

        prompt = f"""{base_prompt}

å½“å‰é˜¶æ®µï¼š{phase_instructions.get(phase, '')}
è¾©è®ºè®®é¢˜ï¼š{topic}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{json.dumps(context, indent=2, ensure_ascii=False)}
"""

        if previous_round and phase != DebatePhase.OPENING:
            # æ·»åŠ å‰ä¸€è½®çš„è®ºç‚¹æ‘˜è¦
            previous_summary = self._summarize_previous_round(previous_round, agent['role'])
            prompt += f"\nå‰ä¸€è½®è®¨è®ºæ‘˜è¦ï¼š\n{previous_summary}"

        return prompt

    async def _simulate_llm_call(self, prompt: str, agent: Dict[str, Any]) -> str:
        """æ¨¡æ‹ŸLLMè°ƒç”¨ï¼ˆå®é™…é¡¹ç›®åº”æ›¿æ¢ä¸ºçœŸå®APIè°ƒç”¨ï¼‰"""
        # æ¨¡æ‹Ÿæ€è€ƒæ—¶é—´
        await asyncio.sleep(0.5)

        # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹ç”Ÿæˆä¸åŒé£æ ¼çš„å“åº”
        style = agent.get('style', '')
        bias = agent.get('bias', '')

        if 'æ¿€è¿›' in style:
            responses = [
                "æˆ‘ä»¬å¿…é¡»é‡‡å–å¤§èƒ†çš„è¡ŒåŠ¨ï¼å½“å‰çš„æ¸è¿›å¼è°ƒæ•´å·²ç»æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚",
                "çªç ´æ€§åˆ›æ–°æ˜¯å”¯ä¸€çš„å‡ºè·¯ï¼Œæˆ‘ä»¬ä¸èƒ½è¢«ä¼ ç»Ÿæ€ç»´æŸç¼šã€‚",
                "é«˜é£é™©æ„å‘³ç€é«˜å›æŠ¥ï¼Œæˆ‘ä»¬åº”è¯¥å‹‡æ•¢å°è¯•æ–°çš„æ¶æ„ã€‚"
            ]
        elif 'æ‰¹è¯„' in style:
            responses = [
                "è¿™ä¸ªæ–¹æ¡ˆå­˜åœ¨æ˜æ˜¾çš„é£é™©ï¼Œæˆ‘ä»¬éœ€è¦æ›´è°¨æ…çš„è¯„ä¼°ã€‚",
                "å¯¹æ–¹å¿½ç•¥äº†å…³é”®çš„æˆæœ¬é—®é¢˜ï¼Œå®æ–½èµ·æ¥å›°éš¾é‡é‡ã€‚",
                "æœ‰æ›´å¥½çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªé«˜é£é™©é€‰é¡¹ï¼Ÿ"
            ]
        elif 'å¹³è¡¡' in style:
            responses = [
                "æˆ‘ä»¬éœ€è¦åœ¨åˆ›æ–°å’Œç¨³å®šä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚",
                "ç»¼åˆåŒæ–¹è§‚ç‚¹ï¼Œæˆ‘è®¤ä¸ºæŠ˜ä¸­æ–¹æ¡ˆæ˜¯æœ€ä½³é€‰æ‹©ã€‚",
                "æ—¢è¦è€ƒè™‘æŠ€æœ¯å¯è¡Œæ€§ï¼Œä¹Ÿè¦è¯„ä¼°å®é™…é£é™©ã€‚"
            ]
        else:
            responses = [
                "åŸºäºæ•°æ®åˆ†æï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ªæ–¹å‘æ˜¯æ­£ç¡®çš„ã€‚",
                "å®éªŒç»“æœæ”¯æŒæˆ‘ä»¬çš„è§‚ç‚¹ï¼Œåº”è¯¥ç»§ç»­æ¨è¿›ã€‚",
                "ä»æŠ€æœ¯è§’åº¦ï¼Œè¿™ä¸ªæ–¹æ¡ˆå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚"
            ]

        # æ·»åŠ ä¸€äº›æŠ€æœ¯å†…å®¹
        tech_terms = [
            "è°±é—¨æ§é˜ˆå€¼éœ€è¦è°ƒæ•´ä»¥ä¼˜åŒ–é¢‘åŸŸç‰¹å¾æå–ã€‚",
            "æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–å¯ä»¥æå‡æ¨¡å‹çš„å¹³æ»‘æ€§ã€‚",
            "å­¦ä¹ ç‡è°ƒåº¦åº”è¯¥æ ¹æ®æŸå¤±æ›²çº¿åŠ¨æ€è°ƒæ•´ã€‚",
            "æ³¨æ„åŠ›æœºåˆ¶éœ€è¦é‡æ–°è®¾è®¡ä»¥æ•æ‰é•¿æœŸä¾èµ–ã€‚"
        ]

        response = random.choice(responses) + " " + random.choice(tech_terms)

        # æ ¹æ®åè§è°ƒæ•´è¯­æ°”
        if 'å€¾å‘' in bias:
            response += " " + bias

        return response

    def _parse_argument_content(self, content: str) -> Tuple[List[str], Dict[str, Any]]:
        """è§£æè®ºç‚¹å†…å®¹ï¼Œæå–ä¸»å¼ å’Œè¯æ®"""
        claims = []
        evidence = {}

        # ç®€å•è§£æï¼šæŸ¥æ‰¾ä¸»å¼ å’Œè¯æ®å…³é”®è¯
        lines = content.split('\n')

        for line in lines:
            if 'ä¸»å¼ ' in line or 'è§‚ç‚¹' in line or 'è®¤ä¸º' in line:
                claim = line.replace('ä¸»å¼ ï¼š', '').replace('è§‚ç‚¹ï¼š', '').replace('è®¤ä¸º', '').strip()
                if claim and len(claim) > 5:
                    claims.append(claim)

            if 'è¯æ®' in line or 'æ•°æ®' in line or 'å®éªŒ' in line:
                # æå–è¯æ®ä¿¡æ¯
                evidence_key = line.split('ï¼š')[0] if 'ï¼š' in line else 'evidence'
                evidence_value = line.split('ï¼š')[1] if 'ï¼š' in line else line
                evidence[evidence_key] = evidence_value.strip()

        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ä¸»å¼ ï¼Œä½¿ç”¨æ•´ä¸ªå†…å®¹ä½œä¸ºä¸»å¼ 
        if not claims and len(content) > 10:
            claims.append(content[:100] + '...')

        return claims, evidence

    def _summarize_previous_round(self, previous_round: DebateRound,
                                  current_role: DebateRole) -> str:
        """æ€»ç»“å‰ä¸€è½®è¾©è®º"""
        if not previous_round or not previous_round.arguments:
            return "æ— å‰ä¸€è½®è®¨è®ºã€‚"

        # æå–å¯¹ç«‹æ–¹çš„è®ºç‚¹
        opposing_args = []
        for arg in previous_round.arguments:
            if arg.role != current_role:
                opposing_args.append(arg)

        if not opposing_args:
            return "å‰ä¸€è½®æ²¡æœ‰å¯¹ç«‹è§‚ç‚¹ã€‚"

        # ç”Ÿæˆæ‘˜è¦
        summary = f"å‰ä¸€è½®({previous_round.phase.value})ä¸­ï¼Œå¯¹æ–¹æå‡ºäº†{len(opposing_args)}ä¸ªè®ºç‚¹ï¼š\n"

        for i, arg in enumerate(opposing_args[:3], 1):  # æœ€å¤š3ä¸ªè®ºç‚¹
            summary += f"{i}. {arg.content[:100]}...\n"

        return summary

    def _update_agent_performance(self, agent_name: str, argument: DebateArgument):
        """æ›´æ–°æ™ºèƒ½ä½“è¡¨ç°"""
        agent = self._get_agent_by_name(agent_name)
        if not agent:
            return

        perf = agent['performance']
        perf['arguments_made'] += 1

        # æ›´æ–°å¹³å‡å¼ºåº¦
        current_avg = perf['argument_strength_avg']
        total_args = perf['arguments_made']
        perf['argument_strength_avg'] = (
                                                current_avg * (total_args - 1) + argument.strength
                                        ) / total_args

        # æ›´æ–°è°¬è¯¯è®¡æ•°
        perf['fallacies_detected'] += len(argument.fallacies)

    def _generate_round_summary(self, phase: DebatePhase,
                                arguments: List[DebateArgument]) -> str:
        """ç”Ÿæˆè½®æ¬¡æ‘˜è¦"""
        if not arguments:
            return "æœ¬è½®æ— è®ºç‚¹ã€‚"

        # æŒ‰è§’è‰²åˆ†ç»„
        pro_args = [arg for arg in arguments if arg.role == DebateRole.PROPOSITION]
        con_args = [arg for arg in arguments if arg.role == DebateRole.OPPOSITION]

        summary = f"{phase.value}é˜¶æ®µæ€»ç»“ï¼š\n"
        summary += f"æ­£æ–¹è®ºç‚¹ï¼š{len(pro_args)}ä¸ªï¼Œå¹³å‡å¼ºåº¦ï¼š"
        summary += f"{sum(a.strength for a in pro_args) / len(pro_args):.2f}\n"
        summary += f"åæ–¹è®ºç‚¹ï¼š{len(con_args)}ä¸ªï¼Œå¹³å‡å¼ºåº¦ï¼š"
        summary += f"{sum(a.strength for a in con_args) / len(con_args):.2f}\n"

        # å…³é”®è®ºç‚¹
        if arguments:
            strongest = max(arguments, key=lambda x: x.strength)
            summary += f"æœ€å¼ºè®ºç‚¹ï¼š{strongest.speaker}ï¼ˆå¼ºåº¦ï¼š{strongest.strength:.2f}ï¼‰\n"

        return summary

    async def _conduct_voting(self, rounds: List[DebateRound],
                              context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒæŠ•ç¥¨"""
        self.logger.info("  ğŸ—³ï¸  å¼€å§‹æŠ•ç¥¨...")

        # æ”¶é›†æ‰€æœ‰è®ºç‚¹
        all_arguments = []
        for round_obj in rounds:
            all_arguments.extend(round_obj.arguments)

        # è¯„å§”æŠ•ç¥¨
        votes = {'proposition': 0, 'opposition': 0, 'abstain': 0}
        judge_reasons = []

        for judge in self.judges:
            vote, reason = await self._judge_vote(judge, all_arguments, context)
            votes[vote] += 1
            judge_reasons.append(f"{judge['name']}ï¼ˆ{judge['specialty']}ï¼‰ï¼š{reason}")

        # ç¡®å®šèƒœæ–¹
        total_votes = sum(votes.values())
        pro_ratio = votes['proposition'] / total_votes if total_votes > 0 else 0

        if pro_ratio > self.debate_params['voting_threshold']:
            winner = 'proposition'
        elif votes['opposition'] > votes['proposition']:
            winner = 'opposition'
        else:
            winner = 'abstain'

        # ç”Ÿæˆå…±è¯†
        consensus = await self._generate_consensus(all_arguments, winner, context)

        return {
            'votes': votes,
            'winner': winner,
            'consensus': consensus,
            'judge_reasons': judge_reasons,
            'reasoning': '\n'.join(judge_reasons)
        }

    async def _judge_vote(self, judge: Dict[str, Any],
                          arguments: List[DebateArgument],
                          context: Dict[str, Any]) -> Tuple[str, str]:
        """è¯„å§”æŠ•ç¥¨"""
        # æ ¹æ®è¯„å§”ä¸“ä¸šé¢†åŸŸè¯„ä¼°è®ºç‚¹
        pro_args = [arg for arg in arguments if arg.role == DebateRole.PROPOSITION]
        con_args = [arg for arg in arguments if arg.role == DebateRole.OPPOSITION]

        # è®¡ç®—æ¯ä¸ªç«‹åœºçš„å¾—åˆ†
        pro_score = self._evaluate_by_criteria(pro_args, judge['criteria'])
        con_score = self._evaluate_by_criteria(con_args, judge['criteria'])

        # åº”ç”¨è¯„å§”æƒé‡
        pro_score *= judge['weight']
        con_score *= judge['weight']

        # å†³å®šæŠ•ç¥¨
        if pro_score > con_score * 1.1:  # 10%ä¼˜åŠ¿
            vote = 'proposition'
            reason = f"æ­£æ–¹åœ¨{judge['specialty']}æ–¹é¢æ›´å…·ä¼˜åŠ¿ï¼ˆ{pro_score:.2f} vs {con_score:.2f}ï¼‰"
        elif con_score > pro_score * 1.1:
            vote = 'opposition'
            reason = f"åæ–¹åœ¨{judge['specialty']}æ–¹é¢æ›´å…·ä¼˜åŠ¿ï¼ˆ{con_score:.2f} vs {pro_score:.2f}ï¼‰"
        else:
            vote = 'abstain'
            reason = f"åŒæ–¹åœ¨{judge['specialty']}æ–¹é¢åŠ¿å‡åŠ›æ•Œï¼ˆ{pro_score:.2f} vs {con_score:.2f}ï¼‰"

        return vote, reason

    def _evaluate_by_criteria(self, arguments: List[DebateArgument],
                              criteria: List[str]) -> float:
        """æ ¹æ®æ ‡å‡†è¯„ä¼°è®ºç‚¹"""
        if not arguments:
            return 0.0

        total_score = 0.0

        for criterion in criteria:
            criterion_score = 0.0

            for arg in arguments:
                # æ ¹æ®æ ‡å‡†è¯„ä¼°æ¯ä¸ªè®ºç‚¹
                if criterion in ['åˆ›æ–°æ€§', 'å¯è¡Œæ€§']:
                    # åˆ›æ–°æ€§å’Œå¯è¡Œæ€§è¯„ä¼°
                    criterion_score += arg.strength * 0.5
                elif criterion in ['å®‰å…¨æ€§', 'ç¨³å®šæ€§']:
                    # å®‰å…¨æ€§å’Œç¨³å®šæ€§è¯„ä¼°
                    if len(arg.fallacies) == 0:  # æ— é€»è¾‘è°¬è¯¯
                        criterion_score += arg.strength * 0.6
                    else:
                        criterion_score += arg.strength * 0.3
                elif criterion in ['æˆæœ¬æ•ˆç›Š', 'æ•ˆç‡']:
                    # æˆæœ¬å’Œæ•ˆç‡è¯„ä¼°
                    if 'è¯æ®' in arg.evidence:
                        criterion_score += arg.strength * 0.7
                    else:
                        criterion_score += arg.strength * 0.4

            total_score += criterion_score / len(criteria)

        return total_score / len(arguments) if arguments else 0.0

    async def _generate_consensus(self, arguments: List[DebateArgument],
                                  winner: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå…±è¯†å†³ç­–"""
        # æå–æœ€ä½³è®ºç‚¹
        pro_args = [arg for arg in arguments if arg.role == DebateRole.PROPOSITION]
        con_args = [arg for arg in arguments if arg.role == DebateRole.OPPOSITION]

        # é€‰æ‹©æœ€å¼ºè®ºç‚¹
        best_pro = max(pro_args, key=lambda x: x.strength) if pro_args else None
        best_con = max(con_args, key=lambda x: x.strength) if con_args else None

        # ç”Ÿæˆå…±è¯†å‚æ•°
        consensus = {
            'debate_winner': winner,
            'proposition_strength': sum(arg.strength for arg in pro_args) / len(pro_args) if pro_args else 0,
            'opposition_strength': sum(arg.strength for arg in con_args) / len(con_args) if con_args else 0,
            'total_arguments': len(arguments),
            'strongest_pro_argument': best_pro.content[:200] + '...' if best_pro else None,
            'strongest_con_argument': best_con.content[:200] + '...' if best_con else None,
            'parameters': self._extract_parameters_from_debate(arguments, winner),
            'recommendations': self._extract_recommendations(arguments, winner)
        }

        return consensus

    def _extract_parameters_from_debate(self, arguments: List[DebateArgument],
                                        winner: str) -> Dict[str, float]:
        """ä»è¾©è®ºä¸­æå–å‚æ•°"""
        parameters = {}

        # åˆ†æè®ºç‚¹ä¸­çš„å‚æ•°å»ºè®®
        param_patterns = {
            'spectral_threshold': ['è°±.*?é˜ˆ.*?å€¼', 'é¢‘.*?é—¨.*?æ§›'],
            'laplacian_weight': ['æ‹‰æ™®æ‹‰æ–¯.*?æƒ.*?é‡', 'å¹³æ»‘.*?ç³»æ•°'],
            'learning_rate': ['å­¦ä¹ .*?ç‡', 'lr', 'learning rate']
        }

        for arg in arguments:
            content = arg.content

            for param_key, patterns in param_patterns.items():
                for pattern in patterns:
                    import re
                    match = re.search(pattern + r'.*?([0-9.]+)', content, re.IGNORECASE)
                    if match:
                        try:
                            value = float(match.group(1))

                            # æ ¹æ®è®ºç‚¹å¼ºåº¦å’Œç«‹åœºè°ƒæ•´æƒé‡
                            if arg.role.value == winner:
                                weight = arg.strength
                            else:
                                weight = arg.strength * 0.5  # å¯¹ç«‹ç«‹åœºæƒé‡å‡åŠ

                            if param_key not in parameters:
                                parameters[param_key] = {'values': [], 'weights': []}

                            parameters[param_key]['values'].append(value)
                            parameters[param_key]['weights'].append(weight)
                        except:
                            pass

        # è®¡ç®—åŠ æƒå¹³å‡å€¼
        final_params = {}
        for param_key, data in parameters.items():
            if data['values']:
                weighted_sum = sum(v * w for v, w in zip(data['values'], data['weights']))
                total_weight = sum(data['weights'])
                final_params[param_key] = weighted_sum / total_weight

        # é»˜è®¤å€¼
        defaults = {
            'spectral_threshold': 0.5,
            'laplacian_weight': 0.01,
            'learning_rate_multiplier': 1.0
        }

        for key, default in defaults.items():
            if key not in final_params:
                final_params[key] = default

        return final_params

    def _extract_recommendations(self, arguments: List[DebateArgument],
                                 winner: str) -> List[str]:
        """ä»è¾©è®ºä¸­æå–å»ºè®®"""
        recommendations = []

        # æ”¶é›†æ‰€æœ‰ä¸»å¼ 
        all_claims = []
        for arg in arguments:
            all_claims.extend(arg.claims)

        # å»é‡å’Œæ’åº
        unique_claims = list(set(all_claims))

        # æ ¹æ®èƒœæ–¹åå¥½æ’åº
        if winner == 'proposition':
            # ä¼˜å…ˆè€ƒè™‘æ¿€è¿›å’Œåˆ›æ–°å»ºè®®
            for claim in unique_claims:
                if any(keyword in claim for keyword in ['åˆ›æ–°', 'çªç ´', 'ä¼˜åŒ–', 'æ”¹è¿›']):
                    recommendations.append(claim)
        elif winner == 'opposition':
            # ä¼˜å…ˆè€ƒè™‘ä¿å®ˆå’Œç¨³å¥å»ºè®®
            for claim in unique_claims:
                if any(keyword in claim for keyword in ['è°¨æ…', 'ç¨³å®š', 'é£é™©', 'éªŒè¯']):
                    recommendations.append(claim)
        else:
            # å¹³è¡¡è€ƒè™‘
            recommendations = unique_claims[:5]  # å–å‰5ä¸ª

        return recommendations[:5]  # æœ€å¤š5ä¸ªå»ºè®®

    def _calculate_debate_metrics(self, rounds: List[DebateRound]) -> Dict[str, Any]:
        """è®¡ç®—è¾©è®ºæŒ‡æ ‡"""
        total_arguments = sum(len(round_obj.arguments) for round_obj in rounds)

        if total_arguments == 0:
            return {}

        # æ”¶é›†æ‰€æœ‰è®ºç‚¹
        all_arguments = []
        for round_obj in rounds:
            all_arguments.extend(round_obj.arguments)

        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'total_rounds': len(rounds),
            'total_arguments': total_arguments,
            'avg_argument_strength': sum(arg.strength for arg in all_arguments) / total_arguments,
            'total_fallacies': sum(len(arg.fallacies) for arg in all_arguments),
            'pro_argument_count': sum(1 for arg in all_arguments if arg.role == DebateRole.PROPOSITION),
            'con_argument_count': sum(1 for arg in all_arguments if arg.role == DebateRole.OPPOSITION),
            'strongest_argument': max(all_arguments, key=lambda x: x.strength).strength if all_arguments else 0,
            'weakest_argument': min(all_arguments, key=lambda x: x.strength).strength if all_arguments else 0
        }

        return metrics

    def get_debate_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–è¾©è®ºå†å²"""
        history = []

        for debate_id, result in list(self.debate_history.items())[-limit:]:
            history.append({
                'id': debate_id,
                'topic': result.topic,
                'winner': result.winner,
                'total_rounds': len(result.rounds),
                'total_arguments': result.metrics.get('total_arguments', 0),
                'consensus_reached': result.consensus is not None
            })

        return history

    def get_agent_performance(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ™ºèƒ½ä½“è¡¨ç°ç»Ÿè®¡"""
        performance = {}

        for agent_key, agent in self.agents.items():
            perf = agent['performance']
            performance[agent['name']] = {
                'role': agent['role'].value,
                'arguments_made': perf['arguments_made'],
                'avg_argument_strength': perf['argument_strength_avg'],
                'fallacies_per_argument': (
                    perf['fallacies_detected'] / perf['arguments_made']
                    if perf['arguments_made'] > 0 else 0
                ),
                'style': agent['style'],
                'bias': agent['bias']
            }

        return performance


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """è¾©è®ºç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹"""
    import yaml

    # åŠ è½½é…ç½®
    with open("./config.yaml", "r") as f:
        config = yaml.safe_load(f)

    # åˆ›å»ºè¾©è®ºç³»ç»Ÿ
    debate_system = DebateSystem(config)

    # å‡†å¤‡è¾©è®ºè®®é¢˜
    topic = "æ˜¯å¦åº”è¯¥æé«˜ISTRç½‘ç»œçš„è°±é—¨æ§é˜ˆå€¼ä»¥æå‡æ€§èƒ½ï¼Ÿ"
    context = {
        'features': {
            'statistics': {'mean': 0.1, 'std': 0.5},
            'frequency': {'dominant_frequency': 12}
        },
        'metrics': {'mse': 0.25, 'mae': 0.35},
        'current_params': {
            'spectral_threshold': 0.5,
            'laplacian_weight': 0.01
        }
    }

    # æ‰§è¡Œè¾©è®º
    result = await debate_system.conduct_debate(topic, context)

    print(f"è¾©è®ºç»“æœï¼šèƒœæ–¹ - {result.winner}")
    print(f"å…±è¯†å‚æ•°ï¼š{json.dumps(result.consensus['parameters'], indent=2)}")

    # æŸ¥çœ‹æ™ºèƒ½ä½“è¡¨ç°
    performance = debate_system.get_agent_performance()
    for agent_name, perf in performance.items():
        print(f"{agent_name}: {perf['avg_argument_strength']:.2f} å¼ºåº¦")


if __name__ == "__main__":
    asyncio.run(main())