"""
AutoGenå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ - å…¼å®¹æ—§ç‰ˆæœ¬
ä½¿ç”¨æ—§ç‰ˆOpenAI API (0.28.1) å’Œæ—§ç‰ˆAutoGen
"""

import os
import sys
import json
from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Union
import warnings
from datetime import datetime

# å°è¯•å¯¼å…¥æ—§ç‰ˆOpenAI
try:
    import openai  # ç‰ˆæœ¬ 0.28.1

    OPENAI_VERSION = getattr(openai, '__version__', 'unknown')
    print(f"âœ… ä½¿ç”¨æ—§ç‰ˆOpenAI: {OPENAI_VERSION}")
    OPENAI_AVAILABLE = True
except ImportError:
    print("âŒ OpenAIæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai==0.28.1")
    OPENAI_AVAILABLE = False

# å°è¯•å¯¼å…¥AutoGen
try:
    import autogen
    from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager

    AUTO_GEN_VERSION = getattr(autogen, '__version__', 'unknown')
    print(f"âœ… ä½¿ç”¨AutoGenç‰ˆæœ¬: {AUTO_GEN_VERSION}")
    AUTO_GEN_AVAILABLE = True
except ImportError as e:
    print(f"âŒ AutoGenå¯¼å…¥å¤±è´¥: {e}")
    AUTO_GEN_AVAILABLE = False

from .memory_bank import MemoryBank


@dataclass
class DebateConfig:
    """è¾©è®ºé…ç½® - æ—§ç‰ˆå…¼å®¹"""
    agent_count: int = 3
    debate_rounds: int = 2
    temperature: float = 0.7
    use_memory: bool = True
    use_real_llm: bool = False  # æ˜¯å¦ä½¿ç”¨çœŸå®LLM
    api_config: Optional[Dict] = None


class DebateResult:
    """è¾©è®ºç»“æœ"""

    def __init__(self, consensus: str = "", recommendations: List[str] = None):
        self.consensus = consensus
        self.recommendations = recommendations or []
        self.debate_log = []
        self.raw_messages = []

    def get_consensus_insights(self) -> Dict[str, Any]:
        """ä»å…±è¯†ä¸­æå–è§è§£"""
        insights = {}
        if "è°ƒæ•´è¶‹åŠ¿" in self.consensus or "adjust trend" in self.consensus.lower():
            insights["adjust_trend"] = 0.1  # é»˜è®¤10%è°ƒæ•´
        if "å¹³æ»‘" in self.consensus or "smooth" in self.consensus.lower():
            insights["smooth_variance"] = True
        if "å­£èŠ‚æ€§" in self.consensus or "seasonal" in self.consensus.lower():
            insights["seasonal_adjust"] = True
        return insights


class AutoGenDebateSystem:
    """AutoGenå¤šæ™ºèƒ½ä½“è¾©è®ºç³»ç»Ÿ - æ—§ç‰ˆå…¼å®¹"""

    def __init__(self, config: DebateConfig, memory_bank: Optional[MemoryBank] = None):
        self.config = config
        self.memory_bank = memory_bank or MemoryBank(config={})

        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        self.qwen_api_key = os.getenv("QWEN_API_KEY")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        print(f"ğŸ¤– åˆå§‹åŒ–AutoGenè¾©è®ºç³»ç»Ÿ (ä½¿ç”¨çœŸå®LLM: {config.use_real_llm})")

        if config.use_real_llm and OPENAI_AVAILABLE and AUTO_GEN_AVAILABLE:
            self.agents = self._initialize_real_agents()
            self.llm_mode = "real"
        else:
            self.agents = self._initialize_mock_agents()
            self.llm_mode = "mock"

        self.conversation_history = []

    def _initialize_real_agents(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–çœŸå®çš„æ™ºèƒ½ä½“ï¼ˆä½¿ç”¨æ—§ç‰ˆAutoGen APIï¼‰"""
        print("ğŸ”§ ä½¿ç”¨æ—§ç‰ˆAutoGen APIåˆå§‹åŒ–çœŸå®æ™ºèƒ½ä½“...")

        agents = {}

        try:
            # é…ç½®DeepSeek API (æ—§ç‰ˆOpenAIæ ¼å¼)
            deepseek_config = {
                "model": "deepseek-chat",  # æˆ– "deepseek-reasoner"
                "api_key": self.deepseek_api_key,
                "api_base": "https://api.deepseek.com/v1",
                "api_type": "open_ai"
            }

            # é…ç½®Qwen API (æ—§ç‰ˆæ ¼å¼)
            qwen_config = {
                "model": "qwen-max",
                "api_key": self.qwen_api_key,
                "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
                "api_type": "open_ai"
            }

            # 1. ç»Ÿè®¡ä¸“å®¶ (ä½¿ç”¨DeepSeek)
            statistician = AssistantAgent(
                name="Statistician",
                llm_config={
                    "config_list": [deepseek_config],
                    "temperature": self.config.temperature,
                    "request_timeout": 120,
                },
                system_message="""ä½ æ˜¯ç»Ÿè®¡å­¦å®¶ä¸“å®¶ï¼Œä¸“æ³¨äºæ—¶é—´åºåˆ—æ•°æ®åˆ†æã€‚
                åˆ†ææ•°æ®è¶‹åŠ¿ã€å­£èŠ‚æ€§ã€å‘¨æœŸæ€§ï¼Œè¯„ä¼°é¢„æµ‹çš„ç»Ÿè®¡æœ‰æ•ˆæ€§ã€‚
                æä¾›åŸºäºç»Ÿè®¡å­¦çš„æ”¹è¿›å»ºè®®ã€‚"""
            )
            agents["statistician"] = statistician

            # 2. é¢†åŸŸä¸“å®¶ (ä½¿ç”¨Qwen)
            domain_expert = AssistantAgent(
                name="DomainExpert",
                llm_config={
                    "config_list": [qwen_config],
                    "temperature": self.config.temperature,
                    "request_timeout": 120,
                },
                system_message="""ä½ æ˜¯æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸä¸“å®¶ã€‚
                åŸºäºå®é™…ç»éªŒåˆ¤æ–­é¢„æµ‹çš„åˆç†æ€§ï¼Œè¯†åˆ«å¼‚å¸¸æ¨¡å¼ã€‚
                æä¾›åŸºäºé¢†åŸŸçŸ¥è¯†çš„æ”¹è¿›å»ºè®®ã€‚"""
            )
            agents["domain_expert"] = domain_expert

            # 3. æ¨¡å‹ä¸“å®¶ (ä½¿ç”¨DeepSeek)
            model_expert = AssistantAgent(
                name="ModelExpert",
                llm_config={
                    "config_list": [deepseek_config],
                    "temperature": self.config.temperature,
                    "request_timeout": 120,
                },
                system_message="""ä½ æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸“å®¶ã€‚
                åˆ†ææ¨¡å‹æ¶æ„å¯¹é¢„æµ‹çš„å½±å“ï¼Œå»ºè®®å‚æ•°è°ƒæ•´ã€‚
                è¯„ä¼°ä¸åŒé¢„æµ‹æŠ€æœ¯çš„é€‚ç”¨æ€§ã€‚"""
            )
            agents["model_expert"] = model_expert

            # 4. åè°ƒè€…
            coordinator = UserProxyAgent(
                name="Coordinator",
                human_input_mode="NEVER",
                max_consecutive_auto_reply=5,
                code_execution_config=False,
                system_message="ä½ æ˜¯è¾©è®ºåè°ƒè€…ï¼Œå¼•å¯¼è®¨è®ºå¹¶æ€»ç»“å…±è¯†ã€‚"
            )
            agents["coordinator"] = coordinator

            print(f"âœ… çœŸå®æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ: {list(agents.keys())}")

            # æµ‹è¯•APIè¿æ¥
            if not self.test_api_connection():
                print("âš ï¸  APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
                return self._initialize_mock_agents()

        except Exception as e:
            print(f"âŒ çœŸå®æ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ™ºèƒ½ä½“")
            return self._initialize_mock_agents()

        return agents

    def _initialize_mock_agents(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æ¨¡æ‹Ÿæ™ºèƒ½ä½“"""
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ™ºèƒ½ä½“")

        class MockAgent:
            def __init__(self, name, role):
                self.name = name
                self.role = role
                self.llm_config = {}

        agents = {
            "statistician": MockAgent("Statistician", "ç»Ÿè®¡ä¸“å®¶"),
            "domain_expert": MockAgent("DomainExpert", "é¢†åŸŸä¸“å®¶"),
            "model_expert": MockAgent("ModelExpert", "æ¨¡å‹ä¸“å®¶"),
            "coordinator": MockAgent("Coordinator", "åè°ƒè€…")
        }

        return agents

    def test_api_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        if self.llm_mode == "mock":
            return True

        try:
            # æµ‹è¯•DeepSeekè¿æ¥
            if self.deepseek_api_key:
                print("ğŸ” æµ‹è¯•DeepSeek APIè¿æ¥...")
                import openai

                # ä½¿ç”¨æ—§ç‰ˆOpenAI API
                openai.api_key = self.deepseek_api_key
                openai.api_base = "https://api.deepseek.com/v1"

                try:
                    response = openai.ChatCompletion.create(
                        model="deepseek-chat",
                        messages=[{"role": "user", "content": "Hello"}],
                        max_tokens=5,
                        timeout=10
                    )
                    if response and hasattr(response, 'choices'):
                        print("âœ… DeepSeek APIè¿æ¥æ­£å¸¸")
                        return True
                except Exception as e:
                    print(f"âŒ DeepSeek APIè¿æ¥å¤±è´¥: {e}")

            return False

        except Exception as e:
            print(f"âŒ APIè¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            return False

    def start_debate(self, topic: str, context: Dict[str, Any], question: str) -> DebateResult:
        """å¯åŠ¨å¤šæ™ºèƒ½ä½“è¾©è®º"""
        print(f"\nğŸ¤– å¯åŠ¨{self.llm_mode.upper()}æ¨¡å¼è¾©è®º: {topic}")

        result = DebateResult()

        if self.llm_mode == "mock" or not AUTO_GEN_AVAILABLE:
            # æ¨¡æ‹Ÿè¾©è®º
            return self._simulate_debate(topic, context, question)

        try:
            # å‡†å¤‡è¾©è®ºä¸Šä¸‹æ–‡
            debate_context = self._prepare_debate_context(context)

            # åˆ›å»ºç¾¤èŠ
            agent_list = [
                self.agents["statistician"],
                self.agents["domain_expert"],
                self.agents["model_expert"],
                self.agents["coordinator"]
            ]

            group_chat = GroupChat(
                agents=agent_list,
                messages=[],
                max_round=self.config.debate_rounds * 2,
                allow_repeat_speaker=False
            )

            manager = GroupChatManager(
                groupchat=group_chat,
                llm_config=self.agents["model_expert"].llm_config
            )

            # å¯åŠ¨è®¨è®º
            initial_message = f"""è¾©è®ºä¸»é¢˜ï¼š{topic}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{debate_context}

è®¨è®ºé—®é¢˜ï¼š
{question}

è¯·å„ä½ä¸“å®¶åŸºäºä¸“ä¸šé¢†åŸŸå‘è¡¨æ„è§ï¼Œæœ€åå½¢æˆå…±è¯†ã€‚"""

            # ä½¿ç”¨æ—§ç‰ˆAutoGençš„èŠå¤©æ–¹å¼
            chat_result = self.agents["coordinator"].initiate_chat(
                manager,
                message=initial_message,
                max_turns=self.config.debate_rounds * 2
            )

            # å¤„ç†ç»“æœ
            if hasattr(chat_result, 'chat_history'):
                result.raw_messages = chat_result.chat_history

                # æå–å…±è¯†
                consensus, recommendations = self._extract_consensus(chat_result.chat_history)
                result.consensus = consensus
                result.recommendations = recommendations

                # ä¿å­˜æ—¥å¿—
                for msg in chat_result.chat_history:
                    if isinstance(msg, dict) and 'content' in msg:
                        speaker = msg.get('name', 'Unknown')
                        content = msg['content']
                        result.debate_log.append(f"{speaker}: {content[:100]}...")

            print(f"âœ… çœŸå®è¾©è®ºå®Œæˆï¼å…±è¯†: {result.consensus[:50]}...")

        except Exception as e:
            print(f"âŒ çœŸå®è¾©è®ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print("ğŸ”„ åˆ‡æ¢åˆ°æ¨¡æ‹Ÿè¾©è®º")
            return self._simulate_debate(topic, context, question)

        # å­˜å‚¨åˆ°è®°å¿†é“¶è¡Œ
        if self.memory_bank:
            self.memory_bank.store_experience({
                "type": "real_debate",
                "timestamp": datetime.now().isoformat(),
                "topic": topic,
                "llm_mode": self.llm_mode,
                "consensus": result.consensus,
                "recommendations": result.recommendations
            })

        return result

    def _prepare_debate_context(self, context: Dict[str, Any]) -> str:
        """å‡†å¤‡è¾©è®ºä¸Šä¸‹æ–‡"""
        lines = []

        if "data_description" in context:
            lines.append(f"æ•°æ®æè¿°: {context['data_description']}")

        if "historical_stats" in context:
            lines.append("å†å²æ•°æ®ç»Ÿè®¡:")
            for k, v in context["historical_stats"].items():
                lines.append(f"  - {k}: {v}")

        if "base_prediction_stats" in context:
            lines.append("é¢„æµ‹ç»“æœç»Ÿè®¡:")
            for k, v in context["base_prediction_stats"].items():
                lines.append(f"  - {k}: {v}")

        if "model_info" in context:
            lines.append("æ¨¡å‹ä¿¡æ¯:")
            for k, v in context["model_info"].items():
                lines.append(f"  - {k}: {v}")

        return "\n".join(lines)

    def _extract_consensus(self, chat_history: List) -> tuple:
        """ä»èŠå¤©å†å²æå–å…±è¯†"""
        if not chat_history:
            return "æœªè¾¾æˆå…±è¯†", []

        # æŸ¥æ‰¾åŒ…å«æ€»ç»“çš„æ¶ˆæ¯
        last_messages = chat_history[-5:] if len(chat_history) >= 5 else chat_history
        consensus_parts = []
        recommendations = []

        for msg in last_messages:
            if isinstance(msg, dict):
                content = msg.get('content', '')
                if content:
                    lower_content = content.lower()

                    # è¯†åˆ«æ€»ç»“æ€§å†…å®¹
                    summary_keywords = ['æ€»ç»“', 'å…±è¯†', 'ç»“è®º', 'å»ºè®®', 'å»ºè®®å¦‚ä¸‹', 'recommend', 'conclusion']
                    if any(kw in lower_content for kw in summary_keywords):
                        consensus_parts.append(content)

                    # æå–å»ºè®®åˆ—è¡¨
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line.startswith(('1.', '2.', '3.', '-', 'â€¢', 'å»ºè®®', 'recommend')):
                            recommendations.append(line)

        # ç”Ÿæˆå…±è¯†
        if consensus_parts:
            consensus = " ".join(consensus_parts[-2:])  # å–æœ€åä¸¤ä¸ªéƒ¨åˆ†
        else:
            # ä½¿ç”¨æœ€åçš„æ¶ˆæ¯
            last_contents = [m.get('content', '') for m in last_messages if isinstance(m, dict)]
            consensus = " ".join(last_contents[-2:])

        # é™åˆ¶é•¿åº¦
        if len(consensus) > 300:
            consensus = consensus[:297] + "..."

        # å¦‚æœæ²¡æœ‰å»ºè®®ï¼Œç”Ÿæˆé»˜è®¤çš„
        if not recommendations:
            recommendations = [
                "å»ºè®®å¯¹é¢„æµ‹è¶‹åŠ¿è¿›è¡Œå°å¹…è°ƒæ•´",
                "å¢åŠ æ•°æ®å¹³æ»‘å¤„ç†",
                "ä¼˜åŒ–æ¨¡å‹æ­£åˆ™åŒ–å‚æ•°"
            ]

        return consensus, recommendations[:5]

    def _simulate_debate(self, topic: str, context: Dict[str, Any], question: str) -> DebateResult:
        """æ¨¡æ‹Ÿè¾©è®º"""
        result = DebateResult()

        # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæ™ºèƒ½æ¨¡æ‹Ÿ
        pred_stats = context.get("base_prediction_stats", {})

        if pred_stats:
            mean_val = pred_stats.get("mean", 1.0)
            std_val = pred_stats.get("std", 0.1)

            if std_val > 0.5:
                result.consensus = "é¢„æµ‹æ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®å¢å¼ºæ¨¡å‹ç¨³å®šæ€§ã€‚"
                result.recommendations = [
                    f"åº”ç”¨æ»‘åŠ¨å¹³å‡å¹³æ»‘ï¼ˆçª—å£å¤§å°å»ºè®®: {int(10 / std_val)}ï¼‰",
                    f"å¢åŠ L2æ­£åˆ™åŒ–æƒé‡: {std_val * 0.2:.3f}",
                    "è€ƒè™‘ä½¿ç”¨æ›´é•¿å†å²æ•°æ®è®­ç»ƒ"
                ]
            else:
                result.consensus = "é¢„æµ‹ç›¸å¯¹ç¨³å®šï¼Œå¯ä¼˜åŒ–æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚"
                result.recommendations = [
                    "å¢åŠ ç¥ç»ç½‘ç»œå±‚æ•°",
                    "å°è¯•ä¸åŒæ¿€æ´»å‡½æ•°",
                    "è°ƒæ•´å­¦ä¹ ç‡è°ƒåº¦"
                ]
        else:
            result.consensus = "ä¸“å®¶å»ºè®®ä»å¤šä¸ªè§’åº¦ä¼˜åŒ–é¢„æµ‹æ¨¡å‹ã€‚"
            result.recommendations = [
                "è°ƒæ•´è¶‹åŠ¿é¢„æµ‹",
                "ä¼˜åŒ–è¶…å‚æ•°",
                "å¢å¼ºç‰¹å¾å·¥ç¨‹"
            ]

        result.debate_log = [
            "Statistician: åˆ†æäº†æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§",
            "DomainExpert: æä¾›äº†é¢†åŸŸç»éªŒå»ºè®®",
            "ModelExpert: å»ºè®®äº†æ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ"
        ]

        print(f"âœ… æ¨¡æ‹Ÿè¾©è®ºå®Œæˆ")

        # å­˜å‚¨åˆ°è®°å¿†é“¶è¡Œ
        if self.memory_bank:
            self.memory_bank.store_experience({
                "type": "mock_debate",
                "timestamp": datetime.now().isoformat(),
                "topic": topic,
                "consensus": result.consensus,
                "recommendations": result.recommendations
            })

        return result