"""
æ™ºèƒ½ä½“è®°å¿†åº“ - å­˜å‚¨å’Œæ£€ç´¢å¯¹è¯å†å²ã€ç»éªŒå’ŒçŸ¥è¯†
æ”¯æŒé•¿æœŸè®°å¿†ã€çŸ­æœŸè®°å¿†å’Œå·¥ä½œè®°å¿†
"""
import json
import pickle
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
import hashlib
import heapq
from collections import defaultdict, deque
import logging
from pathlib import Path
import sqlite3
from contextlib import contextmanager
import faiss  # å‘é‡æ•°æ®åº“


class MemoryType(Enum):
    """è®°å¿†ç±»å‹"""
    EPISODIC = "episodic"  # æƒ…èŠ‚è®°å¿†ï¼ˆå…·ä½“äº‹ä»¶ï¼‰
    SEMANTIC = "semantic"  # è¯­ä¹‰è®°å¿†ï¼ˆçŸ¥è¯†äº‹å®ï¼‰
    PROCEDURAL = "procedural"  # ç¨‹åºè®°å¿†ï¼ˆæŠ€èƒ½æ–¹æ³•ï¼‰
    WORKING = "working"  # å·¥ä½œè®°å¿†ï¼ˆå½“å‰ä»»åŠ¡ï¼‰
    ASSOCIATIVE = "associative"  # å…³è”è®°å¿†ï¼ˆå…³ç³»ç½‘ç»œï¼‰


class MemoryPriority(Enum):
    """è®°å¿†ä¼˜å…ˆçº§"""
    CRITICAL = 5  # å…³é”®è®°å¿†ï¼ˆå¿…é¡»è®°ä½ï¼‰
    HIGH = 4  # é«˜ä¼˜å…ˆçº§
    MEDIUM = 3  # ä¸­ä¼˜å…ˆçº§
    LOW = 2  # ä½ä¼˜å…ˆçº§
    TRIVIAL = 1  # çç¢è®°å¿†ï¼ˆå¯é—å¿˜ï¼‰


@dataclass
class MemoryNode:
    """è®°å¿†èŠ‚ç‚¹"""
    memory_id: str
    memory_type: MemoryType
    content: Dict[str, Any]
    embedding: Optional[np.ndarray] = None  # å‘é‡è¡¨ç¤º
    timestamp: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    access_count: int = 1
    priority: MemoryPriority = MemoryPriority.MEDIUM
    decay_rate: float = 0.1  # é—å¿˜é€Ÿç‡ï¼ˆæ¯å¤©ï¼‰
    associations: List[str] = field(default_factory=list)  # å…³è”è®°å¿†ID
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def strength(self) -> float:
        """è®¡ç®—è®°å¿†å¼ºåº¦"""
        # åŸºç¡€å¼ºåº¦ = ä¼˜å…ˆçº§ + è®¿é—®é¢‘ç‡ - æ—¶é—´è¡°å‡
        time_elapsed = (datetime.now() - self.timestamp).total_seconds() / 86400  # å¤©

        # è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿è°ƒæ•´
        base_strength = self.priority.value
        frequency_boost = np.log1p(self.access_count) * 0.5
        time_decay = np.exp(-self.decay_rate * time_elapsed)

        strength = (base_strength + frequency_boost) * time_decay

        return max(0.0, min(10.0, strength))

    @property
    def relevance(self) -> float:
        """è®¡ç®—è¿‘æœŸç›¸å…³æ€§"""
        time_elapsed = (datetime.now() - self.last_accessed).total_seconds() / 3600  # å°æ—¶
        return np.exp(-0.1 * time_elapsed) * self.strength


@dataclass
class MemoryQuery:
    """è®°å¿†æŸ¥è¯¢"""
    query_text: Optional[str] = None
    query_embedding: Optional[np.ndarray] = None
    memory_type: Optional[MemoryType] = None
    time_range: Optional[Tuple[datetime, datetime]] = None
    priority_filter: Optional[MemoryPriority] = None
    min_strength: float = 0.0
    max_results: int = 10
    similarity_threshold: float = 0.7


@dataclass
class MemoryRetrieval:
    """è®°å¿†æ£€ç´¢ç»“æœ"""
    memories: List[MemoryNode]
    scores: List[float]
    query: MemoryQuery
    retrieval_time: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'memories': [
                {
                    'id': mem.memory_id,
                    'type': mem.memory_type.value,
                    'content': mem.content,
                    'strength': mem.strength,
                    'relevance': mem.relevance,
                    'timestamp': mem.timestamp.isoformat()
                }
                for mem in self.memories
            ],
            'scores': self.scores,
            'query': asdict(self.query) if self.query else None,
            'retrieval_time': self.retrieval_time.isoformat()
        }


class VectorIndex:
    """å‘é‡ç´¢å¼•ç®¡ç†å™¨"""

    def __init__(self, dimension: int = 384):  # ä½¿ç”¨æ ‡å‡†åµŒå…¥ç»´åº¦
        self.dimension = dimension
        self.index = faiss.IndexFlatL2(dimension)
        self.id_to_index = {}  # memory_id -> indexä½ç½®
        self.index_to_id = {}  # indexä½ç½® -> memory_id
        self.next_index = 0

    def add_memory(self, memory_id: str, embedding: np.ndarray):
        """æ·»åŠ è®°å¿†å‘é‡"""
        if memory_id in self.id_to_index:
            # æ›´æ–°ç°æœ‰å‘é‡
            idx = self.id_to_index[memory_id]
            self.index.remove_ids(np.array([idx]))
            self.index.add(embedding.reshape(1, -1))
        else:
            # æ·»åŠ æ–°å‘é‡
            self.index.add(embedding.reshape(1, -1))
            self.id_to_index[memory_id] = self.next_index
            self.index_to_id[self.next_index] = memory_id
            self.next_index += 1

    def search(self, query_embedding: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:
        """æœç´¢ç›¸ä¼¼è®°å¿†"""
        query_embedding = query_embedding.reshape(1, -1)
        distances, indices = self.index.search(query_embedding, k)

        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx in self.index_to_id and idx != -1:
                memory_id = self.index_to_id[idx]
                # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
                similarity = 1.0 / (1.0 + dist)
                results.append((memory_id, similarity))

        return results

    def remove_memory(self, memory_id: str):
        """ç§»é™¤è®°å¿†"""
        if memory_id in self.id_to_index:
            idx = self.id_to_index[memory_id]
            self.index.remove_ids(np.array([idx]))
            del self.id_to_index[memory_id]
            del self.index_to_id[idx]

    def save(self, path: str):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, path)

        # ä¿å­˜æ˜ å°„å…³ç³»
        mapping = {
            'id_to_index': self.id_to_index,
            'index_to_id': self.index_to_id,
            'next_index': self.next_index
        }

        with open(f"{path}.mapping", 'wb') as f:
            pickle.dump(mapping, f)

    def load(self, path: str):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(path)

        # åŠ è½½æ˜ å°„å…³ç³»
        with open(f"{path}.mapping", 'rb') as f:
            mapping = pickle.load(f)

        self.id_to_index = mapping['id_to_index']
        self.index_to_id = mapping['index_to_id']
        self.next_index = mapping['next_index']


class EmbeddingModel:
    """åµŒå…¥æ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""

    def __init__(self, dimension: int = 384):
        self.dimension = dimension

        # é¢„å®šä¹‰çš„ç±»åˆ«å‘é‡ï¼ˆå®é™…åº”ä½¿ç”¨BERTç­‰æ¨¡å‹ï¼‰
        self.category_vectors = {
            'model_architecture': np.random.randn(dimension),
            'hyperparameter': np.random.randn(dimension),
            'performance_metric': np.random.randn(dimension),
            'data_pattern': np.random.randn(dimension),
            'training_strategy': np.random.randn(dimension),
            'error_analysis': np.random.randn(dimension)
        }

    def encode(self, text: str) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…é¡¹ç›®åº”ä½¿ç”¨çœŸå®åµŒå…¥æ¨¡å‹
        # è¿™é‡Œä½¿ç”¨åŸºäºå…³é”®è¯çš„ç®€å•å‘é‡

        vector = np.zeros(self.dimension)

        # å…³é”®è¯åŒ¹é…
        keywords = {
            'è°±é—¨æ§': 'model_architecture',
            'æ‹‰æ™®æ‹‰æ–¯': 'model_architecture',
            'TCN': 'model_architecture',
            'MSE': 'performance_metric',
            'MAE': 'performance_metric',
            'å­¦ä¹ ç‡': 'hyperparameter',
            'æ­£åˆ™åŒ–': 'training_strategy',
            'è¿‡æ‹Ÿåˆ': 'error_analysis',
            'å¹³ç¨³æ€§': 'data_pattern'
        }

        # åˆå¹¶ç›¸å…³ç±»åˆ«å‘é‡
        matched_categories = set()
        for keyword, category in keywords.items():
            if keyword in text:
                matched_categories.add(category)

        if matched_categories:
            for category in matched_categories:
                vector += self.category_vectors[category]
            vector /= len(matched_categories)
        else:
            # éšæœºå‘é‡ä½œä¸ºåå¤‡
            vector = np.random.randn(self.dimension)
            vector = vector / np.linalg.norm(vector)

        # æ·»åŠ æ–‡æœ¬é•¿åº¦ç‰¹å¾
        length_feature = min(len(text) / 1000, 1.0)
        vector[:10] += length_feature * 0.1

        # å½’ä¸€åŒ–
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm

        return vector

    def encode_dict(self, data: Dict[str, Any]) -> np.ndarray:
        """ç¼–ç å­—å…¸æ•°æ®ä¸ºå‘é‡"""
        # å°†å­—å…¸è½¬æ¢ä¸ºæ–‡æœ¬
        text = json.dumps(data, ensure_ascii=False)
        return self.encode(text)


class SQLiteMemoryStore:
    """SQLiteè®°å¿†å­˜å‚¨"""

    def __init__(self, db_path: str = "./memory.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(exist_ok=True)
        self._init_database()

    @contextmanager
    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()

    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with self._get_connection() as conn:
            # åˆ›å»ºè®°å¿†è¡¨
            conn.execute("""
                         CREATE TABLE IF NOT EXISTS memories
                         (
                             memory_id
                             TEXT
                             PRIMARY
                             KEY,
                             memory_type
                             TEXT
                             NOT
                             NULL,
                             content
                             TEXT
                             NOT
                             NULL,
                             embedding
                             BLOB,
                             timestamp
                             DATETIME
                             NOT
                             NULL,
                             last_accessed
                             DATETIME
                             NOT
                             NULL,
                             access_count
                             INTEGER
                             DEFAULT
                             1,
                             priority
                             INTEGER
                             DEFAULT
                             3,
                             decay_rate
                             REAL
                             DEFAULT
                             0.1,
                             associations
                             TEXT,
                             metadata
                             TEXT,
                             created_at
                             DATETIME
                             DEFAULT
                             CURRENT_TIMESTAMP
                         )
                         """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_type ON memories (memory_type)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON memories (timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_priority ON memories (priority)")

            conn.commit()

    def save_memory(self, memory: MemoryNode):
        """ä¿å­˜è®°å¿†"""
        with self._get_connection() as conn:
            # è½¬æ¢æ•°æ®
            content_json = json.dumps(memory.content, ensure_ascii=False)
            associations_json = json.dumps(memory.associations, ensure_ascii=False)
            metadata_json = json.dumps(memory.metadata, ensure_ascii=False)

            embedding_blob = None
            if memory.embedding is not None:
                embedding_blob = memory.embedding.tobytes()

            # æ’å…¥æˆ–æ›´æ–°
            conn.execute("""
                INSERT OR REPLACE INTO memories 
                (memory_id, memory_type, content, embedding, timestamp, 
                 last_accessed, access_count, priority, decay_rate, 
                 associations, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                memory.memory_id,
                memory.memory_type.value,
                content_json,
                embedding_blob,
                memory.timestamp.isoformat(),
                memory.last_accessed.isoformat(),
                memory.access_count,
                memory.priority.value,
                memory.decay_rate,
                associations_json,
                metadata_json
            ))

            conn.commit()

    def load_memory(self, memory_id: str) -> Optional[MemoryNode]:
        """åŠ è½½è®°å¿†"""
        with self._get_connection() as conn:
            cursor = conn.execute(
                "SELECT * FROM memories WHERE memory_id = ?",
                (memory_id,)
            )
            row = cursor.fetchone()

            if row is None:
                return None

            # è§£ææ•°æ®
            embedding = None
            if row['embedding']:
                embedding = np.frombuffer(row['embedding'], dtype=np.float32)

            associations = json.loads(row['associations']) if row['associations'] else []
            metadata = json.loads(row['metadata']) if row['metadata'] else {}

            memory = MemoryNode(
                memory_id=row['memory_id'],
                memory_type=MemoryType(row['memory_type']),
                content=json.loads(row['content']),
                embedding=embedding,
                timestamp=datetime.fromisoformat(row['timestamp']),
                last_accessed=datetime.fromisoformat(row['last_accessed']),
                access_count=row['access_count'],
                priority=MemoryPriority(row['priority']),
                decay_rate=row['decay_rate'],
                associations=associations,
                metadata=metadata
            )

            return memory

    def delete_memory(self, memory_id: str):
        """åˆ é™¤è®°å¿†"""
        with self._get_connection() as conn:
            conn.execute("DELETE FROM memories WHERE memory_id = ?", (memory_id,))
            conn.commit()

    def search_memories(self, query: MemoryQuery) -> List[MemoryNode]:
        """æœç´¢è®°å¿†ï¼ˆåŸºäºå…ƒæ•°æ®ï¼‰"""
        with self._get_connection() as conn:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = []
            params = []

            if query.memory_type:
                conditions.append("memory_type = ?")
                params.append(query.memory_type.value)

            if query.time_range:
                start_time, end_time = query.time_range
                conditions.append("timestamp BETWEEN ? AND ?")
                params.extend([start_time.isoformat(), end_time.isoformat()])

            if query.priority_filter:
                conditions.append("priority >= ?")
                params.append(query.priority_filter.value)

            # æ‰§è¡ŒæŸ¥è¯¢
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            sql = f"""
                SELECT * FROM memories 
                WHERE {where_clause}
                ORDER BY last_accessed DESC
                LIMIT ?
            """
            params.append(query.max_results)

            cursor = conn.execute(sql, params)
            rows = cursor.fetchall()

            # è½¬æ¢ä¸ºMemoryNodeå¯¹è±¡
            memories = []
            for row in rows:
                # è®¡ç®—å¼ºåº¦ï¼ˆç®€åŒ–ï¼‰
                access_count = row['access_count']
                priority = MemoryPriority(row['priority'])

                # è¿‡æ»¤ä½å¼ºåº¦è®°å¿†
                strength = priority.value + np.log1p(access_count)
                if strength < query.min_strength:
                    continue

                # è§£ææ•°æ®
                embedding = None
                if row['embedding']:
                    embedding = np.frombuffer(row['embedding'], dtype=np.float32)

                associations = json.loads(row['associations']) if row['associations'] else []
                metadata = json.loads(row['metadata']) if row['metadata'] else {}

                memory = MemoryNode(
                    memory_id=row['memory_id'],
                    memory_type=MemoryType(row['memory_type']),
                    content=json.loads(row['content']),
                    embedding=embedding,
                    timestamp=datetime.fromisoformat(row['timestamp']),
                    last_accessed=datetime.fromisoformat(row['last_accessed']),
                    access_count=row['access_count'],
                    priority=priority,
                    decay_rate=row['decay_rate'],
                    associations=associations,
                    metadata=metadata
                )

                memories.append(memory)

            return memories

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        with self._get_connection() as conn:
            stats = {}

            # æ€»æ•°
            cursor = conn.execute("SELECT COUNT(*) as total FROM memories")
            stats['total_memories'] = cursor.fetchone()['total']

            # æŒ‰ç±»å‹ç»Ÿè®¡
            cursor = conn.execute("""
                                  SELECT memory_type, COUNT(*) as count
                                  FROM memories
                                  GROUP BY memory_type
                                  """)
            stats['by_type'] = {row['memory_type']: row['count'] for row in cursor.fetchall()}

            # æŒ‰ä¼˜å…ˆçº§ç»Ÿè®¡
            cursor = conn.execute("""
                                  SELECT priority, COUNT(*) as count
                                  FROM memories
                                  GROUP BY priority
                                  """)
            stats['by_priority'] = {row['priority']: row['count'] for row in cursor.fetchall()}

            # æ—¶é—´èŒƒå›´
            cursor = conn.execute("""
                                  SELECT MIN(timestamp) as oldest, MAX(timestamp) as newest
                                  FROM memories
                                  """)
            row = cursor.fetchone()
            stats['time_range'] = {
                'oldest': row['oldest'],
                'newest': row['newest']
            }

            return stats


class MemoryBank:
    """
    æ™ºèƒ½ä½“è®°å¿†åº“

    ç‰¹ç‚¹ï¼š
    1. å¤šç±»å‹è®°å¿†å­˜å‚¨ï¼ˆæƒ…èŠ‚ã€è¯­ä¹‰ã€ç¨‹åºç­‰ï¼‰
    2. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
    3. SQLiteæŒä¹…åŒ–å­˜å‚¨
    4. è®°å¿†å¼ºåº¦å’Œé—å¿˜æœºåˆ¶
    5. å…³è”è®°å¿†ç½‘ç»œ
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # è®°å¿†å­˜å‚¨
        self.memory_store = SQLiteMemoryStore(
            config.get('memory', {}).get('db_path', './memory.db')
        )

        # å‘é‡ç´¢å¼•
        self.embedding_model = EmbeddingModel(
            dimension=config.get('memory', {}).get('embedding_dim', 384)
        )
        self.vector_index = VectorIndex(self.embedding_model.dimension)

        # å·¥ä½œè®°å¿†ï¼ˆçŸ­æœŸï¼‰
        self.working_memory = deque(maxlen=config.get('memory', {}).get('working_memory_size', 10))

        # åŠ è½½ç°æœ‰è®°å¿†
        self._load_existing_memories()

        self.logger.info("âœ… æ™ºèƒ½ä½“è®°å¿†åº“åˆå§‹åŒ–å®Œæˆ")

    def _load_existing_memories(self):
        """åŠ è½½ç°æœ‰è®°å¿†åˆ°å‘é‡ç´¢å¼•"""
        # åŠ è½½æ‰€æœ‰è®°å¿†
        query = MemoryQuery(max_results=1000)
        memories = self.memory_store.search_memories(query)

        # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
        for memory in memories:
            if memory.embedding is not None:
                self.vector_index.add_memory(memory.memory_id, memory.embedding)

        self.logger.info(f"ğŸ“š åŠ è½½ {len(memories)} æ¡ç°æœ‰è®°å¿†")

    def store(self, content: Dict[str, Any],
              memory_type: MemoryType = MemoryType.EPISODIC,
              priority: MemoryPriority = MemoryPriority.MEDIUM,
              associations: List[str] = None,
              metadata: Dict[str, Any] = None) -> str:
        """
        å­˜å‚¨è®°å¿†

        Args:
            content: è®°å¿†å†…å®¹
            memory_type: è®°å¿†ç±»å‹
            priority: è®°å¿†ä¼˜å…ˆçº§
            associations: å…³è”è®°å¿†IDåˆ—è¡¨
            metadata: å…ƒæ•°æ®

        Returns:
            è®°å¿†ID
        """
        # ç”Ÿæˆå”¯ä¸€ID
        memory_id = hashlib.md5(
            f"{json.dumps(content)}{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]

        # åˆ›å»ºåµŒå…¥å‘é‡
        embedding = self.embedding_model.encode_dict(content)

        # åˆ›å»ºè®°å¿†èŠ‚ç‚¹
        memory = MemoryNode(
            memory_id=memory_id,
            memory_type=memory_type,
            content=content,
            embedding=embedding,
            priority=priority,
            associations=associations or [],
            metadata=metadata or {}
        )

        # ä¿å­˜åˆ°å­˜å‚¨
        self.memory_store.save_memory(memory)

        # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
        self.vector_index.add_memory(memory_id, embedding)

        # æ·»åŠ åˆ°å·¥ä½œè®°å¿†
        self.working_memory.append(memory)

        self.logger.debug(f"ğŸ’¾ å­˜å‚¨è®°å¿†: {memory_id}, ç±»å‹: {memory_type.value}")

        return memory_id

    def retrieve(self, query: MemoryQuery) -> MemoryRetrieval:
        """
        æ£€ç´¢è®°å¿†

        Args:
            query: è®°å¿†æŸ¥è¯¢

        Returns:
            æ£€ç´¢ç»“æœ
        """
        memories = []
        scores = []

        # 1. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
        if query.query_embedding is not None:
            vector_results = self.vector_index.search(query.query_embedding, query.max_results)

            for memory_id, similarity in vector_results:
                if similarity >= query.similarity_threshold:
                    memory = self.memory_store.load_memory(memory_id)
                    if memory:
                        # æ›´æ–°è®¿é—®ä¿¡æ¯
                        memory.last_accessed = datetime.now()
                        memory.access_count += 1
                        self.memory_store.save_memory(memory)

                        memories.append(memory)
                        scores.append(similarity)

        # 2. å…ƒæ•°æ®æ£€ç´¢ï¼ˆå¦‚æœå‘é‡æ£€ç´¢ç»“æœä¸è¶³ï¼‰
        if len(memories) < query.max_results:
            metadata_results = self.memory_store.search_memories(query)

            for memory in metadata_results:
                if memory.memory_id not in [m.memory_id for m in memories]:
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆåŸºäºæ—¶é—´è¡°å‡å’Œå¼ºåº¦ï¼‰
                    time_decay = np.exp(-0.1 * (datetime.now() - memory.timestamp).total_seconds() / 3600)
                    relevance = memory.strength * time_decay

                    memories.append(memory)
                    scores.append(relevance)

        # 3. é™åˆ¶ç»“æœæ•°é‡
        if len(memories) > query.max_results:
            # æŒ‰åˆ†æ•°æ’åº
            sorted_pairs = sorted(zip(scores, memories), reverse=True)
            scores, memories = zip(*sorted_pairs[:query.max_results])
            scores, memories = list(scores), list(memories)

        return MemoryRetrieval(
            memories=memories,
            scores=scores,
            query=query
        )

    def retrieve_by_text(self, text: str, **kwargs) -> MemoryRetrieval:
        """é€šè¿‡æ–‡æœ¬æ£€ç´¢è®°å¿†"""
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_embedding = self.embedding_model.encode(text)

        # åˆ›å»ºæŸ¥è¯¢
        query = MemoryQuery(
            query_text=text,
            query_embedding=query_embedding,
            **kwargs
        )

        return self.retrieve(query)

    def retrieve_by_context(self, context: Dict[str, Any], **kwargs) -> MemoryRetrieval:
        """é€šè¿‡ä¸Šä¸‹æ–‡æ£€ç´¢è®°å¿†"""
        # ç¼–ç ä¸Šä¸‹æ–‡
        query_embedding = self.embedding_model.encode_dict(context)

        # åˆ›å»ºæŸ¥è¯¢
        query = MemoryQuery(
            query_embedding=query_embedding,
            **kwargs
        )

        return self.retrieve(query)

    def retrieve_similar_experiences(self, current_situation: Dict[str, Any],
                                     max_results: int = 5) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸ä¼¼ç»éªŒ"""
        # æŸ¥æ‰¾ç±»ä¼¼çš„å†å²æƒ…å¢ƒ
        retrieval = self.retrieve_by_context(
            current_situation,
            memory_type=MemoryType.EPISODIC,
            max_results=max_results
        )

        # æå–ç»éªŒæ•™è®­
        experiences = []
        for memory, score in zip(retrieval.memories, retrieval.scores):
            content = memory.content
            if 'outcome' in content and 'lessons' in content:
                experience = {
                    'situation': content.get('context', {}),
                    'action_taken': content.get('action', ''),
                    'outcome': content['outcome'],
                    'lessons': content['lessons'],
                    'similarity': score,
                    'timestamp': memory.timestamp
                }
                experiences.append(experience)

        return experiences

    def create_association(self, memory_id1: str, memory_id2: str,
                           relationship: str = "related"):
        """åˆ›å»ºè®°å¿†å…³è”"""
        memory1 = self.memory_store.load_memory(memory_id1)
        memory2 = self.memory_store.load_memory(memory_id2)

        if not memory1 or not memory2:
            return False

        # æ·»åŠ åˆ°å…³è”åˆ—è¡¨
        if memory_id2 not in memory1.associations:
            memory1.associations.append(memory_id2)

        if memory_id1 not in memory2.associations:
            memory2.associations.append(memory_id1)

        # æ›´æ–°å…ƒæ•°æ®
        memory1.metadata.setdefault('associations', {})[memory_id2] = {
            'relationship': relationship,
            'created_at': datetime.now().isoformat()
        }

        memory2.metadata.setdefault('associations', {})[memory_id1] = {
            'relationship': relationship,
            'created_at': datetime.now().isoformat()
        }

        # ä¿å­˜æ›´æ–°
        self.memory_store.save_memory(memory1)
        self.memory_store.save_memory(memory2)

        self.logger.debug(f"ğŸ”— åˆ›å»ºå…³è”: {memory_id1} <-> {memory_id2}")

        return True

    def get_association_network(self, memory_id: str, depth: int = 2) -> Dict[str, Any]:
        """è·å–å…³è”ç½‘ç»œ"""
        memory = self.memory_store.load_memory(memory_id)
        if not memory:
            return {}

        network = {
            'center': {
                'id': memory.memory_id,
                'type': memory.memory_type.value,
                'content_preview': str(memory.content)[:100]
            },
            'associations': []
        }

        visited = set([memory_id])
        queue = [(memory_id, 0)]  # (memory_id, depth)

        while queue:
            current_id, current_depth = queue.pop(0)

            if current_depth >= depth:
                continue

            current_memory = self.memory_store.load_memory(current_id)
            if not current_memory:
                continue

            for assoc_id in current_memory.associations:
                if assoc_id not in visited:
                    visited.add(assoc_id)

                    assoc_memory = self.memory_store.load_memory(assoc_id)
                    if assoc_memory:
                        network['associations'].append({
                            'id': assoc_id,
                            'type': assoc_memory.memory_type.value,
                            'content_preview': str(assoc_memory.content)[:100],
                            'depth': current_depth + 1,
                            'relationship': assoc_memory.metadata.get('associations', {})
                            .get(current_id, {})
                            .get('relationship', 'unknown')
                        })

                        if current_depth + 1 < depth:
                            queue.append((assoc_id, current_depth + 1))

        return network

    def consolidate_memories(self):
        """è®°å¿†å·©å›º - åŠ å¼ºé‡è¦è®°å¿†ï¼Œå¼±åŒ–ä¸é‡è¦è®°å¿†"""
        # æ£€ç´¢æ‰€æœ‰è®°å¿†
        query = MemoryQuery(max_results=1000)
        memories = self.memory_store.search_memories(query)

        consolidated_count = 0
        forgotten_count = 0

        for memory in memories:
            current_strength = memory.strength

            # æ ¹æ®è®°å¿†å¼ºåº¦å†³å®šå¤„ç†æ–¹å¼
            if current_strength < 1.0:  # å¾ˆå¼±çš„è®°å¿†
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é—å¿˜
                if memory.priority == MemoryPriority.TRIVIAL:
                    self.forget(memory.memory_id)
                    forgotten_count += 1
                else:
                    # åŠ å¼ºè®°å¿†ï¼ˆæ¨¡æ‹Ÿç¡çœ ä¸­çš„å·©å›ºï¼‰
                    if memory.access_count < 5:
                        memory.decay_rate *= 0.9  # å‡æ…¢é—å¿˜
                        self.memory_store.save_memory(memory)
                        consolidated_count += 1

        self.logger.info(f"ğŸ”„ è®°å¿†å·©å›ºå®Œæˆ: åŠ å¼º {consolidated_count} æ¡, é—å¿˜ {forgotten_count} æ¡")

    def forget(self, memory_id: str):
        """é—å¿˜è®°å¿†"""
        # ä»å‘é‡ç´¢å¼•ç§»é™¤
        self.vector_index.remove_memory(memory_id)

        # ä»å­˜å‚¨ç§»é™¤
        self.memory_store.delete_memory(memory_id)

        # ä»å·¥ä½œè®°å¿†ç§»é™¤
        self.working_memory = deque(
            [m for m in self.working_memory if m.memory_id != memory_id],
            maxlen=self.working_memory.maxlen
        )

        self.logger.debug(f"ğŸ§¹ é—å¿˜è®°å¿†: {memory_id}")

    def cleanup_weak_memories(self, strength_threshold: float = 0.5):
        """æ¸…ç†å¼±è®°å¿†"""
        query = MemoryQuery(max_results=1000)
        memories = self.memory_store.search_memories(query)

        forgotten = []
        for memory in memories:
            if memory.strength < strength_threshold and memory.priority != MemoryPriority.CRITICAL:
                self.forget(memory.memory_id)
                forgotten.append(memory.memory_id)

        self.logger.info(f"ğŸ§¹ æ¸…ç†å¼±è®°å¿†: {len(forgotten)} æ¡")
        return forgotten

    def get_working_memory(self) -> List[MemoryNode]:
        """è·å–å·¥ä½œè®°å¿†"""
        return list(self.working_memory)

    def add_to_working_memory(self, content: Dict[str, Any],
                              memory_type: MemoryType = MemoryType.WORKING):
        """æ·»åŠ åˆ°å·¥ä½œè®°å¿†"""
        memory_id = self.store(content, memory_type, MemoryPriority.HIGH)
        memory = self.memory_store.load_memory(memory_id)

        if memory:
            self.working_memory.append(memory)

        return memory_id

    def clear_working_memory(self):
        """æ¸…ç©ºå·¥ä½œè®°å¿†"""
        self.working_memory.clear()
        self.logger.debug("ğŸ§¹ æ¸…ç©ºå·¥ä½œè®°å¿†")

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è®°å¿†åº“ç»Ÿè®¡ä¿¡æ¯"""
        store_stats = self.memory_store.get_statistics()

        stats = {
            **store_stats,
            'vector_index_size': self.vector_index.next_index,
            'working_memory_size': len(self.working_memory),
            'working_memory_capacity': self.working_memory.maxlen
        }

        return stats

    def export_memories(self, export_path: str,
                        memory_type: Optional[MemoryType] = None):
        """å¯¼å‡ºè®°å¿†"""
        query = MemoryQuery(max_results=10000)
        if memory_type:
            query.memory_type = memory_type

        memories = self.memory_store.search_memories(query)

        export_data = []
        for memory in memories:
            export_data.append({
                'id': memory.memory_id,
                'type': memory.memory_type.value,
                'content': memory.content,
                'strength': memory.strength,
                'timestamp': memory.timestamp.isoformat(),
                'last_accessed': memory.last_accessed.isoformat(),
                'access_count': memory.access_count,
                'priority': memory.priority.value,
                'associations': memory.associations,
                'metadata': memory.metadata
            })

        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

        self.logger.info(f"ğŸ’¾ å¯¼å‡º {len(export_data)} æ¡è®°å¿†åˆ° {export_path}")

    def import_memories(self, import_path: str):
        """å¯¼å…¥è®°å¿†"""
        with open(import_path, 'r', encoding='utf-8') as f:
            import_data = json.load(f)

        imported_count = 0
        for item in import_data:
            # åˆ›å»ºè®°å¿†èŠ‚ç‚¹
            memory = MemoryNode(
                memory_id=item['id'],
                memory_type=MemoryType(item['type']),
                content=item['content'],
                timestamp=datetime.fromisoformat(item['timestamp']),
                last_accessed=datetime.fromisoformat(item['last_accessed']),
                access_count=item['access_count'],
                priority=MemoryPriority(item['priority']),
                associations=item.get('associations', []),
                metadata=item.get('metadata', {})
            )

            # ç”ŸæˆåµŒå…¥å‘é‡
            memory.embedding = self.embedding_model.encode_dict(memory.content)

            # ä¿å­˜è®°å¿†
            self.memory_store.save_memory(memory)
            self.vector_index.add_memory(memory.memory_id, memory.embedding)

            imported_count += 1

        self.logger.info(f"ğŸ“¥ å¯¼å…¥ {imported_count} æ¡è®°å¿†ä» {import_path}")


# ä½¿ç”¨ç¤ºä¾‹
def main():
    """è®°å¿†åº“ä½¿ç”¨ç¤ºä¾‹"""
    import yaml

    # åŠ è½½é…ç½®
    with open("./config.yaml", "r") as f:
        config = yaml.safe_load(f)

    # åˆ›å»ºè®°å¿†åº“
    memory_bank = MemoryBank(config)

    # å­˜å‚¨ä¸€äº›è®°å¿†
    memory1_id = memory_bank.store(
        content={
            'context': 'è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°è¿‡æ‹Ÿåˆ',
            'action': 'å¢åŠ äº†æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–æƒé‡',
            'outcome': 'éªŒè¯æŸå¤±ä¸‹é™äº†15%',
            'lessons': ['æ­£åˆ™åŒ–å¯¹é˜²æ­¢è¿‡æ‹Ÿåˆæœ‰æ•ˆ', 'éœ€è¦å¹³è¡¡æ­£åˆ™åŒ–å¼ºåº¦']
        },
        memory_type=MemoryType.EPISODIC,
        priority=MemoryPriority.HIGH
    )

    memory2_id = memory_bank.store(
        content={
            'concept': 'è°±é—¨æ§æœºåˆ¶',
            'description': 'é€šè¿‡é¢‘åŸŸåˆ†æåŠ¨æ€è°ƒæ•´ç‰¹å¾é‡è¦æ€§',
            'applications': ['æ—¶åºé¢„æµ‹', 'ä¿¡å·å¤„ç†', 'å¼‚å¸¸æ£€æµ‹'],
            'parameters': {'threshold': 0.5, 'bands': 8}
        },
        memory_type=MemoryType.SEMANTIC,
        priority=MemoryPriority.CRITICAL
    )

    # åˆ›å»ºå…³è”
    memory_bank.create_association(memory1_id, memory2_id, 'application_of_concept')

    # æ£€ç´¢è®°å¿†
    retrieval = memory_bank.retrieve_by_text("è¿‡æ‹Ÿåˆ æ­£åˆ™åŒ–")
    print(f"æ£€ç´¢åˆ° {len(retrieval.memories)} æ¡ç›¸å…³è®°å¿†")

    for memory, score in zip(retrieval.memories, retrieval.scores):
        print(f"  è®°å¿†ID: {memory.memory_id}, ç›¸ä¼¼åº¦: {score:.3f}")
        print(f"  å†…å®¹: {memory.content.get('context', 'N/A')}")

    # è·å–å…³è”ç½‘ç»œ
    network = memory_bank.get_association_network(memory1_id)
    print(f"\nå…³è”ç½‘ç»œ: {len(network['associations'])} ä¸ªå…³è”")

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = memory_bank.get_statistics()
    print(f"\nè®°å¿†åº“ç»Ÿè®¡:")
    print(f"  æ€»è®°å¿†æ•°: {stats['total_memories']}")
    print(f"  å·¥ä½œè®°å¿†: {stats['working_memory_size']}/{stats['working_memory_capacity']}")


if __name__ == "__main__":
    main()