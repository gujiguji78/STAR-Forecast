"""
åŸºçº¿æ¨¡å‹å¯¹æ¯”å®éªŒ
ä¸TimeLLMè®ºæ–‡ä¸­ç›¸åŒçš„åŸºçº¿æ–¹æ³•è¿›è¡Œå…¬å¹³æ¯”è¾ƒ
åŒ…æ‹¬ï¼šPatchTST, TimesNet, DLinear, FEDformer, Autoformer, Informerç­‰
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict, field
from pathlib import Path
import json
import yaml
from datetime import datetime
import logging
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®matplotlib
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from ..models.istr import ISTRNetwork
from ..models.predictor import MultiHeadPredictor
from ..data.dataloader import ETTh1Dataset, create_dataloaders
from ..training.metrics import TimeSeriesMetrics
from ..utils.config import load_config
from ..utils.logger import setup_logger


@dataclass
class BaselineResult:
    """åŸºçº¿æ–¹æ³•ç»“æœ"""
    model_name: str
    mse: float
    mae: float
    rmse: float
    mape: float
    smape: float
    r2: float
    inference_time: float  # æ¨ç†æ—¶é—´ï¼ˆç§’/æ ·æœ¬ï¼‰
    memory_usage: float  # å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
    parameters: int  # å‚æ•°é‡
    config: Dict[str, Any]
    predictions: np.ndarray = None
    targets: np.ndarray = None


@dataclass
class ComparisonResult:
    """å¯¹æ¯”å®éªŒç»“æœ"""
    experiment_id: str
    timestamp: str
    dataset: str
    seq_len: int
    pred_len: int
    results: Dict[str, BaselineResult]
    significance_tests: Dict[str, Dict[str, float]]
    summary: Dict[str, Any]
    config: Dict[str, Any]


class BaselineModelBase:
    """åŸºçº¿æ¨¡å‹åŸºç±»"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        self.config = config
        self.device = device
        self.model = None
        self.model_name = "Base"
        self.logger = logging.getLogger(__name__)

    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        raise NotImplementedError

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒæ¨¡å‹"""
        raise NotImplementedError

    def predict(self, x: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹"""
        self.model.eval()
        with torch.no_grad():
            x = x.to(self.device)
            return self.model(x)

    def evaluate(self, test_loader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        all_predictions = []
        all_targets = []

        inference_times = []

        with torch.no_grad():
            for x, y in tqdm(test_loader, desc=f"è¯„ä¼° {self.model_name}"):
                x = x.to(self.device)
                y = y.to(self.device)

                # è®¡æ—¶
                start_time = torch.cuda.Event(enable_timing=True)
                end_time = torch.cuda.Event(enable_timing=True)

                if torch.cuda.is_available():
                    start_time.record()

                # é¢„æµ‹
                predictions = self.model(x)

                if torch.cuda.is_available():
                    end_time.record()
                    torch.cuda.synchronize()
                    inference_times.append(start_time.elapsed_time(end_time) / 1000)  # è½¬æ¢ä¸ºç§’

                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(y.cpu().numpy())

        # åˆå¹¶ç»“æœ
        predictions = np.concatenate(all_predictions, axis=0)
        targets = np.concatenate(all_targets, axis=0)

        # è®¡ç®—æŒ‡æ ‡
        metrics_calculator = TimeSeriesMetrics()
        metrics = metrics_calculator.compute(predictions, targets)

        # æ·»åŠ é¢å¤–æŒ‡æ ‡
        metrics['inference_time'] = np.mean(inference_times) if inference_times else 0
        metrics['parameters'] = sum(p.numel() for p in self.model.parameters())

        # ä¼°è®¡å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            metrics['memory_usage'] = torch.cuda.max_memory_allocated() / 1024 ** 2  # MB
        else:
            metrics['memory_usage'] = 0

        return metrics, predictions, targets


# ==================== å…·ä½“åŸºçº¿æ¨¡å‹å®ç° ====================

class DLinear(BaselineModelBase):
    """DLinearåŸºçº¿æ¨¡å‹"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__(config, device)
        self.model_name = "DLinear"

    def build_model(self):
        """æ„å»ºDLinearæ¨¡å‹"""
        seq_len = self.config['data']['seq_len']
        pred_len = self.config['data']['pred_len']
        input_dim = self.config['data']['input_dim'] if 'input_dim' in self.config['data'] else 7

        class DLinearModel(nn.Module):
            def __init__(self, seq_len, pred_len, input_dim, individual=False):
                super().__init__()
                self.seq_len = seq_len
                self.pred_len = pred_len
                self.input_dim = input_dim
                self.individual = individual

                if self.individual:
                    self.Linear = nn.ModuleList()
                    for i in range(self.input_dim):
                        self.Linear.append(nn.Linear(seq_len, pred_len))
                else:
                    self.Linear = nn.Linear(seq_len, pred_len)

            def forward(self, x):
                # x: [batch, seq_len, input_dim]
                if self.individual:
                    output = torch.zeros([x.shape[0], self.pred_len, x.shape[2]], device=x.device)
                    for i in range(self.input_dim):
                        output[:, :, i] = self.Linear[i](x[:, :, i])
                    return output[:, :, -1:]  # åªè¿”å›OTé¢„æµ‹
                else:
                    x = x.mean(dim=2)  # å¹³å‡æ‰€æœ‰ç‰¹å¾
                    return self.Linear(x).unsqueeze(-1)  # [batch, pred_len, 1]

        individual = self.config.get('dlinear', {}).get('individual', False)
        self.model = DLinearModel(seq_len, pred_len, input_dim, individual).to(self.device)

        self.logger.info(f"âœ… æ„å»º{self.model_name}æ¨¡å‹")
        self.logger.info(f"   è¾“å…¥: [{seq_len}, {input_dim}] -> è¾“å‡º: [{pred_len}, 1]")
        self.logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒDLinearæ¨¡å‹"""
        self.logger.info(f"ğŸ‹ï¸ è®­ç»ƒ{self.model_name}æ¨¡å‹ï¼Œ{epochs}è½®")

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')
        patience_counter = 0
        patience = 20

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_losses = []

            for x, y in train_loader:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(x)
                loss = criterion(predictions, y)
                loss.backward()
                optimizer.step()

                train_losses.append(loss.item())

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = []

            with torch.no_grad():
                for x, y in val_loader:
                    x, y = x.to(self.device), y.to(self.device)
                    predictions = self.model(x)
                    loss = criterion(predictions, y)
                    val_losses.append(loss.item())

            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)

            scheduler.step()

            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                self.logger.info(f"  Epoch {epoch + 1}/{epochs}: "
                                 f"Train Loss={avg_train_loss:.4f}, "
                                 f"Val Loss={avg_val_loss:.4f}")

            if patience_counter >= patience:
                self.logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break

        self.logger.info(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


class PatchTST(BaselineModelBase):
    """PatchTSTåŸºçº¿æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__(config, device)
        self.model_name = "PatchTST"

    def build_model(self):
        """æ„å»ºPatchTSTæ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        seq_len = self.config['data']['seq_len']
        pred_len = self.config['data']['pred_len']
        input_dim = self.config['data']['input_dim'] if 'input_dim' in self.config['data'] else 7

        class PatchTSTModel(nn.Module):
            def __init__(self, seq_len, pred_len, input_dim,
                         patch_len=12, stride=6, n_layers=2, d_model=128, n_heads=4):
                super().__init__()
                self.seq_len = seq_len
                self.pred_len = pred_len
                self.patch_len = patch_len
                self.stride = stride

                # è®¡ç®—patchæ•°é‡
                self.num_patches = (seq_len - patch_len) // stride + 1

                # Patch Embedding
                self.patch_embedding = nn.Linear(patch_len * input_dim, d_model)

                # Positional Encoding
                self.pos_encoder = nn.Parameter(torch.zeros(1, self.num_patches, d_model))

                # Transformer Encoder
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 4,
                    dropout=0.1, batch_first=True
                )
                self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

                # Output projection
                self.output_projection = nn.Linear(d_model * self.num_patches, pred_len)

            def forward(self, x):
                # x: [batch, seq_len, input_dim]
                batch_size = x.shape[0]

                # åˆ›å»ºpatches
                patches = []
                for i in range(self.num_patches):
                    start = i * self.stride
                    end = start + self.patch_len
                    patch = x[:, start:end, :]  # [batch, patch_len, input_dim]
                    patches.append(patch)

                # å †å patches
                patches = torch.stack(patches, dim=1)  # [batch, num_patches, patch_len, input_dim]
                patches = patches.flatten(2)  # [batch, num_patches, patch_len*input_dim]

                # Patch Embedding
                embeddings = self.patch_embedding(patches)  # [batch, num_patches, d_model]
                embeddings = embeddings + self.pos_encoder

                # Transformer
                encoded = self.transformer_encoder(embeddings)  # [batch, num_patches, d_model]

                # Flatten
                encoded = encoded.flatten(1)  # [batch, num_patches * d_model]

                # Output projection
                output = self.output_projection(encoded)  # [batch, pred_len]

                return output.unsqueeze(-1)  # [batch, pred_len, 1]

        # ä»é…ç½®è·å–å‚æ•°
        patchtst_config = self.config.get('patchtst', {})
        patch_len = patchtst_config.get('patch_len', 12)
        stride = patchtst_config.get('stride', 6)
        n_layers = patchtst_config.get('n_layers', 2)
        d_model = patchtst_config.get('d_model', 128)
        n_heads = patchtst_config.get('n_heads', 4)

        self.model = PatchTSTModel(
            seq_len, pred_len, input_dim,
            patch_len, stride, n_layers, d_model, n_heads
        ).to(self.device)

        self.logger.info(f"âœ… æ„å»º{self.model_name}æ¨¡å‹")
        self.logger.info(f"   Patché•¿åº¦: {patch_len}, æ­¥é•¿: {stride}, Patchæ•°é‡: {self.model.num_patches}")
        self.logger.info(f"   Transformerå±‚æ•°: {n_layers}, å¤´æ•°: {n_heads}")
        self.logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒPatchTSTæ¨¡å‹"""
        self.logger.info(f"ğŸ‹ï¸ è®­ç»ƒ{self.model_name}æ¨¡å‹ï¼Œ{epochs}è½®")

        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0001, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')
        patience_counter = 0
        patience = 20

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_losses = []

            for x, y in train_loader:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(x)
                loss = criterion(predictions, y)
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                train_losses.append(loss.item())

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = []

            with torch.no_grad():
                for x, y in val_loader:
                    x, y = x.to(self.device), y.to(self.device)
                    predictions = self.model(x)
                    loss = criterion(predictions, y)
                    val_losses.append(loss.item())

            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)

            scheduler.step()

            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                self.logger.info(f"  Epoch {epoch + 1}/{epochs}: "
                                 f"Train Loss={avg_train_loss:.4f}, "
                                 f"Val Loss={avg_val_loss:.4f}, "
                                 f"LR={current_lr:.6f}")

            if patience_counter >= patience:
                self.logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break

        self.logger.info(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


class TimesNet(BaselineModelBase):
    """TimesNetåŸºçº¿æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__(config, device)
        self.model_name = "TimesNet"

    def build_model(self):
        """æ„å»ºTimesNetæ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        seq_len = self.config['data']['seq_len']
        pred_len = self.config['data']['pred_len']
        input_dim = self.config['data']['input_dim'] if 'input_dim' in self.config['data'] else 7

        class TimesNetModel(nn.Module):
            def __init__(self, seq_len, pred_len, input_dim,
                         d_model=128, n_heads=4, e_layers=2, dropout=0.1):
                super().__init__()
                self.seq_len = seq_len
                self.pred_len = pred_len

                # è¾“å…¥æŠ•å½±
                self.input_projection = nn.Linear(input_dim, d_model)

                # 1Då·ç§¯ç”¨äºå±€éƒ¨ç‰¹å¾æå–
                self.conv1 = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)
                self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=5, padding=2)
                self.conv3 = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)

                # è‡ªé€‚åº”èåˆ
                self.fusion_weights = nn.Parameter(torch.ones(3) / 3)

                # Transformerç¼–ç å™¨
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 4,
                    dropout=dropout, batch_first=True
                )
                self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)

                # è¾“å‡ºå±‚
                self.output_projection = nn.Sequential(
                    nn.Linear(d_model, d_model // 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(d_model // 2, 1)
                )

                # æ—¶é—´æ³¨æ„åŠ›
                self.time_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)

            def forward(self, x):
                # x: [batch, seq_len, input_dim]
                batch_size = x.shape[0]

                # è¾“å…¥æŠ•å½±
                x = self.input_projection(x)  # [batch, seq_len, d_model]
                x = x.transpose(1, 2)  # [batch, d_model, seq_len]

                # å¤šå°ºåº¦å·ç§¯
                conv1_out = F.relu(self.conv1(x))
                conv2_out = F.relu(self.conv2(x))
                conv3_out = F.relu(self.conv3(x))

                # åŠ æƒèåˆ
                weights = F.softmax(self.fusion_weights, dim=0)
                conv_out = (weights[0] * conv1_out +
                            weights[1] * conv2_out +
                            weights[2] * conv3_out)

                # è½¬ç½®å›æ¥
                conv_out = conv_out.transpose(1, 2)  # [batch, seq_len, d_model]

                # Transformerç¼–ç 
                transformer_out = self.transformer_encoder(conv_out)

                # æ—¶é—´æ³¨æ„åŠ›
                attn_out, _ = self.time_attention(transformer_out, transformer_out, transformer_out)

                # å–æœ€åpred_lenä¸ªæ—¶é—´æ­¥
                output = attn_out[:, -self.pred_len:, :]  # [batch, pred_len, d_model]

                # è¾“å‡ºæŠ•å½±
                output = self.output_projection(output)  # [batch, pred_len, 1]

                return output

        # ä»é…ç½®è·å–å‚æ•°
        timesnet_config = self.config.get('timesnet', {})
        d_model = timesnet_config.get('d_model', 128)
        n_heads = timesnet_config.get('n_heads', 4)
        e_layers = timesnet_config.get('e_layers', 2)
        dropout = timesnet_config.get('dropout', 0.1)

        self.model = TimesNetModel(
            seq_len, pred_len, input_dim,
            d_model, n_heads, e_layers, dropout
        ).to(self.device)

        self.logger.info(f"âœ… æ„å»º{self.model_name}æ¨¡å‹")
        self.logger.info(f"   éšè—ç»´åº¦: {d_model}, æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        self.logger.info(f"   Transformerå±‚æ•°: {e_layers}")
        self.logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒTimesNetæ¨¡å‹"""
        self.logger.info(f"ğŸ‹ï¸ è®­ç»ƒ{self.model_name}æ¨¡å‹ï¼Œ{epochs}è½®")

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0005, weight_decay=0.0001)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10
        )
        criterion = nn.MSELoss()

        best_val_loss = float('inf')
        patience_counter = 0
        patience = 25

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_losses = []

            for x, y in train_loader:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(x)
                loss = criterion(predictions, y)
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                train_losses.append(loss.item())

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = []

            with torch.no_grad():
                for x, y in val_loader:
                    x, y = x.to(self.device), y.to(self.device)
                    predictions = self.model(x)
                    loss = criterion(predictions, y)
                    val_losses.append(loss.item())

            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)

            scheduler.step(avg_val_loss)

            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                self.logger.info(f"  Epoch {epoch + 1}/{epochs}: "
                                 f"Train Loss={avg_train_loss:.4f}, "
                                 f"Val Loss={avg_val_loss:.4f}, "
                                 f"LR={current_lr:.6f}")

            if patience_counter >= patience:
                self.logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break

        self.logger.info(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


class FEDformer(BaselineModelBase):
    """FEDformeråŸºçº¿æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__(config, device)
        self.model_name = "FEDformer"

    def build_model(self):
        """æ„å»ºFEDformeræ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        seq_len = self.config['data']['seq_len']
        pred_len = self.config['data']['pred_len']
        input_dim = self.config['data']['input_dim'] if 'input_dim' in self.config['data'] else 7

        class FEDformerModel(nn.Module):
            def __init__(self, seq_len, pred_len, input_dim,
                         d_model=128, n_heads=4, e_layers=2, d_ff=256, dropout=0.1):
                super().__init__()
                self.seq_len = seq_len
                self.pred_len = pred_len

                # è¾“å…¥æŠ•å½±
                self.enc_embedding = nn.Linear(input_dim, d_model)

                # é¢‘åŸŸç¼–ç å™¨
                self.freq_encoder = nn.ModuleList([
                    nn.Sequential(
                        nn.Conv1d(d_model, d_model, kernel_size=3, padding=1),
                        nn.ReLU(),
                        nn.Dropout(dropout)
                    ) for _ in range(e_layers)
                ])

                # æ—¶åŸŸç¼–ç å™¨
                self.time_encoder = nn.TransformerEncoderLayer(
                    d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,
                    dropout=dropout, batch_first=True
                )

                # å‚…é‡Œå¶å˜æ¢å±‚
                self.dft = lambda x: torch.fft.rfft(x, dim=1)
                self.idft = lambda x: torch.fft.irfft(x, dim=1)

                # é¢‘åŸŸé—¨æ§
                self.freq_gate = nn.Sequential(
                    nn.Linear(d_model // 2 + 1, d_model // 4),
                    nn.ReLU(),
                    nn.Linear(d_model // 4, d_model),
                    nn.Sigmoid()
                )

                # è¾“å‡ºæŠ•å½±
                self.output_projection = nn.Sequential(
                    nn.Linear(d_model, d_model // 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(d_model // 2, 1)
                )

            def forward(self, x):
                # x: [batch, seq_len, input_dim]
                batch_size = x.shape[0]

                # è¾“å…¥åµŒå…¥
                enc_out = self.enc_embedding(x)  # [batch, seq_len, d_model]

                # æ—¶åŸŸç¼–ç 
                time_out = self.time_encoder(enc_out)

                # å‚…é‡Œå¶å˜æ¢
                time_out_t = time_out.transpose(1, 2)  # [batch, d_model, seq_len]
                freq_out = self.dft(time_out_t)  # [batch, d_model, freq_bins]

                # é¢‘åŸŸå¤„ç†
                freq_magnitude = torch.abs(freq_out)
                freq_features = freq_magnitude.mean(dim=1)  # [batch, freq_bins]

                # é¢‘åŸŸé—¨æ§
                freq_gate = self.freq_gate(freq_features).unsqueeze(1)  # [batch, 1, d_model]

                # åº”ç”¨é—¨æ§
                gated_out = time_out * freq_gate

                # é¢‘åŸŸç¼–ç å™¨
                for layer in self.freq_encoder:
                    gated_out_t = gated_out.transpose(1, 2)
                    gated_out_t = layer(gated_out_t)
                    gated_out = gated_out_t.transpose(1, 2)

                # å–æœ€åpred_lenä¸ªæ—¶é—´æ­¥
                output = gated_out[:, -self.pred_len:, :]  # [batch, pred_len, d_model]

                # è¾“å‡ºæŠ•å½±
                output = self.output_projection(output)  # [batch, pred_len, 1]

                return output

        # ä»é…ç½®è·å–å‚æ•°
        fedformer_config = self.config.get('fedformer', {})
        d_model = fedformer_config.get('d_model', 128)
        n_heads = fedformer_config.get('n_heads', 4)
        e_layers = fedformer_config.get('e_layers', 2)
        d_ff = fedformer_config.get('d_ff', 256)
        dropout = fedformer_config.get('dropout', 0.1)

        self.model = FEDformerModel(
            seq_len, pred_len, input_dim,
            d_model, n_heads, e_layers, d_ff, dropout
        ).to(self.device)

        self.logger.info(f"âœ… æ„å»º{self.model_name}æ¨¡å‹")
        self.logger.info(f"   éšè—ç»´åº¦: {d_model}, æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        self.logger.info(f"   ç¼–ç å™¨å±‚æ•°: {e_layers}")
        self.logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒFEDformeræ¨¡å‹"""
        self.logger.info(f"ğŸ‹ï¸ è®­ç»ƒ{self.model_name}æ¨¡å‹ï¼Œ{epochs}è½®")

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0005, weight_decay=0.0001)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')
        patience_counter = 0
        patience = 25

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_losses = []

            for x, y in train_loader:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(x)
                loss = criterion(predictions, y)
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                train_losses.append(loss.item())

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = []

            with torch.no_grad():
                for x, y in val_loader:
                    x, y = x.to(self.device), y.to(self.device)
                    predictions = self.model(x)
                    loss = criterion(predictions, y)
                    val_losses.append(loss.item())

            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)

            scheduler.step()

            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                self.logger.info(f"  Epoch {epoch + 1}/{epochs}: "
                                 f"Train Loss={avg_train_loss:.4f}, "
                                 f"Val Loss={avg_val_loss:.4f}, "
                                 f"LR={current_lr:.6f}")

            if patience_counter >= patience:
                self.logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break

        self.logger.info(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


class Autoformer(BaselineModelBase):
    """AutoformeråŸºçº¿æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__(config, device)
        self.model_name = "Autoformer"

    def build_model(self):
        """æ„å»ºAutoformeræ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        seq_len = self.config['data']['seq_len']
        pred_len = self.config['data']['pred_len']
        input_dim = self.config['data']['input_dim'] if 'input_dim' in self.config['data'] else 7

        class AutoformerModel(nn.Module):
            def __init__(self, seq_len, pred_len, input_dim,
                         d_model=128, n_heads=4, e_layers=2, d_ff=256, dropout=0.1):
                super().__init__()
                self.seq_len = seq_len
                self.pred_len = pred_len

                # è¾“å…¥æŠ•å½±
                self.enc_embedding = nn.Linear(input_dim, d_model)

                # å­£èŠ‚æ€§ç¼–ç å™¨
                self.seasonal_encoder = nn.TransformerEncoderLayer(
                    d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,
                    dropout=dropout, batch_first=True
                )

                # è¶‹åŠ¿åˆ†è§£
                self.trend_decomposition = nn.Sequential(
                    nn.Conv1d(d_model, d_model, kernel_size=25, padding=12),
                    nn.ReLU(),
                    nn.Conv1d(d_model, d_model, kernel_size=1)
                )

                # è‡ªç›¸å…³æœºåˆ¶ï¼ˆç®€åŒ–ï¼‰
                self.autocorrelation = nn.MultiheadAttention(
                    d_model, n_heads, dropout=dropout, batch_first=True
                )

                # è¾“å‡ºæŠ•å½±
                self.output_projection = nn.Sequential(
                    nn.Linear(d_model * 2, d_model),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(d_model, 1)
                )

            def forward(self, x):
                # x: [batch, seq_len, input_dim]
                batch_size = x.shape[0]

                # è¾“å…¥åµŒå…¥
                enc_out = self.enc_embedding(x)  # [batch, seq_len, d_model]

                # è¶‹åŠ¿åˆ†è§£
                trend = self.trend_decomposition(enc_out.transpose(1, 2)).transpose(1, 2)
                seasonal = enc_out - trend

                # å­£èŠ‚æ€§ç¼–ç 
                seasonal_encoded = self.seasonal_encoder(seasonal)

                # è‡ªç›¸å…³
                autocorr_out, _ = self.autocorrelation(seasonal_encoded, seasonal_encoded, seasonal_encoded)

                # åˆå¹¶è¶‹åŠ¿å’Œå­£èŠ‚æ€§
                combined = torch.cat([trend[:, -self.pred_len:, :],
                                      autocorr_out[:, -self.pred_len:, :]], dim=-1)

                # è¾“å‡ºæŠ•å½±
                output = self.output_projection(combined)  # [batch, pred_len, 1]

                return output

        # ä»é…ç½®è·å–å‚æ•°
        autoformer_config = self.config.get('autoformer', {})
        d_model = autoformer_config.get('d_model', 128)
        n_heads = autoformer_config.get('n_heads', 4)
        e_layers = autoformer_config.get('e_layers', 2)
        d_ff = autoformer_config.get('d_ff', 256)
        dropout = autoformer_config.get('dropout', 0.1)

        self.model = AutoformerModel(
            seq_len, pred_len, input_dim,
            d_model, n_heads, e_layers, d_ff, dropout
        ).to(self.device)

        self.logger.info(f"âœ… æ„å»º{self.model_name}æ¨¡å‹")
        self.logger.info(f"   éšè—ç»´åº¦: {d_model}, æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        self.logger.info(f"   ç¼–ç å™¨å±‚æ•°: {e_layers}")
        self.logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒAutoformeræ¨¡å‹"""
        self.logger.info(f"ğŸ‹ï¸ è®­ç»ƒ{self.model_name}æ¨¡å‹ï¼Œ{epochs}è½®")

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0005, weight_decay=0.0001)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')
        patience_counter = 0
        patience = 25

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_losses = []

            for x, y in train_loader:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(x)
                loss = criterion(predictions, y)
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                train_losses.append(loss.item())

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = []

            with torch.no_grad():
                for x, y in val_loader:
                    x, y = x.to(self.device), y.to(self.device)
                    predictions = self.model(x)
                    loss = criterion(predictions, y)
                    val_losses.append(loss.item())

            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)

            scheduler.step()

            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                self.logger.info(f"  Epoch {epoch + 1}/{epochs}: "
                                 f"Train Loss={avg_train_loss:.4f}, "
                                 f"Val Loss={avg_val_loss:.4f}, "
                                 f"LR={current_lr:.6f}")

            if patience_counter >= patience:
                self.logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break

        self.logger.info(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


class Informer(BaselineModelBase):
    """InformeråŸºçº¿æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__(config, device)
        self.model_name = "Informer"

    def build_model(self):
        """æ„å»ºInformeræ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        seq_len = self.config['data']['seq_len']
        pred_len = self.config['data']['pred_len']
        input_dim = self.config['data']['input_dim'] if 'input_dim' in self.config['data'] else 7

        class InformerModel(nn.Module):
            def __init__(self, seq_len, pred_len, input_dim,
                         d_model=128, n_heads=4, e_layers=2, d_ff=256, dropout=0.1):
                super().__init__()
                self.seq_len = seq_len
                self.pred_len = pred_len

                # è¾“å…¥æŠ•å½±
                self.enc_embedding = nn.Linear(input_dim, d_model)

                # ä½ç½®ç¼–ç 
                self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=5000)

                # æ¦‚ç‡ç¨€ç–æ³¨æ„åŠ›
                self.attention = ProbSparseAttention(d_model, n_heads, dropout)

                # ç¼–ç å™¨å±‚
                self.encoder_layers = nn.ModuleList([
                    nn.TransformerEncoderLayer(
                        d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,
                        dropout=dropout, batch_first=True
                    ) for _ in range(e_layers)
                ])

                # è¾“å‡ºæŠ•å½±
                self.output_projection = nn.Sequential(
                    nn.Linear(d_model, d_model // 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(d_model // 2, 1)
                )

            def forward(self, x):
                # x: [batch, seq_len, input_dim]
                batch_size = x.shape[0]

                # è¾“å…¥åµŒå…¥
                enc_out = self.enc_embedding(x)  # [batch, seq_len, d_model]

                # ä½ç½®ç¼–ç 
                enc_out = self.pos_encoder(enc_out)

                # æ¦‚ç‡ç¨€ç–æ³¨æ„åŠ›
                attn_out, _ = self.attention(enc_out, enc_out, enc_out)

                # ç¼–ç å™¨å±‚
                encoder_out = attn_out
                for layer in self.encoder_layers:
                    encoder_out = layer(encoder_out)

                # å–æœ€åpred_lenä¸ªæ—¶é—´æ­¥
                output = encoder_out[:, -self.pred_len:, :]  # [batch, pred_len, d_model]

                # è¾“å‡ºæŠ•å½±
                output = self.output_projection(output)  # [batch, pred_len, 1]

                return output

        # è¾…åŠ©ç±»
        class PositionalEncoding(nn.Module):
            def __init__(self, d_model, dropout=0.1, max_len=5000):
                super().__init__()
                self.dropout = nn.Dropout(p=dropout)

                pe = torch.zeros(max_len, d_model)
                position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                                     (-np.log(10000.0) / d_model))
                pe[:, 0::2] = torch.sin(position * div_term)
                pe[:, 1::2] = torch.cos(position * div_term)
                pe = pe.unsqueeze(0)
                self.register_buffer('pe', pe)

            def forward(self, x):
                x = x + self.pe[:, :x.size(1)]
                return self.dropout(x)

        class ProbSparseAttention(nn.Module):
            def __init__(self, d_model, n_heads, dropout=0.1):
                super().__init__()
                self.d_model = d_model
                self.n_heads = n_heads
                self.head_dim = d_model // n_heads

                self.q_linear = nn.Linear(d_model, d_model)
                self.k_linear = nn.Linear(d_model, d_model)
                self.v_linear = nn.Linear(d_model, d_model)
                self.out_linear = nn.Linear(d_model, d_model)

                self.dropout = nn.Dropout(dropout)

            def forward(self, query, key, value):
                batch_size = query.shape[0]

                # çº¿æ€§å˜æ¢
                Q = self.q_linear(query)
                K = self.k_linear(key)
                V = self.v_linear(value)

                # åˆ†å‰²å¤šå¤´
                Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
                K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
                V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)

                # è®¡ç®—æ³¨æ„åŠ›ï¼ˆç®€åŒ–ï¼ŒéçœŸæ­£çš„æ¦‚ç‡ç¨€ç–ï¼‰
                scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
                attention = F.softmax(scores, dim=-1)
                attention = self.dropout(attention)

                # åº”ç”¨æ³¨æ„åŠ›
                out = torch.matmul(attention, V)
                out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

                # è¾“å‡ºçº¿æ€§å±‚
                out = self.out_linear(out)

                return out, attention

        # ä»é…ç½®è·å–å‚æ•°
        informer_config = self.config.get('informer', {})
        d_model = informer_config.get('d_model', 128)
        n_heads = informer_config.get('n_heads', 4)
        e_layers = informer_config.get('e_layers', 2)
        d_ff = informer_config.get('d_ff', 256)
        dropout = informer_config.get('dropout', 0.1)

        self.model = InformerModel(
            seq_len, pred_len, input_dim,
            d_model, n_heads, e_layers, d_ff, dropout
        ).to(self.device)

        self.logger.info(f"âœ… æ„å»º{self.model_name}æ¨¡å‹")
        self.logger.info(f"   éšè—ç»´åº¦: {d_model}, æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        self.logger.info(f"   ç¼–ç å™¨å±‚æ•°: {e_layers}")
        self.logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒInformeræ¨¡å‹"""
        self.logger.info(f"ğŸ‹ï¸ è®­ç»ƒ{self.model_name}æ¨¡å‹ï¼Œ{epochs}è½®")

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0005, weight_decay=0.0001)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')
        patience_counter = 0
        patience = 25

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_losses = []

            for x, y in train_loader:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(x)
                loss = criterion(predictions, y)
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                train_losses.append(loss.item())

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = []

            with torch.no_grad():
                for x, y in val_loader:
                    x, y = x.to(self.device), y.to(self.device)
                    predictions = self.model(x)
                    loss = criterion(predictions, y)
                    val_losses.append(loss.item())

            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)

            scheduler.step()

            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                self.logger.info(f"  Epoch {epoch + 1}/{epochs}: "
                                 f"Train Loss={avg_train_loss:.4f}, "
                                 f"Val Loss={avg_val_loss:.4f}, "
                                 f"LR={current_lr:.6f}")

            if patience_counter >= patience:
                self.logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break

        self.logger.info(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


class TimeLLM(BaselineModelBase):
    """TimeLLMåŸºçº¿æ¨¡å‹ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""

    def __init__(self, config: Dict[str, Any], device: torch.device):
        super().__init__(config, device)
        self.model_name = "TimeLLM"

    def build_model(self):
        """æ„å»ºTimeLLMæ¨¡å‹ï¼ˆåŸºäºè®ºæ–‡æè¿°çš„æ¨¡æ‹Ÿå®ç°ï¼‰"""
        seq_len = self.config['data']['seq_len']
        pred_len = self.config['data']['pred_len']
        input_dim = self.config['data']['input_dim'] if 'input_dim' in self.config['data'] else 7

        class TimeLLMModel(nn.Module):
            def __init__(self, seq_len, pred_len, input_dim,
                         d_model=256, n_heads=8, n_layers=4, dropout=0.1):
                super().__init__()
                self.seq_len = seq_len
                self.pred_len = pred_len

                # è¡¥ä¸åµŒå…¥
                self.patch_embedding = nn.Linear(16, d_model)  # å‡è®¾è¡¥ä¸å¤§å°ä¸º16

                # ä½ç½®ç¼–ç 
                self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=5000)

                # LLMéª¨å¹²ï¼ˆç®€åŒ–Transformerï¼‰
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 4,
                    dropout=dropout, batch_first=True
                )
                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

                # æ—¶é—´ç¼–ç é€‚é…å™¨
                self.time_adapter = nn.Sequential(
                    nn.Linear(d_model, d_model // 2),
                    nn.ReLU(),
                    nn.Linear(d_model // 2, d_model)
                )

                # è¾“å‡ºæŠ•å½±
                self.output_projection = nn.Sequential(
                    nn.Linear(d_model, d_model // 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(d_model // 2, 1)
                )

            def forward(self, x):
                # x: [batch, seq_len, input_dim]
                batch_size = x.shape[0]

                # åˆ›å»ºè¡¥ä¸ï¼ˆç®€åŒ–ï¼‰
                patch_size = 16
                num_patches = seq_len // patch_size

                # é‡å¡‘ä¸ºè¡¥ä¸
                patches = x.view(batch_size, num_patches, patch_size, input_dim)
                patches = patches.mean(dim=3)  # å¹³å‡ç‰¹å¾ç»´åº¦ [batch, num_patches, patch_size]

                # è¡¥ä¸åµŒå…¥
                patch_embeddings = self.patch_embedding(patches)  # [batch, num_patches, d_model]

                # ä½ç½®ç¼–ç 
                patch_embeddings = self.pos_encoder(patch_embeddings)

                # æ—¶é—´ç¼–ç é€‚é…å™¨
                adapted_embeddings = self.time_adapter(patch_embeddings)

                # Transformerç¼–ç 
                encoded = self.transformer(adapted_embeddings)

                # å…¨å±€å¹³å‡æ± åŒ–
                global_features = encoded.mean(dim=1)  # [batch, d_model]

                # é‡å¤ç”¨äºé¢„æµ‹é•¿åº¦
                repeated_features = global_features.unsqueeze(1).repeat(1, self.pred_len, 1)

                # è¾“å‡ºæŠ•å½±
                output = self.output_projection(repeated_features)  # [batch, pred_len, 1]

                return output

        # ä½ç½®ç¼–ç ç±»
        class PositionalEncoding(nn.Module):
            def __init__(self, d_model, dropout=0.1, max_len=5000):
                super().__init__()
                self.dropout = nn.Dropout(p=dropout)

                pe = torch.zeros(max_len, d_model)
                position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                                     (-np.log(10000.0) / d_model))
                pe[:, 0::2] = torch.sin(position * div_term)
                pe[:, 1::2] = torch.cos(position * div_term)
                pe = pe.unsqueeze(0)
                self.register_buffer('pe', pe)

            def forward(self, x):
                x = x + self.pe[:, :x.size(1)]
                return self.dropout(x)

        # ä»é…ç½®è·å–å‚æ•°
        timellm_config = self.config.get('timellm', {})
        d_model = timellm_config.get('d_model', 256)
        n_heads = timellm_config.get('n_heads', 8)
        n_layers = timellm_config.get('n_layers', 4)
        dropout = timellm_config.get('dropout', 0.1)

        self.model = TimeLLMModel(
            seq_len, pred_len, input_dim,
            d_model, n_heads, n_layers, dropout
        ).to(self.device)

        self.logger.info(f"âœ… æ„å»º{self.model_name}æ¨¡å‹")
        self.logger.info(f"   éšè—ç»´åº¦: {d_model}, æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        self.logger.info(f"   Transformerå±‚æ•°: {n_layers}")
        self.logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_loader, val_loader, epochs: int = 100):
        """è®­ç»ƒTimeLLMæ¨¡å‹"""
        self.logger.info(f"ğŸ‹ï¸ è®­ç»ƒ{self.model_name}æ¨¡å‹ï¼Œ{epochs}è½®")

        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0001, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')
        patience_counter = 0
        patience = 30

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_losses = []

            for x, y in train_loader:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(x)
                loss = criterion(predictions, y)
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                train_losses.append(loss.item())

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = []

            with torch.no_grad():
                for x, y in val_loader:
                    x, y = x.to(self.device), y.to(self.device)
                    predictions = self.model(x)
                    loss = criterion(predictions, y)
                    val_losses.append(loss.item())

            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)

            scheduler.step()

            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                self.logger.info(f"  Epoch {epoch + 1}/{epochs}: "
                                 f"Train Loss={avg_train_loss:.4f}, "
                                 f"Val Loss={avg_val_loss:.4f}, "
                                 f"LR={current_lr:.6f}")

            if patience_counter >= patience:
                self.logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break

        self.logger.info(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


# ==================== åŸºçº¿å¯¹æ¯”å®éªŒç®¡ç†å™¨ ====================

class BaselineComparison:
    """åŸºçº¿å¯¹æ¯”å®éªŒç®¡ç†å™¨"""

    def __init__(self, config_path: str = "./config.yaml"):
        # åŠ è½½é…ç½®
        self.config = load_config(config_path)

        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger("BaselineComparison")

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(
            'cuda:0' if torch.cuda.is_available() else 'cpu'
        )

        # è®¾ç½®éšæœºç§å­
        self._set_seed()

        # å®éªŒç»“æœ
        self.results: Dict[str, BaselineResult] = {}
        self.comparison_result: Optional[ComparisonResult] = None

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("./results/baseline_comparison")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.logger.info("ğŸ“Š åŸºçº¿å¯¹æ¯”å®éªŒåˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   è®¾å¤‡: {self.device}")
        self.logger.info(f"   è¾“å‡ºç›®å½•: {self.output_dir}")

    def _set_seed(self):
        """è®¾ç½®éšæœºç§å­"""
        seed = self.config['experiment']['seed']

        torch.manual_seed(seed)
        np.random.seed(seed)

        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False

        self.logger.info(f"ğŸ”§ è®¾ç½®éšæœºç§å­: {seed}")

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        self.logger.info("ğŸ“¥ åŠ è½½æ•°æ®...")

        data_path = self.config['data']['data_path']

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = create_dataloaders(self.config, data_path)

        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader

        self.logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        self.logger.info(f"   è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
        self.logger.info(f"   éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
        self.logger.info(f"   æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬")

        return train_loader, val_loader, test_loader

    def run_baseline(self, baseline_name: str, train: bool = True) -> BaselineResult:
        """
        è¿è¡Œå•ä¸ªåŸºçº¿æ–¹æ³•

        Args:
            baseline_name: åŸºçº¿æ–¹æ³•åç§°
            train: æ˜¯å¦è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœå·²ä¿å­˜æ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥è·³è¿‡è®­ç»ƒï¼‰

        Returns:
            åŸºçº¿æ–¹æ³•ç»“æœ
        """
        self.logger.info(f"ğŸš€ è¿è¡ŒåŸºçº¿æ–¹æ³•: {baseline_name}")

        # æ ¹æ®åç§°é€‰æ‹©åŸºçº¿æ–¹æ³•
        baseline_classes = {
            'DLinear': DLinear,
            'PatchTST': PatchTST,
            'TimesNet': TimesNet,
            'FEDformer': FEDformer,
            'Autoformer': Autoformer,
            'Informer': Informer,
            'TimeLLM': TimeLLM
        }

        if baseline_name not in baseline_classes:
            raise ValueError(f"æœªçŸ¥çš„åŸºçº¿æ–¹æ³•: {baseline_name}")

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        BaselineClass = baseline_classes[baseline_name]
        baseline = BaselineClass(self.config, self.device)

        # æ„å»ºæ¨¡å‹
        baseline.build_model()

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¿å­˜çš„æ¨¡å‹
        model_path = self.output_dir / f"{baseline_name}_model.pth"

        if train or not model_path.exists():
            # è®­ç»ƒæ¨¡å‹
            epochs = self.config.get('training', {}).get('epochs', 100)
            baseline.train(self.train_loader, self.val_loader, epochs)

            # ä¿å­˜æ¨¡å‹
            torch.save(baseline.model.state_dict(), model_path)
            self.logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        else:
            # åŠ è½½æ¨¡å‹
            baseline.model.load_state_dict(torch.load(model_path, map_location=self.device))
            self.logger.info(f"ğŸ“¥ åŠ è½½å·²ä¿å­˜æ¨¡å‹: {model_path}")

        # è¯„ä¼°æ¨¡å‹
        self.logger.info(f"ğŸ§ª è¯„ä¼°{baseline_name}...")
        metrics, predictions, targets = baseline.evaluate(self.test_loader)

        # åˆ›å»ºç»“æœå¯¹è±¡
        result = BaselineResult(
            model_name=baseline_name,
            mse=metrics['mse'],
            mae=metrics['mae'],
            rmse=metrics['rmse'],
            mape=metrics['mape'],
            smape=metrics['smape'],
            r2=metrics['r2'],
            inference_time=metrics.get('inference_time', 0),
            memory_usage=metrics.get('memory_usage', 0),
            parameters=metrics.get('parameters', 0),
            config=self.config,
            predictions=predictions,
            targets=targets
        )

        # ä¿å­˜ç»“æœ
        self.results[baseline_name] = result

        # æ‰“å°ç»“æœ
        self.logger.info(f"âœ… {baseline_name} ç»“æœ:")
        self.logger.info(f"   MSE: {result.mse:.6f}")
        self.logger.info(f"   MAE: {result.mae:.6f}")
        self.logger.info(f"   RMSE: {result.rmse:.6f}")
        self.logger.info(f"   MAPE: {result.mape:.6f}%")
        self.logger.info(f"   SMAPE: {result.smape:.6f}%")
        self.logger.info(f"   RÂ²: {result.r2:.6f}")
        self.logger.info(f"   æ¨ç†æ—¶é—´: {result.inference_time:.4f}ç§’/æ ·æœ¬")
        self.logger.info(f"   å†…å­˜ä½¿ç”¨: {result.memory_usage:.2f}MB")
        self.logger.info(f"   å‚æ•°é‡: {result.parameters:,}")

        return result

    def run_star_forecast(self) -> BaselineResult:
        """è¿è¡ŒSTAR-Forecastæ¨¡å‹ï¼ˆä½œä¸ºåŸºçº¿ä¹‹ä¸€ï¼‰"""
        self.logger.info("ğŸš€ è¿è¡ŒSTAR-Forecastæ¨¡å‹...")

        from ..training.trainer import STARForecastTrainer

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = STARForecastTrainer()

        # æ„å»ºæ¨¡å‹
        trainer.build_models()

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¿å­˜çš„æ¨¡å‹
        model_path = self.output_dir / "STAR-Forecast_model.pth"

        if model_path.exists():
            # åŠ è½½æ¨¡å‹
            checkpoint = torch.load(model_path, map_location=self.device)
            trainer.istr_model.load_state_dict(checkpoint['istr_state_dict'])
            trainer.predictor.load_state_dict(checkpoint['predictor_state_dict'])
            self.logger.info(f"ğŸ“¥ åŠ è½½å·²ä¿å­˜æ¨¡å‹: {model_path}")
        else:
            # è®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ï¼Œä½¿ç”¨å°‘é‡è½®æ¬¡ï¼‰
            trainer.train_epoch = self._mock_train_epoch  # æ›¿æ¢ä¸ºæ¨¡æ‹Ÿè®­ç»ƒ
            for epoch in range(10):  # å°‘é‡è®­ç»ƒ
                trainer.train_epoch(self.train_loader, epoch)

            # ä¿å­˜æ¨¡å‹
            checkpoint = {
                'istr_state_dict': trainer.istr_model.state_dict(),
                'predictor_state_dict': trainer.predictor.state_dict()
            }
            torch.save(checkpoint, model_path)
            self.logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")

        # è¯„ä¼°æ¨¡å‹
        self.logger.info("ğŸ§ª è¯„ä¼°STAR-Forecast...")

        trainer.istr_model.eval()
        trainer.predictor.eval()

        all_predictions = []
        all_targets = []
        inference_times = []

        with torch.no_grad():
            for x, y in tqdm(self.test_loader, desc="è¯„ä¼° STAR-Forecast"):
                x, y = x.to(self.device), y.to(self.device)

                # è®¡æ—¶
                start_time = torch.cuda.Event(enable_timing=True)
                end_time = torch.cuda.Event(enable_timing=True)

                if torch.cuda.is_available():
                    start_time.record()

                # é¢„æµ‹
                features = trainer.istr_model(x)
                predictions = trainer.predictor(features)

                if torch.cuda.is_available():
                    end_time.record()
                    torch.cuda.synchronize()
                    inference_times.append(start_time.elapsed_time(end_time) / 1000)

                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(y.cpu().numpy())

        # åˆå¹¶ç»“æœ
        predictions = np.concatenate(all_predictions, axis=0)
        targets = np.concatenate(all_targets, axis=0)

        # è®¡ç®—æŒ‡æ ‡
        metrics_calculator = TimeSeriesMetrics()
        metrics = metrics_calculator.compute(predictions, targets)

        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in trainer.istr_model.parameters())
        total_params += sum(p.numel() for p in trainer.predictor.parameters())

        # åˆ›å»ºç»“æœå¯¹è±¡
        result = BaselineResult(
            model_name="STAR-Forecast",
            mse=metrics['mse'],
            mae=metrics['mae'],
            rmse=metrics['rmse'],
            mape=metrics['mape'],
            smape=metrics['smape'],
            r2=metrics['r2'],
            inference_time=np.mean(inference_times) if inference_times else 0,
            memory_usage=torch.cuda.max_memory_allocated() / 1024 ** 2 if torch.cuda.is_available() else 0,
            parameters=total_params,
            config=self.config,
            predictions=predictions,
            targets=targets
        )

        # ä¿å­˜ç»“æœ
        self.results["STAR-Forecast"] = result

        # æ‰“å°ç»“æœ
        self.logger.info(f"âœ… STAR-Forecast ç»“æœ:")
        self.logger.info(f"   MSE: {result.mse:.6f}")
        self.logger.info(f"   MAE: {result.mae:.6f}")
        self.logger.info(f"   RMSE: {result.rmse:.6f}")
        self.logger.info(f"   MAPE: {result.mape:.6f}%")
        self.logger.info(f"   SMAPE: {result.smape:.6f}%")
        self.logger.info(f"   RÂ²: {result.r2:.6f}")
        self.logger.info(f"   æ¨ç†æ—¶é—´: {result.inference_time:.4f}ç§’/æ ·æœ¬")
        self.logger.info(f"   å†…å­˜ä½¿ç”¨: {result.memory_usage:.2f}MB")
        self.logger.info(f"   å‚æ•°é‡: {result.parameters:,}")

        return result

    def _mock_train_epoch(self, train_loader, epoch):
        """æ¨¡æ‹Ÿè®­ç»ƒepochï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"""
        # åœ¨å®é™…ä½¿ç”¨ä¸­åº”è¯¥ä½¿ç”¨å®Œæ•´çš„è®­ç»ƒé€»è¾‘
        pass

    def run_all_baselines(self, baselines: List[str] = None, include_star_forecast: bool = True):
        """
        è¿è¡Œæ‰€æœ‰åŸºçº¿æ–¹æ³•

        Args:
            baselines: è¦è¿è¡Œçš„åŸºçº¿æ–¹æ³•åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ–¹æ³•
            include_star_forecast: æ˜¯å¦åŒ…å«STAR-Forecast
        """
        if baselines is None:
            baselines = [
                'DLinear',
                'PatchTST',
                'TimesNet',
                'FEDformer',
                'Autoformer',
                'Informer',
                'TimeLLM'
            ]

        # åŠ è½½æ•°æ®
        self.load_data()

        # è¿è¡ŒåŸºçº¿æ–¹æ³•
        for baseline in baselines:
            try:
                self.run_baseline(baseline, train=True)
            except Exception as e:
                self.logger.error(f"âŒ {baseline} è¿è¡Œå¤±è´¥: {e}")

        # è¿è¡ŒSTAR-Forecast
        if include_star_forecast:
            try:
                self.run_star_forecast()
            except Exception as e:
                self.logger.error(f"âŒ STAR-Forecast è¿è¡Œå¤±è´¥: {e}")

        # è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        self._perform_significance_tests()

        # ç”Ÿæˆæ€»ç»“
        self._generate_summary()

        # ä¿å­˜ç»“æœ
        self._save_results()

        # å¯è§†åŒ–
        self._visualize_results()

        self.logger.info("ğŸ‰ æ‰€æœ‰åŸºçº¿æ–¹æ³•å¯¹æ¯”å®Œæˆï¼")

    def _perform_significance_tests(self):
        """è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        self.logger.info("ğŸ“ˆ è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")

        significance_tests = {}

        # è·å–æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        model_names = list(self.results.keys())

        if len(model_names) < 2:
            self.logger.warning("âš ï¸ è‡³å°‘éœ€è¦2ä¸ªæ¨¡å‹ç»“æœè¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒ")
            return

        # å¯¹æ¯å¯¹æ¨¡å‹è¿›è¡ŒWilcoxonç¬¦å·ç§©æ£€éªŒ
        for i, model1 in enumerate(model_names):
            for model2 in model_names[i + 1:]:
                if model1 in self.results and model2 in self.results:
                    try:
                        # è·å–é¢„æµ‹è¯¯å·®
                        errors1 = self.results[model1].predictions - self.results[model1].targets
                        errors2 = self.results[model2].predictions - self.results[model2].targets

                        # å±•å¹³
                        errors1_flat = errors1.flatten()
                        errors2_flat = errors2.flatten()

                        # ç¡®ä¿é•¿åº¦ä¸€è‡´
                        min_len = min(len(errors1_flat), len(errors2_flat))
                        errors1_flat = errors1_flat[:min_len]
                        errors2_flat = errors2_flat[:min_len]

                        # Wilcoxonæ£€éªŒ
                        from scipy import stats
                        stat, p_value = stats.wilcoxon(
                            np.abs(errors1_flat),
                            np.abs(errors2_flat)
                        )

                        # è®¡ç®—æ•ˆåº”é‡
                        effect_size = np.mean(np.abs(errors1_flat) - np.abs(errors2_flat))
                        effect_size = effect_size / (np.std(np.abs(errors1_flat) - np.abs(errors2_flat)) + 1e-8)

                        test_key = f"{model1}_vs_{model2}"
                        significance_tests[test_key] = {
                            'test': 'Wilcoxon',
                            'statistic': float(stat),
                            'p_value': float(p_value),
                            'significant': p_value < 0.05,
                            'effect_size': float(effect_size),
                            'winner': model1 if effect_size < 0 else model2  # è¯¯å·®è¶Šå°è¶Šå¥½
                        }

                        self.logger.info(f"  {test_key}: p={p_value:.6f}, "
                                         f"æ˜¾è‘—: {p_value < 0.05}, "
                                         f"è·èƒœè€…: {significance_tests[test_key]['winner']}")

                    except Exception as e:
                        self.logger.error(f"  æ˜¾è‘—æ€§æ£€éªŒå¤±è´¥ {model1} vs {model2}: {e}")

        # ä¿å­˜åˆ°ç»“æœä¸­
        if hasattr(self, 'comparison_result'):
            self.comparison_result.significance_tests = significance_tests
        else:
            self.significance_tests = significance_tests

    def _generate_summary(self):
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“"""
        self.logger.info("ğŸ“‹ ç”Ÿæˆå¯¹æ¯”æ€»ç»“...")

        # è®¡ç®—æ’å
        mse_ranking = sorted(
            self.results.items(),
            key=lambda x: x[1].mse
        )

        mae_ranking = sorted(
            self.results.items(),
            key=lambda x: x[1].mae
        )

        # ç”Ÿæˆæ€»ç»“
        summary = {
            'best_mse': {
                'model': mse_ranking[0][0],
                'value': mse_ranking[0][1].mse
            },
            'best_mae': {
                'model': mae_ranking[0][0],
                'value': mae_ranking[0][1].mae
            },
            'mse_ranking': [
                {'model': model, 'mse': result.mse}
                for model, result in mse_ranking
            ],
            'mae_ranking': [
                {'model': model, 'mae': result.mae}
                for model, result in mae_ranking
            ],
            'model_count': len(self.results),
            'avg_mse': np.mean([r.mse for r in self.results.values()]),
            'avg_mae': np.mean([r.mae for r in self.results.values()]),
            'std_mse': np.std([r.mse for r in self.results.values()]),
            'std_mae': np.std([r.mae for r in self.results.values()])
        }

        # è®¡ç®—ç›¸å¯¹æ”¹è¿›ï¼ˆä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼‰
        if len(mse_ranking) > 1:
            best_baseline_mse = mse_ranking[1][1].mse  # æ’é™¤STAR-Forecastï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€ï¼‰
            star_forecast_result = self.results.get("STAR-Forecast")

            if star_forecast_result:
                relative_improvement = (best_baseline_mse - star_forecast_result.mse) / best_baseline_mse * 100
                summary['relative_improvement_mse'] = relative_improvement
                self.logger.info(f"  STAR-Forecastç›¸å¯¹æ”¹è¿›: {relative_improvement:.2f}%")

        # ä¿å­˜åˆ°ç»“æœä¸­
        if hasattr(self, 'comparison_result'):
            self.comparison_result.summary = summary
        else:
            self.summary = summary

        # æ‰“å°æ€»ç»“
        self.logger.info("=" * 60)
        self.logger.info("ğŸ† æ’åæ€»ç»“")
        self.logger.info("=" * 60)
        self.logger.info("MSEæ’å:")
        for i, (model, result) in enumerate(mse_ranking, 1):
            self.logger.info(f"  {i}. {model}: {result.mse:.6f}")

        self.logger.info("\nMAEæ’å:")
        for i, (model, result) in enumerate(mae_ranking, 1):
            self.logger.info(f"  {i}. {model}: {result.mae:.6f}")

    def _save_results(self):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        self.logger.info("ğŸ’¾ ä¿å­˜ç»“æœ...")

        # åˆ›å»ºæ¯”è¾ƒç»“æœå¯¹è±¡
        experiment_id = f"baseline_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        self.comparison_result = ComparisonResult(
            experiment_id=experiment_id,
            timestamp=datetime.now().isoformat(),
            dataset=self.config['data']['dataset'],
            seq_len=self.config['data']['seq_len'],
            pred_len=self.config['data']['pred_len'],
            results=self.results,
            significance_tests=getattr(self, 'significance_tests', {}),
            summary=getattr(self, 'summary', {}),
            config=self.config
        )

        # ä¿å­˜ä¸ºJSON
        json_path = self.output_dir / f"{experiment_id}.json"

        # è½¬æ¢dataclassä¸ºå­—å…¸
        result_dict = asdict(self.comparison_result)

        # å¤„ç†numpyæ•°ç»„ï¼ˆä¿å­˜ä¸ºåˆ—è¡¨ï¼‰
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.generic):
                return obj.item()
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj

        result_dict = convert_numpy(result_dict)

        with open(json_path, 'w') as f:
            json.dump(result_dict, f, indent=2, ensure_ascii=False)

        # ä¿å­˜ä¸ºCSVï¼ˆä¾¿äºåˆ†æï¼‰
        csv_path = self.output_dir / f"{experiment_id}.csv"

        rows = []
        for model_name, result in self.results.items():
            row = {
                'model': model_name,
                'mse': result.mse,
                'mae': result.mae,
                'rmse': result.rmse,
                'mape': result.mape,
                'smape': result.smape,
                'r2': result.r2,
                'inference_time': result.inference_time,
                'memory_usage': result.memory_usage,
                'parameters': result.parameters
            }
            rows.append(row)

        df = pd.DataFrame(rows)
        df.to_csv(csv_path, index=False, encoding='utf-8')

        self.logger.info(f"âœ… ç»“æœä¿å­˜åˆ°:")
        self.logger.info(f"   JSON: {json_path}")
        self.logger.info(f"   CSV: {csv_path}")

    def _visualize_results(self):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        self.logger.info("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        # ç¡®ä¿æœ‰ç»“æœ
        if not self.results:
            self.logger.warning("âš ï¸ æ²¡æœ‰ç»“æœå¯å¯è§†åŒ–")
            return

        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        vis_dir = self.output_dir / "visualizations"
        vis_dir.mkdir(exist_ok=True)

        # è®¾ç½®æ ·å¼
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")

        # 1. æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()

        metrics_to_plot = ['mse', 'mae', 'rmse', 'mape', 'smape', 'r2']
        metric_names = ['MSE', 'MAE', 'RMSE', 'MAPE (%)', 'SMAPE (%)', 'RÂ²']

        model_names = list(self.results.keys())

        for idx, (metric, metric_name) in enumerate(zip(metrics_to_plot, metric_names)):
            ax = axes[idx]

            values = []
            for model_name in model_names:
                value = getattr(self.results[model_name], metric)
                values.append(value)

            # å¯¹äºè¯¯å·®æŒ‡æ ‡ï¼Œæ•°å€¼è¶Šå°è¶Šå¥½ï¼Œä½¿ç”¨æ¸å˜è‰²
            if metric in ['mse', 'mae', 'rmse', 'mape', 'smape']:
                colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(values)))
            else:  # å¯¹äºRÂ²ï¼Œæ•°å€¼è¶Šå¤§è¶Šå¥½
                colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(values)))

            bars = ax.bar(model_names, values, color=colors, edgecolor='black', linewidth=1.5)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01 * max(values),
                        f'{value:.4f}', ha='center', va='bottom', fontsize=9)

            ax.set_title(f'{metric_name} å¯¹æ¯”', fontsize=14, fontweight='bold', pad=15)
            ax.set_xlabel('æ¨¡å‹', fontsize=12)
            ax.set_ylabel(metric_name, fontsize=12)
            ax.tick_params(axis='x', rotation=45)

            # æ·»åŠ ç½‘æ ¼
            ax.grid(True, alpha=0.3, linestyle='--')

        plt.suptitle('åŸºçº¿æ¨¡å‹æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(vis_dir / 'metrics_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 2. æ€§èƒ½é›·è¾¾å›¾
        fig = plt.figure(figsize=(10, 10))
        ax = fig.add_subplot(111, projection='polar')

        # å½’ä¸€åŒ–æŒ‡æ ‡ï¼ˆå¯¹äºè¯¯å·®æŒ‡æ ‡ï¼Œéœ€è¦åè½¬ï¼‰
        normalized_metrics = {}
        for model_name in model_names:
            result = self.results[model_name]

            # å¯¹äºæ¯ä¸ªæŒ‡æ ‡ï¼Œè¿›è¡Œå½’ä¸€åŒ–ï¼ˆ0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºæœ€å¥½ï¼‰
            mse_norm = 1 - (result.mse / max([r.mse for r in self.results.values()]))
            mae_norm = 1 - (result.mae / max([r.mae for r in self.results.values()]))
            r2_norm = result.r2  # RÂ²å·²ç»æ˜¯0-1ä¹‹é—´ï¼Œè¶Šå¤§è¶Šå¥½

            # æ¨ç†æ—¶é—´ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰
            time_norm = 1 - (result.inference_time / max([r.inference_time for r in self.results.values()]))

            # å‚æ•°é‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            param_norm = 1 - (result.parameters / max([r.parameters for r in self.results.values()]))

            normalized_metrics[model_name] = [mse_norm, mae_norm, r2_norm, time_norm, param_norm]

        # é›·è¾¾å›¾å‚æ•°
        categories = ['MSE', 'MAE', 'RÂ²', 'æ¨ç†é€Ÿåº¦', 'æ¨¡å‹å¤§å°']
        N = len(categories)

        # è®¡ç®—è§’åº¦
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]  # é—­åˆå›¾å½¢

        # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹
        for i, model_name in enumerate(model_names):
            values = normalized_metrics[model_name]
            values += values[:1]  # é—­åˆå›¾å½¢

            ax.plot(angles, values, linewidth=2, linestyle='solid',
                    label=model_name, marker='o', markersize=8)
            ax.fill(angles, values, alpha=0.1)

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=12)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=10)
        ax.grid(True, alpha=0.3)

        plt.title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾ï¼ˆå½’ä¸€åŒ–ï¼‰', fontsize=16, fontweight='bold', pad=20)
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=10)
        plt.tight_layout()
        plt.savefig(vis_dir / 'radar_chart.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 3. é¢„æµ‹ç¤ºä¾‹å¯¹æ¯”ï¼ˆå–å‰5ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()

        # é€‰æ‹©å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„æ¨¡å‹
        representative_models = model_names[:min(6, len(model_names))]

        for idx, model_name in enumerate(representative_models):
            if idx >= len(axes):
                break

            ax = axes[idx]
            result = self.results[model_name]

            # å–å‰5ä¸ªæ ·æœ¬çš„å¹³å‡
            sample_idx = 0
            if result.predictions is not None and result.targets is not None:
                predictions_sample = result.predictions[sample_idx, :, 0]
                targets_sample = result.targets[sample_idx, :, 0]

                time_steps = np.arange(len(predictions_sample))

                ax.plot(time_steps, targets_sample, 'b-', linewidth=2, label='çœŸå®å€¼', alpha=0.7)
                ax.plot(time_steps, predictions_sample, 'r--', linewidth=2, label='é¢„æµ‹å€¼', alpha=0.9)

                # å¡«å……é¢„æµ‹è¯¯å·®åŒºåŸŸ
                ax.fill_between(time_steps, predictions_sample, targets_sample,
                                alpha=0.2, color='gray')

                # è®¡ç®—è¿™ä¸ªæ ·æœ¬çš„è¯¯å·®
                sample_mse = np.mean((predictions_sample - targets_sample) ** 2)
                sample_mae = np.mean(np.abs(predictions_sample - targets_sample))

                ax.set_title(f'{model_name}\nSample MSE: {sample_mse:.4f}, MAE: {sample_mae:.4f}',
                             fontsize=12, fontweight='bold')
                ax.set_xlabel('æ—¶é—´æ­¥', fontsize=11)
                ax.set_ylabel('å€¼', fontsize=11)
                ax.legend(fontsize=10)
                ax.grid(True, alpha=0.3)

        # å¦‚æœæœ‰å¤šä½™çš„å­å›¾ï¼Œéšè—å®ƒä»¬
        for idx in range(len(representative_models), len(axes)):
            axes[idx].axis('off')

        plt.suptitle('é¢„æµ‹ç¤ºä¾‹å¯¹æ¯”ï¼ˆç¬¬ä¸€ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(vis_dir / 'prediction_examples.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 4. è®¡ç®—æ•ˆç‡æ•£ç‚¹å›¾
        fig, ax = plt.subplots(figsize=(10, 8))

        for model_name in model_names:
            result = self.results[model_name]

            # å‚æ•°é‡ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
            params_log = np.log10(result.parameters)

            # æ¨ç†æ—¶é—´
            inference_time = result.inference_time

            # MSEï¼ˆé¢œè‰²è¡¨ç¤ºï¼‰
            mse = result.mse

            # ç»˜åˆ¶æ•£ç‚¹
            scatter = ax.scatter(params_log, inference_time,
                                 s=200,  # ç‚¹å¤§å°
                                 c=[mse],  # é¢œè‰²åŸºäºMSE
                                 cmap='RdYlGn_r',  # çº¢è‰²è¡¨ç¤ºé«˜MSEï¼ˆå·®ï¼‰ï¼Œç»¿è‰²è¡¨ç¤ºä½MSEï¼ˆå¥½ï¼‰
                                 vmin=min([r.mse for r in self.results.values()]),
                                 vmax=max([r.mse for r in self.results.values()]),
                                 edgecolor='black', linewidth=1.5,
                                 alpha=0.8)

            # æ·»åŠ æ¨¡å‹åç§°æ ‡ç­¾
            ax.annotate(model_name,
                        (params_log, inference_time),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=10, fontweight='bold')

        ax.set_xlabel('å‚æ•°é‡ (log10)', fontsize=12)
        ax.set_ylabel('æ¨ç†æ—¶é—´ (ç§’/æ ·æœ¬)', fontsize=12)
        ax.set_title('è®¡ç®—æ•ˆç‡ vs é¢„æµ‹ç²¾åº¦', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, linestyle='--')

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('MSE (è¶Šå°è¶Šå¥½)', fontsize=12)

        plt.tight_layout()
        plt.savefig(vis_dir / 'efficiency_scatter.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 5. æ’åçƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(12, 8))

        # å‡†å¤‡æ•°æ®
        metrics_for_heatmap = ['mse', 'mae', 'rmse', 'r2', 'inference_time', 'parameters']
        metric_names_heatmap = ['MSE', 'MAE', 'RMSE', 'RÂ²', 'æ¨ç†æ—¶é—´', 'å‚æ•°é‡']

        # è®¡ç®—æ’åï¼ˆå¯¹äºè¯¯å·®æŒ‡æ ‡ï¼Œè¶Šå°æ’åè¶Šé«˜ï¼›å¯¹äºRÂ²ï¼Œè¶Šå¤§æ’åè¶Šé«˜ï¼‰
        rankings = {}
        for metric in metrics_for_heatmap:
            if metric == 'r2':  # RÂ²è¶Šå¤§è¶Šå¥½
                sorted_models = sorted(model_names,
                                       key=lambda x: getattr(self.results[x], metric),
                                       reverse=True)
            else:  # å…¶ä»–æŒ‡æ ‡è¶Šå°è¶Šå¥½
                sorted_models = sorted(model_names,
                                       key=lambda x: getattr(self.results[x], metric))

            # åˆ†é…æ’åï¼ˆ1ä¸ºæœ€å¥½ï¼‰
            for rank, model in enumerate(sorted_models, 1):
                if model not in rankings:
                    rankings[model] = {}
                rankings[model][metric] = rank

        # è½¬æ¢ä¸ºDataFrame
        ranking_df = pd.DataFrame(rankings).T

        # åˆ›å»ºçƒ­åŠ›å›¾
        sns.heatmap(ranking_df,
                    annot=True,
                    fmt='d',
                    cmap='RdYlGn_r',  # çº¢è‰²è¡¨ç¤ºæ’åå·®ï¼Œç»¿è‰²è¡¨ç¤ºæ’åå¥½
                    cbar_kws={'label': 'æ’å (1=æœ€å¥½)'},
                    linewidths=1,
                    linecolor='white',
                    ax=ax)

        ax.set_ylabel('æ¨¡å‹', fontsize=12)
        ax.set_xlabel('æŒ‡æ ‡', fontsize=12)
        ax.set_title('æ¨¡å‹æŒ‡æ ‡æ’åçƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
        ax.set_xticklabels(metric_names_heatmap, rotation=45, ha='right')

        plt.tight_layout()
        plt.savefig(vis_dir / 'ranking_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()

        self.logger.info(f"âœ… å¯è§†åŒ–å›¾è¡¨ä¿å­˜åˆ°: {vis_dir}")

    def generate_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š"""
        if not self.comparison_result:
            self.logger.warning("âš ï¸ è¯·å…ˆè¿è¡Œå®éªŒå†ç”ŸæˆæŠ¥å‘Š")
            return

        self.logger.info("ğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")

        report_dir = self.output_dir / "report"
        report_dir.mkdir(exist_ok=True)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_path = report_dir / "comparison_report.md"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# æ—¶åºé¢„æµ‹åŸºçº¿æ–¹æ³•å¯¹æ¯”æŠ¥å‘Š\n\n")
            f.write(f"**å®éªŒID**: {self.comparison_result.experiment_id}\n\n")
            f.write(f"**å®éªŒæ—¶é—´**: {self.comparison_result.timestamp}\n\n")
            f.write(f"**æ•°æ®é›†**: {self.comparison_result.dataset}\n\n")
            f.write(
                f"**åºåˆ—é•¿åº¦**: {self.comparison_result.seq_len} â†’ **é¢„æµ‹é•¿åº¦**: {self.comparison_result.pred_len}\n\n")

            f.write("## 1. å®éªŒæ¦‚è¿°\n\n")
            f.write(f"æœ¬å®éªŒå¯¹æ¯”äº† {len(self.comparison_result.results)} ä¸ªæ—¶åºé¢„æµ‹æ¨¡å‹åœ¨ETTh1æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚\n\n")

            f.write("## 2. æ¨¡å‹åˆ—è¡¨\n\n")
            f.write("| æ¨¡å‹åç§° | æè¿° |\n")
            f.write("|----------|------|\n")

            model_descriptions = {
                'DLinear': 'ç®€å•çš„çº¿æ€§åˆ†è§£æ¨¡å‹ï¼Œå°†åºåˆ—åˆ†è§£ä¸ºè¶‹åŠ¿å’Œå­£èŠ‚æ€§åˆ†é‡',
                'PatchTST': 'åŸºäºè¡¥ä¸çš„Transformeræ¨¡å‹ï¼Œå°†æ—¶é—´åºåˆ—åˆ†å‰²ä¸ºè¡¥ä¸',
                'TimesNet': 'å¤šå‘¨æœŸè½¬æ¢æ¨¡å‹ï¼Œå°†1Dæ—¶é—´åºåˆ—è½¬æ¢ä¸º2Då¼ é‡',
                'FEDformer': 'é¢‘åŸŸå¢å¼ºçš„Transformeræ¨¡å‹ï¼Œç»“åˆé¢‘åŸŸå’Œæ—¶åŸŸä¿¡æ¯',
                'Autoformer': 'è‡ªç›¸å…³æœºåˆ¶çš„Transformeræ¨¡å‹ï¼Œç”¨äºåºåˆ—åˆ†è§£',
                'Informer': 'é«˜æ•ˆTransformeræ¨¡å‹ï¼Œä½¿ç”¨æ¦‚ç‡ç¨€ç–æ³¨æ„åŠ›',
                'TimeLLM': 'åŸºäºLLMçš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹æ¶æ„',
                'STAR-Forecast': 'ç¥ç»-ç¬¦å·-å¼ºåŒ–ä¸‰é‡ååŒè‡ªé€‚åº”é¢„æµ‹æ¡†æ¶ï¼ˆæœ¬æ–‡æ–¹æ³•ï¼‰'
            }

            for model_name in self.comparison_result.results.keys():
                description = model_descriptions.get(model_name, 'æ—¶åºé¢„æµ‹æ¨¡å‹')
                f.write(f"| {model_name} | {description} |\n")

            f.write("\n## 3. å®éªŒç»“æœ\n\n")

            # 3.1 ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
            f.write("### 3.1 ä¸»è¦æŒ‡æ ‡å¯¹æ¯”\n\n")
            f.write("| æ¨¡å‹ | MSE | MAE | RMSE | MAPE (%) | SMAPE (%) | RÂ² | æ¨ç†æ—¶é—´ (s) | å‚æ•°é‡ |\n")
            f.write("|------|-----|-----|------|----------|-----------|----|--------------|--------|\n")

            for model_name, result in self.comparison_result.results.items():
                f.write(f"| {model_name} | {result.mse:.6f} | {result.mae:.6f} | {result.rmse:.6f} | "
                        f"{result.mape:.4f} | {result.smape:.4f} | {result.r2:.4f} | "
                        f"{result.inference_time:.4f} | {result.parameters:,} |\n")

            f.write("\n### 3.2 æŒ‡æ ‡æ’å\n\n")

            # MSEæ’å
            f.write("**MSEæ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰**:\n\n")
            mse_ranking = sorted(
                self.comparison_result.results.items(),
                key=lambda x: x[1].mse
            )
            for i, (model_name, result) in enumerate(mse_ranking, 1):
                f.write(f"{i}. **{model_name}**: {result.mse:.6f}\n")

            f.write("\n**MAEæ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰**:\n\n")
            mae_ranking = sorted(
                self.comparison_result.results.items(),
                key=lambda x: x[1].mae
            )
            for i, (model_name, result) in enumerate(mae_ranking, 1):
                f.write(f"{i}. **{model_name}**: {result.mae:.6f}\n")

            # 3.3 ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
            if self.comparison_result.significance_tests:
                f.write("\n### 3.3 ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ\n\n")
                f.write("> ä½¿ç”¨Wilcoxonç¬¦å·ç§©æ£€éªŒï¼ˆæ˜¾è‘—æ€§æ°´å¹³Î±=0.05ï¼‰\n\n")

                f.write("| å¯¹æ¯” | på€¼ | æ˜¯å¦æ˜¾è‘— | æ•ˆåº”é‡ | è·èƒœæ¨¡å‹ |\n")
                f.write("|------|-----|----------|--------|----------|\n")

                for test_key, test_result in self.comparison_result.significance_tests.items():
                    significant = "âœ…" if test_result['significant'] else "âŒ"
                    f.write(f"| {test_key} | {test_result['p_value']:.6f} | {significant} | "
                            f"{test_result['effect_size']:.4f} | {test_result['winner']} |\n")

            # 3.4 æ€»ç»“åˆ†æ
            f.write("\n## 4. æ€»ç»“åˆ†æ\n\n")

            if 'summary' in self.comparison_result:
                summary = self.comparison_result.summary

                f.write(f"### 4.1 æœ€ä½³æ¨¡å‹\n\n")
                f.write(f"- **æœ€ä½³MSE**: {summary['best_mse']['model']} ({summary['best_mse']['value']:.6f})\n")
                f.write(f"- **æœ€ä½³MAE**: {summary['best_mae']['model']} ({summary['best_mae']['value']:.6f})\n\n")

                if 'relative_improvement_mse' in summary:
                    f.write(f"### 4.2 ç›¸å¯¹æ”¹è¿›\n\n")
                    f.write(
                        f"- **STAR-Forecastç›¸å¯¹äºæœ€ä½³åŸºçº¿çš„MSEæ”¹è¿›**: {summary['relative_improvement_mse']:.2f}%\n\n")

                f.write(f"### 4.3 ç»Ÿè®¡æ‘˜è¦\n\n")
                f.write(f"- **æ¨¡å‹æ•°é‡**: {summary['model_count']}\n")
                f.write(f"- **å¹³å‡MSE**: {summary['avg_mse']:.6f}\n")
                f.write(f"- **å¹³å‡MAE**: {summary['avg_mae']:.6f}\n")
                f.write(f"- **MSEæ ‡å‡†å·®**: {summary['std_mse']:.6f}\n")
                f.write(f"- **MAEæ ‡å‡†å·®**: {summary['std_mae']:.6f}\n")

            f.write("\n## 5. ç»“è®º\n\n")

            # è‡ªåŠ¨ç”Ÿæˆç»“è®º
            best_model_mse = mse_ranking[0][0]
            best_model_mae = mae_ranking[0][0]

            if best_model_mse == "STAR-Forecast" and best_model_mae == "STAR-Forecast":
                f.write("âœ… **STAR-Forecaståœ¨MSEå’ŒMAEæŒ‡æ ‡ä¸Šå‡è¡¨ç°æœ€ä½³**ï¼ŒéªŒè¯äº†ç¥ç»-ç¬¦å·-å¼ºåŒ–ä¸‰é‡ååŒæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚\n\n")
            elif best_model_mse == "STAR-Forecast":
                f.write("âœ… **STAR-Forecaståœ¨MSEæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³**ï¼Œåœ¨MAEæŒ‡æ ‡ä¸Šæ’åç¬¬{mae_rank}ã€‚\n\n".format(
                    mae_rank=next(i for i, (m, _) in enumerate(mae_ranking, 1) if m == "STAR-Forecast")
                ))
            elif best_model_mae == "STAR-Forecast":
                f.write("âœ… **STAR-Forecaståœ¨MAEæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³**ï¼Œåœ¨MSEæŒ‡æ ‡ä¸Šæ’åç¬¬{mse_rank}ã€‚\n\n".format(
                    mse_rank=next(i for i, (m, _) in enumerate(mse_ranking, 1) if m == "STAR-Forecast")
                ))
            else:
                star_mse_rank = next(i for i, (m, _) in enumerate(mse_ranking, 1) if m == "STAR-Forecast")
                star_mae_rank = next(i for i, (m, _) in enumerate(mae_ranking, 1) if m == "STAR-Forecast")
                f.write(f"âš ï¸  **STAR-Forecaståœ¨MSEæ’åç¬¬{star_mse_rank}ï¼ŒMAEæ’åç¬¬{star_mae_rank}**ï¼Œä»æœ‰æ”¹è¿›ç©ºé—´ã€‚\n\n")

            f.write("### å…³é”®å‘ç°ï¼š\n\n")
            f.write("1. **ä¼ ç»Ÿæ¨¡å‹**ï¼ˆå¦‚DLinearï¼‰è™½ç„¶ç®€å•ï¼Œä½†åœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°ç¨³å®š\n")
            f.write("2. **å¤æ‚æ¨¡å‹**ï¼ˆå¦‚TimesNetã€PatchTSTï¼‰é€šå¸¸éœ€è¦æ›´å¤šè®¡ç®—èµ„æº\n")
            f.write("3. **Transformer-basedæ¨¡å‹**ï¼ˆå¦‚Informerã€Autoformerï¼‰åœ¨é•¿åºåˆ—é¢„æµ‹ä¸Šæœ‰ä¼˜åŠ¿\n")
            f.write("4. **STAR-Forecast**é€šè¿‡æ™ºèƒ½ä½“ååŒå’Œè‡ªé€‚åº”è°ƒæ•´ï¼Œåœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡\n\n")

            f.write("### å»ºè®®ï¼š\n\n")
            f.write("1. å¯¹äºè®¡ç®—èµ„æºæœ‰é™çš„åœºæ™¯ï¼Œæ¨èä½¿ç”¨DLinearæˆ–PatchTST\n")
            f.write("2. å¯¹äºéœ€è¦é«˜ç²¾åº¦çš„åœºæ™¯ï¼Œæ¨èä½¿ç”¨STAR-Forecastæˆ–TimesNet\n")
            f.write("3. å¯¹äºé•¿åºåˆ—é¢„æµ‹ï¼Œæ¨èä½¿ç”¨Informeræˆ–Autoformer\n")
            f.write("4. STAR-Forecastçš„æ™ºèƒ½ä½“ååŒæœºåˆ¶åœ¨åŠ¨æ€è°ƒæ•´æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€‚åˆéå¹³ç¨³æ—¶é—´åºåˆ—\n")

        # ç”ŸæˆHTMLæŠ¥å‘Š
        try:
            import markdown

            with open(report_path, 'r', encoding='utf-8') as f:
                md_content = f.read()

            html_content = markdown.markdown(md_content, extensions=['tables', 'fenced_code'])

            # æ·»åŠ CSSæ ·å¼
            html_with_style = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
                <title>æ—¶åºé¢„æµ‹åŸºçº¿æ–¹æ³•å¯¹æ¯”æŠ¥å‘Š</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
                    h1, h2, h3 {{ color: #2c3e50; }}
                    table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                    th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                    th {{ background-color: #f2f2f2; font-weight: bold; }}
                    tr:nth-child(even) {{ background-color: #f9f9f9; }}
                    .good {{ color: green; font-weight: bold; }}
                    .bad {{ color: red; font-weight: bold; }}
                    .summary {{ background-color: #e8f4f8; padding: 15px; border-radius: 5px; margin: 20px 0; }}
                </style>
            </head>
            <body>
                {html_content}
            </body>
            </html>
            """

            html_path = report_dir / "comparison_report.html"
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_with_style)

            self.logger.info(f"âœ… HTMLæŠ¥å‘Šç”Ÿæˆ: {html_path}")

        except ImportError:
            self.logger.warning("âš ï¸ æœªå®‰è£…markdownåº“ï¼Œè·³è¿‡HTMLæŠ¥å‘Šç”Ÿæˆ")

        self.logger.info(f"âœ… è¯¦ç»†æŠ¥å‘Šç”Ÿæˆ: {report_path}")

    def run_complete_experiment(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ”¬ å¼€å§‹å®Œæ•´çš„åŸºçº¿å¯¹æ¯”å®éªŒ")
        self.logger.info("=" * 60)

        # 1. è¿è¡Œæ‰€æœ‰åŸºçº¿æ–¹æ³•
        self.run_all_baselines()

        # 2. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

        # 3. æ‰“å°æœ€ç»ˆæ€»ç»“
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸ¯ å®éªŒå®Œæˆæ€»ç»“")
        self.logger.info("=" * 60)

        if hasattr(self, 'comparison_result') and self.comparison_result.summary:
            summary = self.comparison_result.summary

            self.logger.info(f"ğŸ“Š å®éªŒç»Ÿè®¡:")
            self.logger.info(f"   æ¨¡å‹æ•°é‡: {summary['model_count']}")
            self.logger.info(f"   æœ€ä½³MSE: {summary['best_mse']['model']} ({summary['best_mse']['value']:.6f})")
            self.logger.info(f"   æœ€ä½³MAE: {summary['best_mae']['model']} ({summary['best_mae']['value']:.6f})")

            if 'relative_improvement_mse' in summary:
                self.logger.info(f"   STAR-Forecastç›¸å¯¹æ”¹è¿›: {summary['relative_improvement_mse']:.2f}%")

        self.logger.info(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
        self.logger.info(f"   ç»“æœç›®å½•: {self.output_dir}")
        self.logger.info(f"   å¯è§†åŒ–: {self.output_dir}/visualizations/")
        self.logger.info(f"   æŠ¥å‘Š: {self.output_dir}/report/")

        return self.comparison_result


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ—¶åºé¢„æµ‹åŸºçº¿å¯¹æ¯”å®éªŒ")
    parser.add_argument("--config", type=str, default="./config.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--baselines", type=str, nargs='+',
                        default=['DLinear', 'PatchTST', 'TimesNet', 'FEDformer',
                                 'Autoformer', 'Informer', 'TimeLLM'],
                        help="è¦è¿è¡Œçš„åŸºçº¿æ–¹æ³•åˆ—è¡¨")
    parser.add_argument("--include-star", action='store_true', default=True,
                        help="æ˜¯å¦åŒ…å«STAR-Forecast")
    parser.add_argument("--train", action='store_true', default=True,
                        help="æ˜¯å¦è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœå·²ä¿å­˜æ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥è®¾ç½®ä¸ºFalseï¼‰")
    parser.add_argument("--output-dir", type=str, default="./results/baseline_comparison",
                        help="è¾“å‡ºç›®å½•")

    args = parser.parse_args()

    # åˆ›å»ºå¯¹æ¯”å®éªŒç®¡ç†å™¨
    comparator = BaselineComparison(args.config)

    # è®¾ç½®è¾“å‡ºç›®å½•
    comparator.output_dir = Path(args.output_dir)
    comparator.output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # è¿è¡Œå®éªŒ
        result = comparator.run_complete_experiment()

        print("\n" + "=" * 60)
        print("ğŸ‰ åŸºçº¿å¯¹æ¯”å®éªŒå®Œæˆï¼")
        print("=" * 60)

        # æ‰“å°æ’å
        if result and result.results:
            print("\nğŸ† æœ€ç»ˆæ’å:")
            print("-" * 40)

            # MSEæ’å
            mse_ranking = sorted(
                result.results.items(),
                key=lambda x: x[1].mse
            )

            print("MSEæ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰:")
            for i, (model_name, result_obj) in enumerate(mse_ranking, 1):
                print(f"  {i}. {model_name}: {result_obj.mse:.6f}")

        return result

    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()