"""
æ¶ˆèå®éªŒç®¡ç†å™¨ - ç³»ç»Ÿæ€§åœ°æµ‹è¯•STAR-Forecastå„ç»„ä»¶çš„ä½œç”¨
æ”¯æŒå¤šç§æ¶ˆèå˜ä½“ã€ç»Ÿè®¡åˆ†æã€ç»“æœå¯è§†åŒ–
"""
import json
import yaml
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
import logging
import warnings

warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import itertools
from enum import Enum
import uuid
import pickle

# å¯¼å…¥é¡¹ç›®æ¨¡å—
import sys

sys.path.append('..')
from training.trainer import STARForecastTrainer
from models.istr import ISTRNetwork
from client.api_client import AgentLightningClient
from agents.autogen_system import AutoGenMultiAgentSystem


class AblationVariant(Enum):
    """æ¶ˆèå˜ä½“ç±»å‹"""
    FULL_MODEL = "full"  # å®Œæ•´æ¨¡å‹
    NO_AUTOGEN = "no_autogen"  # æ— AutoGenæ™ºèƒ½ä½“
    NO_AGENT_LIGHTNING = "no_agent_lightning"  # æ— Agent Lightning
    NO_ISTR = "no_istr"  # æ— ISTRç½‘ç»œï¼ˆåŸºç¡€TCNï¼‰
    NO_LAPLACIAN = "no_laplacian"  # æ— æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–
    NO_SPECTRAL_GATE = "no_spectral_gate"  # æ— è°±é—¨æ§
    FROZEN_ISTR = "frozen_istr"  # ISTRå®Œå…¨å†»ç»“
    SINGLE_AGENT = "single_agent"  # å•æ™ºèƒ½ä½“ï¼ˆéå¤šæ™ºèƒ½ä½“ï¼‰
    NO_SEMANTIC_REWARD = "no_semantic_reward"  # æ— è¯­ä¹‰å¥–åŠ±
    SIMPLE_BASELINE = "simple_baseline"  # ç®€å•åŸºçº¿ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰


@dataclass
class AblationConfig:
    """æ¶ˆèå®éªŒé…ç½®"""
    variant: AblationVariant
    description: str
    config_modifications: Dict[str, Any]
    training_epochs: int = 50  # æ¶ˆèå®éªŒç”¨è¾ƒå°‘epochs
    num_runs: int = 3  # æ¯ä¸ªå˜ä½“è¿è¡Œæ¬¡æ•°ï¼ˆå‡å°‘éšæœºæ€§ï¼‰
    random_seeds: List[int] = field(default_factory=lambda: [42, 43, 44])


@dataclass
class AblationResult:
    """æ¶ˆèå®éªŒç»“æœ"""
    variant: AblationVariant
    run_id: str
    seed: int
    config: Dict[str, Any]
    training_history: Dict[str, List[float]]
    test_metrics: Dict[str, float]
    training_time: float  # ç§’
    resource_usage: Dict[str, float]  # GPUå†…å­˜ã€æ˜¾å­˜ç­‰
    created_at: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'variant': self.variant.value,
            'run_id': self.run_id,
            'seed': self.seed,
            'config': self.config,
            'test_metrics': self.test_metrics,
            'training_time': self.training_time,
            'resource_usage': self.resource_usage,
            'created_at': self.created_at.isoformat()
        }


@dataclass
class AblationComparison:
    """æ¶ˆèå®éªŒå¯¹æ¯”ç»“æœ"""
    experiment_id: str
    variants: List[AblationVariant]
    results: Dict[str, List[AblationResult]]  # variant -> list of results
    summary_stats: Dict[str, Dict[str, float]]
    statistical_tests: Dict[str, Dict[str, float]]
    created_at: datetime = field(default_factory=datetime.now)

    def to_dataframe(self) -> pd.DataFrame:
        """è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ"""
        rows = []

        for variant, result_list in self.results.items():
            for result in result_list:
                row = {
                    'experiment_id': self.experiment_id,
                    'variant': variant,
                    'run_id': result.run_id,
                    'seed': result.seed,
                    'training_time': result.training_time,
                    **result.test_metrics
                }
                rows.append(row)

        return pd.DataFrame(rows)


class ResourceMonitor:
    """èµ„æºä½¿ç”¨ç›‘æ§å™¨"""

    def __init__(self):
        try:
            import pynvml
            pynvml.nvmlInit()
            self.has_gpu = True
            self.gpu_count = pynvml.nvmlDeviceGetCount()
        except:
            self.has_gpu = False
            self.gpu_count = 0

        self.start_time = None
        self.max_gpu_memory = 0
        self.max_cpu_memory = 0

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = datetime.now()

        if self.has_gpu:
            self._reset_gpu_stats()

        self._reset_cpu_stats()

    def stop_monitoring(self) -> Dict[str, float]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡"""
        if not self.start_time:
            return {}

        # è®¡ç®—è¿è¡Œæ—¶é—´
        duration = (datetime.now() - self.start_time).total_seconds()

        # è·å–èµ„æºä½¿ç”¨å³°å€¼
        resource_stats = {
            'training_time_seconds': duration,
            'has_gpu': self.has_gpu
        }

        if self.has_gpu:
            gpu_stats = self._get_gpu_stats()
            resource_stats.update(gpu_stats)

        cpu_stats = self._get_cpu_stats()
        resource_stats.update(cpu_stats)

        return resource_stats

    def _reset_gpu_stats(self):
        """é‡ç½®GPUç»Ÿè®¡"""
        self.max_gpu_memory = 0

    def _reset_cpu_stats(self):
        """é‡ç½®CPUç»Ÿè®¡"""
        self.max_cpu_memory = 0

    def _get_gpu_stats(self) -> Dict[str, float]:
        """è·å–GPUç»Ÿè®¡"""
        try:
            import pynvml

            gpu_stats = {}
            total_memory = 0
            max_used = 0

            for i in range(self.gpu_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                info = pynvml.nvmlDeviceGetMemoryInfo(handle)

                total_memory += info.total / 1024 ** 3  # GB
                used = info.used / 1024 ** 3
                max_used = max(max_used, used)

            gpu_stats['gpu_memory_total_gb'] = total_memory
            gpu_stats['gpu_memory_max_used_gb'] = max_used
            gpu_stats['gpu_memory_utilization'] = max_used / total_memory if total_memory > 0 else 0

            return gpu_stats

        except Exception as e:
            logging.warning(f"æ— æ³•è·å–GPUç»Ÿè®¡: {e}")
            return {}

    def _get_cpu_stats(self) -> Dict[str, float]:
        """è·å–CPUç»Ÿè®¡"""
        try:
            import psutil
            import os

            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()

            cpu_stats = {
                'cpu_memory_rss_gb': memory_info.rss / 1024 ** 3,
                'cpu_memory_vms_gb': memory_info.vms / 1024 ** 3,
                'cpu_percent': process.cpu_percent(interval=1)
            }

            return cpu_stats

        except Exception as e:
            logging.warning(f"æ— æ³•è·å–CPUç»Ÿè®¡: {e}")
            return {}


class AblationStudyManager:
    """
    æ¶ˆèå®éªŒç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨ç”Ÿæˆä¸åŒæ¶ˆèå˜ä½“çš„é…ç½®
    2. å¹¶è¡Œ/é¡ºåºè¿è¡Œå¤šä¸ªå®éªŒ
    3. æ”¶é›†å’Œåˆ†æå®éªŒç»“æœ
    4. ç”Ÿæˆç»Ÿè®¡æ£€éªŒå’Œå¯è§†åŒ–
    5. ä¿å­˜å’Œæ¯”è¾ƒå®éªŒç»“æœ
    """

    def __init__(self, base_config_path: str = "./config.yaml"):
        self.base_config = self._load_config(base_config_path)
        self.logger = logging.getLogger(__name__)

        # å®éªŒç»“æœå­˜å‚¨
        self.results_dir = Path("./experiments/ablation_results")
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # å®éªŒå†å²
        self.experiment_history: Dict[str, AblationComparison] = {}

        # èµ„æºç›‘æ§å™¨
        self.resource_monitor = ResourceMonitor()

        self.logger.info("ğŸ”¬ æ¶ˆèå®éªŒç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def get_ablation_variants(self) -> Dict[AblationVariant, AblationConfig]:
        """è·å–æ‰€æœ‰æ¶ˆèå˜ä½“é…ç½®"""
        variants = {}

        # 1. å®Œæ•´æ¨¡å‹ï¼ˆåŸºå‡†ï¼‰
        variants[AblationVariant.FULL_MODEL] = AblationConfig(
            variant=AblationVariant.FULL_MODEL,
            description="å®Œæ•´STAR-Forecastæ¨¡å‹",
            config_modifications={}
        )

        # 2. æ— AutoGenæ™ºèƒ½ä½“
        variants[AblationVariant.NO_AUTOGEN] = AblationConfig(
            variant=AblationVariant.NO_AUTOGEN,
            description="æ— AutoGenå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
            config_modifications={
                'autogen': {
                    'trigger': {'check_interval': 1000000},  # æå¤§å€¼ï¼ŒåŸºæœ¬ä¸è§¦å‘
                    'conversation': {'max_rounds': 0}
                }
            }
        )

        # 3. æ— Agent Lightning
        variants[AblationVariant.NO_AGENT_LIGHTNING] = AblationConfig(
            variant=AblationVariant.NO_AGENT_LIGHTNING,
            description="æ— Agent Lightningå¼ºåŒ–å­¦ä¹ ",
            config_modifications={
                'agent_lightning': {
                    'client': {'fallback_enabled': False},
                    'rl': {'reward': {'weights': {'semantic': 0.0, 'constraint': 0.0}}
                           }
                }
            }
        )

        # 4. æ— ISTRç½‘ç»œï¼ˆä½¿ç”¨ç®€å•TCNï¼‰
        variants[AblationVariant.NO_ISTR] = AblationConfig(
            variant=AblationVariant.NO_ISTR,
            description="æ— ISTRç½‘ç»œï¼Œä½¿ç”¨æ ‡å‡†TCN",
            config_modifications={
                'istr': {
                    'tcn': {'num_blocks': 1},
                    'spectral_gate': {'enabled': False},
                    'laplacian': {'enabled': False},
                    'trainable_ratio': 1.0  # å…¨éƒ¨å¯è®­ç»ƒ
                }
            }
        )

        # 5. æ— æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–
        variants[AblationVariant.NO_LAPLACIAN] = AblationConfig(
            variant=AblationVariant.NO_LAPLACIAN,
            description="æ— æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–",
            config_modifications={
                'istr': {'laplacian': {'enabled': False}}
            }
        )

        # 6. æ— è°±é—¨æ§
        variants[AblationVariant.NO_SPECTRAL_GATE] = AblationConfig(
            variant=AblationVariant.NO_SPECTRAL_GATE,
            description="æ— è°±é—¨æ§æœºåˆ¶",
            config_modifications={
                'istr': {'spectral_gate': {'enabled': False}}
            }
        )

        # 7. å†»ç»“ISTR
        variants[AblationVariant.FROZEN_ISTR] = AblationConfig(
            variant=AblationVariant.FROZEN_ISTR,
            description="ISTRç½‘ç»œå®Œå…¨å†»ç»“",
            config_modifications={
                'istr': {'trainable_ratio': 0.0}
            }
        )

        # 8. å•æ™ºèƒ½ä½“
        variants[AblationVariant.SINGLE_AGENT] = AblationConfig(
            variant=AblationVariant.SINGLE_AGENT,
            description="å•æ™ºèƒ½ä½“ï¼ˆéå¤šæ™ºèƒ½ä½“ååŒï¼‰",
            config_modifications={
                'autogen': {
                    'agents': {
                        'architect': None,  # ç¦ç”¨æ¶æ„å¸ˆ
                        'critic': None  # ç¦ç”¨æ‰¹è¯„å®¶
                    }
                }
            }
        )

        # 9. æ— è¯­ä¹‰å¥–åŠ±
        variants[AblationVariant.NO_SEMANTIC_REWARD] = AblationConfig(
            variant=AblationVariant.NO_SEMANTIC_REWARD,
            description="å¼ºåŒ–å­¦ä¹ ä¸­æ— è¯­ä¹‰å¥–åŠ±",
            config_modifications={
                'agent_lightning': {
                    'rl': {'reward': {'weights': {'semantic': 0.0}}}
                }
            }
        )

        # 10. ç®€å•åŸºçº¿
        variants[AblationVariant.SIMPLE_BASELINE] = AblationConfig(
            variant=AblationVariant.SIMPLE_BASELINE,
            description="ç®€å•çº¿æ€§æ¨¡å‹åŸºçº¿",
            config_modifications={
                'istr': {
                    'hidden_dim': 16,
                    'tcn': {'num_blocks': 0},
                    'spectral_gate': {'enabled': False},
                    'laplacian': {'enabled': False}
                },
                'predictor': {
                    'type': 'linear',
                    'hidden_dims': []
                }
            }
        )

        return variants

    def run_ablation_experiment(self,
                                variants: List[AblationVariant] = None,
                                data_path: str = "./data/ETTh1.csv",
                                experiment_name: str = None) -> AblationComparison:
        """
        è¿è¡Œæ¶ˆèå®éªŒ

        Args:
            variants: è¦æµ‹è¯•çš„å˜ä½“åˆ—è¡¨ï¼ˆNoneåˆ™æµ‹è¯•æ‰€æœ‰ï¼‰
            data_path: æ•°æ®è·¯å¾„
            experiment_name: å®éªŒåç§°

        Returns:
            æ¶ˆèå®éªŒå¯¹æ¯”ç»“æœ
        """
        # è·å–å˜ä½“é…ç½®
        all_variants = self.get_ablation_variants()

        if variants is None:
            variants_to_test = list(all_variants.keys())
        else:
            variants_to_test = variants

        # åˆ›å»ºå®éªŒID
        experiment_id = experiment_name or f"ablation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        self.logger.info(f"ğŸ§ª å¼€å§‹æ¶ˆèå®éªŒ: {experiment_id}")
        self.logger.info(f"   æµ‹è¯•å˜ä½“: {[v.value for v in variants_to_test]}")

        # è¿è¡Œæ¯ä¸ªå˜ä½“
        results = {}

        for variant in variants_to_test:
            if variant not in all_variants:
                self.logger.warning(f"æœªçŸ¥å˜ä½“: {variant}")
                continue

            config = all_variants[variant]
            variant_results = []

            self.logger.info(f"\nğŸ”¬ æµ‹è¯•å˜ä½“: {variant.value}")
            self.logger.info(f"   æè¿°: {config.description}")

            for run_idx, seed in enumerate(config.random_seeds[:config.num_runs]):
                run_id = f"{variant.value}_run{run_idx + 1}"

                self.logger.info(f"   è¿è¡Œ {run_id} (ç§å­: {seed})...")

                # è¿è¡Œå•ä¸ªå®éªŒ
                result = self._run_single_experiment(
                    config, seed, data_path, run_id
                )

                if result:
                    variant_results.append(result)
                    self.logger.info(f"      MSE: {result.test_metrics.get('mse', 0):.6f}, "
                                     f"MAE: {result.test_metrics.get('mae', 0):.6f}, "
                                     f"æ—¶é—´: {result.training_time:.1f}s")

            results[variant.value] = variant_results

        # åˆ†æç»“æœ
        comparison = self._analyze_results(experiment_id, results)

        # ä¿å­˜ç»“æœ
        self._save_experiment(comparison)

        # å¯è§†åŒ–
        self._visualize_results(comparison)

        self.logger.info(f"âœ… æ¶ˆèå®éªŒå®Œæˆ: {experiment_id}")

        return comparison

    def _run_single_experiment(self,
                               ablation_config: AblationConfig,
                               seed: int,
                               data_path: str,
                               run_id: str) -> Optional[AblationResult]:
        """è¿è¡Œå•ä¸ªæ¶ˆèå®éªŒ"""
        try:
            # åˆ›å»ºé…ç½®å‰¯æœ¬å¹¶åº”ç”¨ä¿®æ”¹
            config = self._deep_copy_config(self.base_config)
            config = self._apply_config_modifications(config, ablation_config.config_modifications)

            # è®¾ç½®éšæœºç§å­
            config['experiment']['seed'] = seed

            # å‡å°‘è®­ç»ƒè½®æ¬¡ä»¥åŠ å¿«æ¶ˆèå®éªŒ
            if 'training' in config:
                config['training']['epochs'] = ablation_config.training_epochs

            # å¼€å§‹èµ„æºç›‘æ§
            self.resource_monitor.start_monitoring()
            start_time = datetime.now()

            # åˆ›å»ºå¹¶è¿è¡Œè®­ç»ƒå™¨
            trainer = STARForecastTrainer(config)
            trainer.build_models()
            trainer.build_optimizer()

            # å¯¹äºæŸäº›å˜ä½“ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            if ablation_config.variant == AblationVariant.NO_AUTOGEN:
                # ä¸åˆå§‹åŒ–AutoGenç³»ç»Ÿ
                pass
            else:
                trainer.initialize_agents()

            # è®­ç»ƒæ¨¡å‹
            test_metrics = trainer.train(data_path)

            # åœæ­¢èµ„æºç›‘æ§
            training_time = (datetime.now() - start_time).total_seconds()
            resource_usage = self.resource_monitor.stop_monitoring()

            # åˆ›å»ºç»“æœ
            result = AblationResult(
                variant=ablation_config.variant,
                run_id=run_id,
                seed=seed,
                config=config,
                training_history=getattr(trainer, 'training_history', {}),
                test_metrics=test_metrics,
                training_time=training_time,
                resource_usage=resource_usage
            )

            return result

        except Exception as e:
            self.logger.error(f"âŒ å®éªŒ {run_id} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _deep_copy_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±æ‹·è´é…ç½®"""
        import copy
        return copy.deepcopy(config)

    def _apply_config_modifications(self,
                                    config: Dict[str, Any],
                                    modifications: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨é…ç½®ä¿®æ”¹"""
        if not modifications:
            return config

        def update_dict(d, u):
            for k, v in u.items():
                if isinstance(v, dict):
                    d[k] = update_dict(d.get(k, {}), v)
                else:
                    d[k] = v
            return d

        return update_dict(config, modifications)

    def _analyze_results(self,
                         experiment_id: str,
                         results: Dict[str, List[AblationResult]]) -> AblationComparison:
        """åˆ†æå®éªŒç»“æœ"""
        summary_stats = {}
        statistical_tests = {}

        # æå–ä¸»è¦æŒ‡æ ‡
        primary_metric = 'mse'  # ä¸»è¦æ¯”è¾ƒMSE

        for variant, result_list in results.items():
            if not result_list:
                continue

            # è®¡ç®—ç»Ÿè®¡é‡
            metrics = []
            for result in result_list:
                if primary_metric in result.test_metrics:
                    metrics.append(result.test_metrics[primary_metric])

            if metrics:
                summary_stats[variant] = {
                    'mean': np.mean(metrics),
                    'std': np.std(metrics),
                    'min': np.min(metrics),
                    'max': np.max(metrics),
                    'median': np.median(metrics),
                    'count': len(metrics)
                }

        # ç»Ÿè®¡æ£€éªŒï¼ˆä¸å®Œæ•´æ¨¡å‹æ¯”è¾ƒï¼‰
        if 'full' in results and results['full']:
            full_model_metrics = []
            for result in results['full']:
                if primary_metric in result.test_metrics:
                    full_model_metrics.append(result.test_metrics[primary_metric])

            if full_model_metrics:
                for variant, result_list in results.items():
                    if variant == 'full' or not result_list:
                        continue

                    other_metrics = []
                    for result in result_list:
                        if primary_metric in result.test_metrics:
                            other_metrics.append(result.test_metrics[primary_metric])

                    if other_metrics:
                        # tæ£€éªŒ
                        t_stat, p_value = stats.ttest_ind(
                            full_model_metrics,
                            other_metrics,
                            equal_var=False  # Welch's t-test
                        )

                        # Wilcoxonç§©å’Œæ£€éªŒ
                        if len(full_model_metrics) == len(other_metrics):
                            w_stat, w_pvalue = stats.wilcoxon(
                                full_model_metrics,
                                other_metrics
                            )
                        else:
                            w_stat, w_pvalue = stats.ranksums(
                                full_model_metrics,
                                other_metrics
                            )

                        statistical_tests[variant] = {
                            't_test': {
                                'statistic': float(t_stat),
                                'p_value': float(p_value),
                                'significant': p_value < 0.05
                            },
                            'wilcoxon': {
                                'statistic': float(w_stat),
                                'p_value': float(w_pvalue),
                                'significant': w_pvalue < 0.05
                            },
                            'effect_size': self._calculate_effect_size(
                                full_model_metrics, other_metrics
                            )
                        }

        # åˆ›å»ºå¯¹æ¯”ç»“æœ
        comparison = AblationComparison(
            experiment_id=experiment_id,
            variants=[AblationVariant(v) for v in results.keys()],
            results=results,
            summary_stats=summary_stats,
            statistical_tests=statistical_tests
        )

        return comparison

    def _calculate_effect_size(self, group1: List[float], group2: List[float]) -> float:
        """è®¡ç®—æ•ˆåº”å¤§å°ï¼ˆCohen's dï¼‰"""
        if len(group1) == 0 or len(group2) == 0:
            return 0.0

        mean1, mean2 = np.mean(group1), np.mean(group2)
        std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)
        n1, n2 = len(group1), len(group2)

        # åˆå¹¶æ ‡å‡†å·®
        pooled_std = np.sqrt(((n1 - 1) * std1 ** 2 + (n2 - 1) * std2 ** 2) / (n1 + n2 - 2))

        if pooled_std == 0:
            return 0.0

        return abs(mean1 - mean2) / pooled_std

    def _save_experiment(self, comparison: AblationComparison):
        """ä¿å­˜å®éªŒç»“æœ"""
        # ä¿å­˜ä¸ºJSON
        json_path = self.results_dir / f"{comparison.experiment_id}.json"

        with open(json_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            data = {
                'experiment_id': comparison.experiment_id,
                'created_at': comparison.created_at.isoformat(),
                'variants': [v.value for v in comparison.variants],
                'summary_stats': comparison.summary_stats,
                'statistical_tests': comparison.statistical_tests
            }

            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results = {}
            for variant, result_list in comparison.results.items():
                detailed_results[variant] = [r.to_dict() for r in result_list]

            data['detailed_results'] = detailed_results

            json.dump(data, f, indent=2, ensure_ascii=False)

        # ä¿å­˜ä¸ºCSVï¼ˆä¾¿äºåˆ†æï¼‰
        df = comparison.to_dataframe()
        csv_path = self.results_dir / f"{comparison.experiment_id}.csv"
        df.to_csv(csv_path, index=False)

        # ä¿å­˜ä¸ºpickleï¼ˆå®Œæ•´å¯¹è±¡ï¼‰
        pickle_path = self.results_dir / f"{comparison.experiment_id}.pkl"
        with open(pickle_path, 'wb') as f:
            pickle.dump(comparison, f)

        self.logger.info(f"ğŸ’¾ å®éªŒç»“æœä¿å­˜åˆ°:")
        self.logger.info(f"   JSON: {json_path}")
        self.logger.info(f"   CSV: {csv_path}")
        self.logger.info(f"   Pickle: {pickle_path}")

        # æ›´æ–°å®éªŒå†å²
        self.experiment_history[comparison.experiment_id] = comparison

    def _visualize_results(self, comparison: AblationComparison):
        """å¯è§†åŒ–å®éªŒç»“æœ"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            # è®¾ç½®æ ·å¼
            plt.style.use('seaborn-v0_8-darkgrid')
            sns.set_palette("husl")

            # åˆ›å»ºå¯è§†åŒ–ç›®å½•
            vis_dir = self.results_dir / "visualizations"
            vis_dir.mkdir(exist_ok=True)

            # 1. æ€§èƒ½å¯¹æ¯”å›¾ï¼ˆç®±çº¿å›¾ï¼‰
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Ablation Study: {comparison.experiment_id}', fontsize=16)

            # å‡†å¤‡æ•°æ®
            rows = []
            for variant, result_list in comparison.results.items():
                for result in result_list:
                    rows.append({
                        'Variant': variant,
                        'MSE': result.test_metrics.get('mse', 0),
                        'MAE': result.test_metrics.get('mae', 0),
                        'Training Time (s)': result.training_time
                    })

            df = pd.DataFrame(rows)

            # 1.1 MSEç®±çº¿å›¾
            ax1 = axes[0, 0]
            sns.boxplot(data=df, x='Variant', y='MSE', ax=ax1)
            ax1.set_title('Test MSE Distribution')
            ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)
            ax1.set_ylabel('MSE (Lower is Better)')

            # 1.2 MAEç®±çº¿å›¾
            ax2 = axes[0, 1]
            sns.boxplot(data=df, x='Variant', y='MAE', ax=ax2)
            ax2.set_title('Test MAE Distribution')
            ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)
            ax2.set_ylabel('MAE (Lower is Better)')

            # 1.3 è®­ç»ƒæ—¶é—´æ¡å½¢å›¾
            ax3 = axes[1, 0]
            time_stats = df.groupby('Variant')['Training Time (s)'].mean()
            time_stats.plot(kind='bar', ax=ax3, color='skyblue')
            ax3.set_title('Average Training Time')
            ax3.set_xlabel('Variant')
            ax3.set_ylabel('Time (seconds)')
            ax3.tick_params(axis='x', rotation=45)

            # 1.4 æ€§èƒ½-æ—¶é—´æ•£ç‚¹å›¾
            ax4 = axes[1, 1]
            avg_metrics = df.groupby('Variant').agg({
                'MSE': 'mean',
                'Training Time (s)': 'mean'
            }).reset_index()

            sns.scatterplot(data=avg_metrics, x='Training Time (s)', y='MSE',
                            hue='Variant', s=100, ax=ax4)
            ax4.set_title('Performance vs Training Time Trade-off')
            ax4.set_xlabel('Training Time (seconds)')
            ax4.set_ylabel('Average MSE')

            # æ·»åŠ æ ‡ç­¾
            for idx, row in avg_metrics.iterrows():
                ax4.annotate(row['Variant'],
                             (row['Training Time (s)'], row['MSE']),
                             textcoords="offset points",
                             xytext=(0, 10), ha='center')

            plt.tight_layout()
            plt.savefig(vis_dir / f"{comparison.experiment_id}_performance.png",
                        dpi=300, bbox_inches='tight')
            plt.close()

            # 2. ç»Ÿè®¡æ˜¾è‘—æ€§çƒ­å›¾
            if comparison.statistical_tests:
                variants = list(comparison.statistical_tests.keys())
                p_values = np.zeros((len(variants), 2))  # t-testå’ŒWilcoxon

                for i, variant in enumerate(variants):
                    tests = comparison.statistical_tests[variant]
                    p_values[i, 0] = tests['t_test']['p_value']
                    p_values[i, 1] = tests['wilcoxon']['p_value']

                fig, ax = plt.subplots(figsize=(8, 6))
                im = ax.imshow(p_values, cmap='Reds', aspect='auto')

                ax.set_xticks([0, 1])
                ax.set_xticklabels(['t-test', 'Wilcoxon'])
                ax.set_yticks(range(len(variants)))
                ax.set_yticklabels(variants)

                # æ·»åŠ æ•°å€¼
                for i in range(len(variants)):
                    for j in range(2):
                        text = ax.text(j, i, f'{p_values[i, j]:.3f}',
                                       ha="center", va="center",
                                       color="white" if p_values[i, j] > 0.5 else "black")

                ax.set_title('Statistical Significance (p-values)\nvs Full Model')
                plt.colorbar(im, ax=ax, label='p-value')
                plt.tight_layout()
                plt.savefig(vis_dir / f"{comparison.experiment_id}_significance.png",
                            dpi=300, bbox_inches='tight')
                plt.close()

            # 3. æ•ˆåº”å¤§å°æ¡å½¢å›¾
            if comparison.statistical_tests:
                effect_sizes = []
                variant_names = []

                for variant, tests in comparison.statistical_tests.items():
                    effect_sizes.append(tests['effect_size'])
                    variant_names.append(variant)

                fig, ax = plt.subplots(figsize=(10, 6))
                bars = ax.bar(variant_names, effect_sizes, color='lightcoral')

                # æ·»åŠ æ•ˆåº”å¤§å°æ ‡ç­¾
                ax.axhline(y=0.2, color='gray', linestyle='--', alpha=0.5, label='Small effect')
                ax.axhline(y=0.5, color='gray', linestyle='-.', alpha=0.5, label='Medium effect')
                ax.axhline(y=0.8, color='gray', linestyle=':', alpha=0.5, label='Large effect')

                ax.set_xlabel('Variant')
                ax.set_ylabel("Cohen's d Effect Size")
                ax.set_title('Effect Size vs Full Model')
                ax.tick_params(axis='x', rotation=45)
                ax.legend()

                # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
                for bar, effect in zip(bars, effect_sizes):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                            f'{effect:.2f}', ha='center', va='bottom')

                plt.tight_layout()
                plt.savefig(vis_dir / f"{comparison.experiment_id}_effect_size.png",
                            dpi=300, bbox_inches='tight')
                plt.close()

            self.logger.info(f"ğŸ“Š å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {vis_dir}")

        except Exception as e:
            self.logger.warning(f"å¯è§†åŒ–å¤±è´¥: {e}")

    def load_experiment(self, experiment_id: str) -> Optional[AblationComparison]:
        """åŠ è½½å®éªŒ"""
        pickle_path = self.results_dir / f"{experiment_id}.pkl"

        if pickle_path.exists():
            with open(pickle_path, 'rb') as f:
                comparison = pickle.load(f)
            return comparison

        return None

    def compare_experiments(self, experiment_ids: List[str]) -> pd.DataFrame:
        """æ¯”è¾ƒå¤šä¸ªå®éªŒ"""
        comparisons = []

        for exp_id in experiment_ids:
            comparison = self.load_experiment(exp_id)
            if comparison:
                df = comparison.to_dataframe()
                df['experiment'] = exp_id
                comparisons.append(df)

        if comparisons:
            return pd.concat(comparisons, ignore_index=True)

        return pd.DataFrame()

    def generate_report(self, comparison: AblationComparison) -> str:
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        report = []

        report.append("=" * 80)
        report.append("æ¶ˆèå®éªŒæŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"å®éªŒID: {comparison.experiment_id}")
        report.append(f"åˆ›å»ºæ—¶é—´: {comparison.created_at}")
        report.append(f"æµ‹è¯•å˜ä½“æ•°: {len(comparison.variants)}")
        report.append("")

        # æ€§èƒ½æ€»ç»“
        report.append("ğŸ“Š æ€§èƒ½æ€»ç»“ (MSE)")
        report.append("-" * 40)

        for variant, stats in comparison.summary_stats.items():
            report.append(f"{variant:<20} Mean: {stats['mean']:.6f} Â± {stats['std']:.6f} "
                          f"(Min: {stats['min']:.6f}, Max: {stats['max']:.6f})")

        report.append("")

        # ç»Ÿè®¡æ˜¾è‘—æ€§
        if comparison.statistical_tests:
            report.append("ğŸ“ˆ ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (vs å®Œæ•´æ¨¡å‹)")
            report.append("-" * 40)

            for variant, tests in comparison.statistical_tests.items():
                t_sig = "âœ“" if tests['t_test']['significant'] else "âœ—"
                w_sig = "âœ“" if tests['wilcoxon']['significant'] else "âœ—"

                report.append(f"{variant:<20} t-test: p={tests['t_test']['p_value']:.4f} {t_sig} "
                              f"| Wilcoxon: p={tests['wilcoxon']['p_value']:.4f} {w_sig} "
                              f"| Effect size: {tests['effect_size']:.3f}")

        report.append("")

        # å…³é”®å‘ç°
        report.append("ğŸ”‘ å…³é”®å‘ç°")
        report.append("-" * 40)

        # æ‰¾å‡ºæ€§èƒ½æœ€å¥½çš„å˜ä½“
        best_variant = None
        best_mse = float('inf')

        for variant, stats in comparison.summary_stats.items():
            if stats['mean'] < best_mse:
                best_mse = stats['mean']
                best_variant = variant

        if best_variant:
            report.append(f"1. æœ€ä½³æ€§èƒ½å˜ä½“: {best_variant} (MSE: {best_mse:.6f})")

        # æ‰¾å‡ºæ€§èƒ½ä¸‹é™æœ€å¤šçš„å˜ä½“ï¼ˆç›¸æ¯”å®Œæ•´æ¨¡å‹ï¼‰
        if 'full' in comparison.summary_stats:
            full_mse = comparison.summary_stats['full']['mean']

            worst_relative = None
            worst_ratio = 0

            for variant, stats in comparison.summary_stats.items():
                if variant != 'full':
                    ratio = stats['mean'] / full_mse
                    if ratio > worst_ratio:
                        worst_ratio = ratio
                        worst_relative = variant

            if worst_relative:
                report.append(f"2. æ€§èƒ½ä¸‹é™æœ€å¤š: {worst_relative} ({worst_ratio:.1%} of full model)")

        # æ‰¾å‡ºè®­ç»ƒæ—¶é—´å·®å¼‚
        time_data = []
        for variant, result_list in comparison.results.items():
            if result_list:
                avg_time = np.mean([r.training_time for r in result_list])
                time_data.append((variant, avg_time))

        if time_data:
            fastest = min(time_data, key=lambda x: x[1])
            slowest = max(time_data, key=lambda x: x[1])

            report.append(f"3. æœ€å¿«è®­ç»ƒ: {fastest[0]} ({fastest[1]:.1f}s)")
            report.append(f"4. æœ€æ…¢è®­ç»ƒ: {slowest[0]} ({slowest[1]:.1f}s)")

        report.append("")
        report.append("=" * 80)

        return "\n".join(report)

    def export_for_latex(self, comparison: AblationComparison,
                         output_path: str = None) -> str:
        """å¯¼å‡ºä¸ºLaTeXè¡¨æ ¼æ ¼å¼"""
        if not output_path:
            output_path = self.results_dir / f"{comparison.experiment_id}_table.tex"

        latex = []
        latex.append("\\begin{table}[htbp]")
        latex.append("\\centering")
        latex.append("\\caption{Ablation Study Results}")
        latex.append("\\label{tab:ablation_results}")
        latex.append("\\begin{tabular}{lcccc}")
        latex.append("\\toprule")
        latex.append("Variant & MSE (mean Â± std) & MAE & Training Time (s) & Significant \\\\")
        latex.append("\\midrule")

        for variant, stats in comparison.summary_stats.items():
            # è·å–MAEï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            mae = comparison.results.get(variant, [{}])[0].test_metrics.get('mae', 0)

            # æ£€æŸ¥ç»Ÿè®¡æ˜¾è‘—æ€§
            sig_marker = ""
            if variant in comparison.statistical_tests:
                if (comparison.statistical_tests[variant]['t_test']['significant'] or
                        comparison.statistical_tests[variant]['wilcoxon']['significant']):
                    sig_marker = "\\checkmark"
                else:
                    sig_marker = "\\times"

            # è·å–å¹³å‡è®­ç»ƒæ—¶é—´
            avg_time = np.mean([r.training_time for r in comparison.results.get(variant, [])])

            latex.append(f"{variant:<20} & {stats['mean']:.4f} Â± {stats['std']:.4f} & "
                         f"{mae:.4f} & {avg_time:.1f} & {sig_marker} \\\\")

        latex.append("\\bottomrule")
        latex.append("\\end{tabular}")
        latex.append("\\end{table}")

        latex_str = "\n".join(latex)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(latex_str)

        self.logger.info(f"ğŸ“‹ LaTeXè¡¨æ ¼ä¿å­˜åˆ°: {output_path}")

        return latex_str


# ä½¿ç”¨ç¤ºä¾‹
def main():
    """æ¶ˆèå®éªŒä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="è¿è¡Œæ¶ˆèå®éªŒ")
    parser.add_argument("--config", type=str, default="./config.yaml",
                        help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data", type=str, default="./data/ETTh1.csv",
                        help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--variants", type=str, nargs='+',
                        choices=[v.value for v in AblationVariant],
                        help="è¦æµ‹è¯•çš„å˜ä½“åˆ—è¡¨")
    parser.add_argument("--name", type=str, help="å®éªŒåç§°")
    parser.add_argument("--epochs", type=int, default=50,
                        help="æ¯ä¸ªå˜ä½“çš„è®­ç»ƒè½®æ•°")
    parser.add_argument("--runs", type=int, default=3,
                        help="æ¯ä¸ªå˜ä½“çš„è¿è¡Œæ¬¡æ•°")

    args = parser.parse_args()

    # åˆ›å»ºæ¶ˆèå®éªŒç®¡ç†å™¨
    manager = AblationStudyManager(args.config)

    # è½¬æ¢å˜ä½“å‚æ•°
    variants = None
    if args.variants:
        variants = [AblationVariant(v) for v in args.variants]

    # è¿è¡Œå®éªŒ
    comparison = manager.run_ablation_experiment(
        variants=variants,
        data_path=args.data,
        experiment_name=args.name
    )

    # ç”ŸæˆæŠ¥å‘Š
    report = manager.generate_report(comparison)
    print(report)

    # å¯¼å‡ºLaTeXè¡¨æ ¼
    manager.export_for_latex(comparison)

    return comparison


if __name__ == "__main__":
    comparison = main()