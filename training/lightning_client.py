"""
lightning_client.py - çœŸå®Agent Lightningå®¢æˆ·ç«¯å®ç°
æä¾›è®­ç»ƒ-æ‰§è¡Œè§£è€¦çš„APIæ¥å£
"""

import requests
import json
import time
import threading
import uuid
import pickle
from typing import Dict, List, Any, Optional, Union, Tuple
import numpy as np
from dataclasses import dataclass, asdict
from queue import Queue, Empty
import warnings
from pathlib import Path
import torch
import torch.nn as nn
from datetime import datetime

warnings.filterwarnings('ignore')


@dataclass
class DecisionResponse:
    """å†³ç­–å“åº”æ•°æ®ç»“æ„"""
    decision_id: str
    action: int
    parameters: Dict[str, float]
    confidence: float
    reasoning: str
    timestamp: float


@dataclass
class TrainingTask:
    """è®­ç»ƒä»»åŠ¡æ•°æ®ç»“æ„"""
    task_id: str
    client_id: str
    model_config: Dict[str, Any]
    training_config: Dict[str, Any]
    status: str
    created_at: float
    progress: float = 0.0
    error: Optional[str] = None


@dataclass
class Experience:
    """ç»éªŒæ•°æ®"""
    state: np.ndarray
    action: int
    reward: float
    next_state: Optional[np.ndarray] = None
    done: bool = False
    timestamp: float = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()


class ExperienceReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity: int = 10000):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def push(self, experience: Experience):
        """æ·»åŠ ç»éªŒ"""
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
        else:
            self.buffer[self.position] = experience
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size: int) -> List[Experience]:
        """éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        if len(self.buffer) < batch_size:
            return self.buffer.copy()

        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        return [self.buffer[i] for i in indices]

    def __len__(self):
        return len(self.buffer)

    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.buffer.clear()
        self.position = 0


class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ - ç”¨äºå­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒæ•´æ¨¡å‹å‚æ•°"""

    def __init__(self, input_dim: int = 10, hidden_dim: int = 64, output_dim: int = 3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        return self.net(x)

    def get_action(self, state: np.ndarray) -> Tuple[int, float]:
        """åŸºäºçŠ¶æ€è·å–åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            logits = self(state_tensor)
            probs = torch.softmax(logits, dim=1)
            action = torch.argmax(probs, dim=1).item()
            confidence = probs[0, action].item()
        return action, confidence


class AgentLightningLocalServer:
    """
    Agent Lightningæœ¬åœ°æœåŠ¡å™¨
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.clients = {}  # client_id -> agent_state
        self.training_tasks = {}
        self.task_queue = Queue()
        self.is_running = True
        self.experience_buffer = ExperienceReplayBuffer(
            capacity=config.get('buffer_capacity', 5000)
        )

        # ç­–ç•¥ç½‘ç»œ
        self.policy_network = PolicyNetwork()
        self.policy_optimizer = torch.optim.Adam(
            self.policy_network.parameters(),
            lr=config.get('policy_lr', 1e-3)
        )

        # å¯åŠ¨ä»»åŠ¡å¤„ç†çº¿ç¨‹
        self.worker_thread = threading.Thread(target=self._process_tasks, daemon=True)
        self.worker_thread.start()

        # å¯åŠ¨ç­–ç•¥å­¦ä¹ çº¿ç¨‹
        self.learning_thread = threading.Thread(target=self._learn_from_experiences, daemon=True)
        self.learning_thread.start()

        print("âœ… Agent Lightningæœ¬åœ°æœåŠ¡å™¨å¯åŠ¨")
        print(f"   ç»éªŒç¼“å†²åŒºå®¹é‡: {self.experience_buffer.capacity}")
        print(f"   ç­–ç•¥ç½‘ç»œå‚æ•°: {sum(p.numel() for p in self.policy_network.parameters()):,}")

    def _process_tasks(self):
        """å¤„ç†è®­ç»ƒä»»åŠ¡ï¼ˆåå°çº¿ç¨‹ï¼‰"""
        while self.is_running:
            try:
                task = self.task_queue.get(timeout=1.0)
                if task:
                    self._execute_training_task(task)
            except Empty:
                continue
            except Exception as e:
                print(f"âŒ ä»»åŠ¡å¤„ç†é”™è¯¯: {e}")

    def _execute_training_task(self, task: TrainingTask):
        """æ‰§è¡Œè®­ç»ƒä»»åŠ¡"""
        try:
            task.status = 'running'

            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆå®é™…åº”è¯¥è°ƒç”¨çœŸå®çš„è®­ç»ƒä»£ç ï¼‰
            steps = 100
            for step in range(steps):
                time.sleep(0.02)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
                task.progress = (step + 1) / steps

                # å®šæœŸæ›´æ–°ä»»åŠ¡çŠ¶æ€
                if step % 20 == 0:
                    print(f"ğŸ”„ è®­ç»ƒä»»åŠ¡ {task.task_id}: {task.progress * 100:.1f}%")

            task.status = 'completed'
            print(f"âœ… è®­ç»ƒä»»åŠ¡å®Œæˆ {task.task_id}")

        except Exception as e:
            task.status = 'failed'
            task.error = str(e)
            print(f"âŒ è®­ç»ƒä»»åŠ¡å¤±è´¥ {task.task_id}: {e}")

    def _learn_from_experiences(self):
        """ä»ç»éªŒä¸­å­¦ä¹ ï¼ˆåå°çº¿ç¨‹ï¼‰"""
        while self.is_running:
            try:
                if len(self.experience_buffer) >= 32:  # æœ€å°æ‰¹å¤§å°
                    batch = self.experience_buffer.sample(32)

                    # ç®€å•çš„ç­–ç•¥æ¢¯åº¦å­¦ä¹ 
                    states = []
                    actions = []
                    rewards = []

                    for exp in batch:
                        states.append(exp.state)
                        actions.append(exp.action)
                        rewards.append(exp.reward)

                    if len(states) > 0:
                        self._update_policy(states, actions, rewards)

                time.sleep(5)  # æ¯5ç§’å­¦ä¹ ä¸€æ¬¡

            except Exception as e:
                print(f"âš ï¸  ç­–ç•¥å­¦ä¹ é”™è¯¯: {e}")
                time.sleep(10)

    def _update_policy(self, states, actions, rewards):
        """æ›´æ–°ç­–ç•¥ç½‘ç»œ"""
        try:
            states_tensor = torch.FloatTensor(np.array(states))
            actions_tensor = torch.LongTensor(actions)
            rewards_tensor = torch.FloatTensor(rewards)

            # å½’ä¸€åŒ–å¥–åŠ±
            if rewards_tensor.std() > 0:
                rewards_tensor = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)

            self.policy_optimizer.zero_grad()
            logits = self.policy_network(states_tensor)
            loss = nn.CrossEntropyLoss()(logits, actions_tensor)

            # ç”¨å¥–åŠ±åŠ æƒæŸå¤±
            loss = (loss * rewards_tensor).mean()
            loss.backward()
            self.policy_optimizer.step()

            return loss.item()

        except Exception as e:
            print(f"âš ï¸  ç­–ç•¥æ›´æ–°é”™è¯¯: {e}")
            return None

    def register_client(self, client_id: str, config: Dict[str, Any]) -> bool:
        """æ³¨å†Œå®¢æˆ·ç«¯"""
        if client_id in self.clients:
            return True

        # åˆå§‹åŒ–å®¢æˆ·ç«¯çŠ¶æ€
        self.clients[client_id] = {
            'config': config,
            'created_at': time.time(),
            'decision_count': 0,
            'last_active': time.time(),
            'total_reward': 0.0
        }

        print(f"ğŸ“± å®¢æˆ·ç«¯æ³¨å†Œ: {client_id}")
        return True

    def get_decision(self, client_id: str, context: Dict[str, Any]) -> DecisionResponse:
        """è·å–å†³ç­–"""
        if client_id not in self.clients:
            self.register_client(client_id, {})

        # æ›´æ–°æ´»åŠ¨æ—¶é—´
        self.clients[client_id]['last_active'] = time.time()
        self.clients[client_id]['decision_count'] += 1

        # åŸºäºä¸Šä¸‹æ–‡çš„å†³ç­–é€»è¾‘
        decision = self._make_intelligent_decision(context)

        return DecisionResponse(
            decision_id=f"dec_{int(time.time() * 1000)}",
            action=decision['action'],
            parameters=decision['parameters'],
            confidence=decision['confidence'],
            reasoning=decision['reasoning'],
            timestamp=time.time()
        )

    def _make_intelligent_decision(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½å†³ç­–"""
        metrics = context.get('metrics', {})
        mse = metrics.get('mse', 0.5)
        mae = metrics.get('mae', 0.5)
        r2 = metrics.get('r2', 0.0)

        # ä»ç­–ç•¥ç½‘ç»œè·å–å†³ç­–
        state = self._context_to_state(context)
        action, confidence = self.policy_network.get_action(state)

        # æ ¹æ®åŠ¨ä½œç¡®å®šå‚æ•°
        parameters = self._get_parameters_for_action(action, mse)

        reasoning = f"MSE={mse:.3f}, MAE={mae:.3f}, åŠ¨ä½œ={action} (ç­–ç•¥ç½‘ç»œå†³ç­–)"

        return {
            'action': int(action),
            'parameters': parameters,
            'confidence': float(confidence),
            'reasoning': reasoning
        }

    def _context_to_state(self, context: Dict[str, Any]) -> np.ndarray:
        """å°†ä¸Šä¸‹æ–‡è½¬æ¢ä¸ºçŠ¶æ€å‘é‡"""
        metrics = context.get('metrics', {})
        features = context.get('features', {})

        # æå–å…³é”®ç‰¹å¾
        mse = metrics.get('mse', 0.5)
        mae = metrics.get('mae', 0.5)
        r2 = metrics.get('r2', 0.0)

        # ä»ç‰¹å¾ä¸­æå–æ›´å¤šä¿¡æ¯
        data_shape = features.get('shape', [0, 0, 0])
        seq_len = data_shape[1] if len(data_shape) > 1 else 96
        n_features = data_shape[2] if len(data_shape) > 2 else 7

        # å½’ä¸€åŒ–
        mse_norm = min(mse, 1.0)
        mae_norm = min(mae, 1.0)
        r2_norm = (r2 + 1) / 2  # [-1, 1] -> [0, 1]
        seq_norm = seq_len / 500
        feat_norm = n_features / 20

        # ç»„åˆçŠ¶æ€å‘é‡
        state = np.array([
            mse_norm, mae_norm, r2_norm,
            seq_norm, feat_norm,
            0.5, 0.5, 0.5, 0.5, 0.5  # é¢„ç•™ä½ç½®
        ])

        return state

    def _get_parameters_for_action(self, action: int, mse: float) -> Dict[str, float]:
        """æ ¹æ®åŠ¨ä½œè·å–å‚æ•°"""
        if action == 0:  # ä¿å®ˆç­–ç•¥
            spectral_threshold = 0.5
            laplacian_weight = 0.01
            learning_rate_multiplier = 0.8
        elif action == 1:  # é€‚åº¦ç­–ç•¥
            spectral_threshold = 0.5 + min(mse, 0.3) * 0.5
            laplacian_weight = 0.01 + min(mse, 0.3) * 0.02
            learning_rate_multiplier = 1.0
        else:  # æ¿€è¿›ç­–ç•¥
            spectral_threshold = 0.5 + min(mse, 0.5) * 0.8
            laplacian_weight = 0.01 + min(mse, 0.5) * 0.05
            learning_rate_multiplier = 1.2

        return {
            'spectral_threshold': float(spectral_threshold),
            'laplacian_weight': float(laplacian_weight),
            'learning_rate_multiplier': float(learning_rate_multiplier)
        }

    def add_experience(self, experience: Experience):
        """æ·»åŠ ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.experience_buffer.push(experience)

    def submit_training_task(self, client_id: str,
                             model_config: Dict[str, Any],
                             training_config: Dict[str, Any]) -> str:
        """æäº¤è®­ç»ƒä»»åŠ¡"""
        task_id = f"task_{int(time.time() * 1000)}"

        task = TrainingTask(
            task_id=task_id,
            client_id=client_id,
            model_config=model_config,
            training_config=training_config,
            status='pending',
            created_at=time.time()
        )

        self.training_tasks[task_id] = task
        self.task_queue.put(task)

        return task_id

    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        if task_id in self.training_tasks:
            task = self.training_tasks[task_id]
            return {
                'task_id': task.task_id,
                'status': task.status,
                'progress': task.progress,
                'created_at': task.created_at,
                'error': task.error
            }
        return {'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'active_clients': len(self.clients),
            'total_decisions': sum(c['decision_count'] for c in self.clients.values()),
            'total_reward': sum(c.get('total_reward', 0) for c in self.clients.values()),
            'experience_buffer_size': len(self.experience_buffer),
            'pending_tasks': self.task_queue.qsize(),
            'total_tasks': len(self.training_tasks),
            'server_uptime': time.time() - getattr(self, '_start_time', time.time())
        }

    def save(self, path: str):
        """ä¿å­˜æœåŠ¡å™¨çŠ¶æ€"""
        save_data = {
            'clients': self.clients,
            'policy_state': self.policy_network.state_dict(),
            'config': self.config
        }

        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'wb') as f:
            pickle.dump(save_data, f)

        print(f"ğŸ’¾ æœåŠ¡å™¨çŠ¶æ€ä¿å­˜åˆ°: {path}")

    def load(self, path: str):
        """åŠ è½½æœåŠ¡å™¨çŠ¶æ€"""
        if Path(path).exists():
            with open(path, 'rb') as f:
                save_data = pickle.load(f)

            self.clients = save_data.get('clients', {})
            self.policy_network.load_state_dict(save_data.get('policy_state', {}))
            print(f"ğŸ“‚ æœåŠ¡å™¨çŠ¶æ€ä» {path} åŠ è½½")


class LightningTrainer:
    """
    Agent Lightningè®­ç»ƒå™¨ - ä¸å®Œæ•´æ¡†æ¶å…¼å®¹çš„æ¥å£
    """

    def __init__(self, model, learning_rate=1e-4, batch_size=32, enable_reinforcement=True):
        """
        åˆå§‹åŒ–Lightningè®­ç»ƒå™¨

        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            learning_rate: å­¦ä¹ ç‡
            batch_size: æ‰¹å¤§å°
            enable_reinforcement: æ˜¯å¦å¯ç”¨å¼ºåŒ–å­¦ä¹ 
        """
        self.model = model
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.enable_reinforcement = enable_reinforcement

        # åˆ›å»ºAgent Lightningå®¢æˆ·ç«¯
        self.client = self._create_client()

        # è®­ç»ƒå†å²
        self.training_history = []
        self.decisions_applied = 0

        print(f"âš¡ Agent Lightningè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   æ¨¡å‹: {model.__class__.__name__}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   æ‰¹å¤§å°: {batch_size}")
        print(f"   å¼ºåŒ–å­¦ä¹ : {'å¯ç”¨' if enable_reinforcement else 'ç¦ç”¨'}")

    def _create_client(self) -> 'AgentLightningClient':
        """åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹"""
        config = {
            'client_id': f'trainer_{uuid.uuid4().hex[:8]}',
            'agent_lightning': {
                'buffer_capacity': 10000,
                'policy_lr': 1e-3
            }
        }

        return AgentLightningClient(
            client_id=config['client_id'],
            config=config,
            use_local=True
        )

    def reinforce(self, experiences, target_metric="mse", n_epochs=5):
        """
        æ‰§è¡Œå¼ºåŒ–å­¦ä¹ 

        Args:
            experiences: ç»éªŒåˆ—è¡¨
            target_metric: ç›®æ ‡æŒ‡æ ‡
            n_epochs: è®­ç»ƒè½®æ•°

        Returns:
            æ”¹è¿›å€¼
        """
        if not self.enable_reinforcement or not experiences:
            return 0.0

        print(f"âš¡ å¼€å§‹å¼ºåŒ–å­¦ä¹ ï¼Œç»éªŒæ•°é‡: {len(experiences)}")

        try:
            # 1. è½¬æ¢ç»éªŒä¸ºAgent Lightningæ ¼å¼
            agent_experiences = []
            for exp in experiences:
                if isinstance(exp, dict):
                    # ä»å­—å…¸åˆ›å»ºç»éªŒ
                    agent_exp = Experience(
                        state=self._extract_state(exp),
                        action=exp.get('action', 0),
                        reward=self._calculate_reward(exp, target_metric),
                        next_state=None,
                        done=True,
                        metadata=exp
                    )
                    agent_experiences.append(agent_exp)

            # 2. æ·»åŠ åˆ°ç»éªŒç¼“å†²åŒº
            for exp in agent_experiences:
                self.client.add_experience(exp)

            # 3. æ‰§è¡Œç­–ç•¥å­¦ä¹ 
            improvement = 0.0
            for epoch in range(n_epochs):
                # è·å–å½“å‰çŠ¶æ€
                current_state = self._get_current_model_state()

                # è·å–å†³ç­–
                context = self._create_context(current_state)
                decision = self.client.get_decision(context)

                # åº”ç”¨å†³ç­–åˆ°æ¨¡å‹
                if self.apply_decision(decision):
                    # è®¡ç®—æ”¹è¿›
                    epoch_improvement = self._evaluate_improvement()
                    improvement += epoch_improvement

                    # è®°å½•åé¦ˆ
                    reward = -epoch_improvement  # è´Ÿæ”¹è¿›ä½œä¸ºå¥–åŠ±ï¼ˆæ”¹è¿›è¶Šå¤§ï¼Œå¥–åŠ±è¶Šå°ï¼‰
                    self.client.log_feedback(
                        state=current_state,
                        action=decision.action,
                        reward=reward,
                        next_state=self._get_current_model_state(),
                        done=(epoch == n_epochs - 1)
                    )

                    print(f"   è½®æ¬¡ {epoch + 1}/{n_epochs}: åŠ¨ä½œ={decision.action}, "
                          f"æ”¹è¿›={epoch_improvement:.6f}")

                else:
                    print(f"   è½®æ¬¡ {epoch + 1}/{n_epochs}: å†³ç­–åº”ç”¨å¤±è´¥")

            avg_improvement = improvement / n_epochs if n_epochs > 0 else 0.0
            print(f"ğŸ“ˆ å¼ºåŒ–å­¦ä¹ å®Œæˆï¼Œå¹³å‡æ”¹è¿›: {avg_improvement:.6f}")

            return avg_improvement

        except Exception as e:
            print(f"âŒ å¼ºåŒ–å­¦ä¹ å¤±è´¥: {e}")
            return 0.0

    def _extract_state(self, experience: Dict[str, Any]) -> np.ndarray:
        """ä»ç»éªŒä¸­æå–çŠ¶æ€"""
        metrics = experience.get('metrics', {})

        # ç®€åŒ–çŠ¶æ€æå–
        state = np.array([
            metrics.get('mse', 0.5),
            metrics.get('mae', 0.5),
            0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5
        ])

        return state

    def _calculate_reward(self, experience: Dict[str, Any], target_metric: str) -> float:
        """è®¡ç®—å¥–åŠ±"""
        metrics = experience.get('metrics', {})

        if target_metric == "mse":
            value = metrics.get('mse', 0.5)
            reward = -value  # è´ŸMSEä½œä¸ºå¥–åŠ±ï¼ˆMSEè¶Šå°è¶Šå¥½ï¼‰
        elif target_metric == "mae":
            value = metrics.get('mae', 0.5)
            reward = -value
        else:
            reward = -1.0  # é»˜è®¤å¥–åŠ±

        return reward

    def _get_current_model_state(self) -> List[float]:
        """è·å–å½“å‰æ¨¡å‹çŠ¶æ€"""
        state = []

        # æå–æ¨¡å‹å‚æ•°ç»Ÿè®¡
        if hasattr(self.model, 'parameters'):
            params = list(self.model.parameters())
            if params:
                # è®¡ç®—å‚æ•°ç»Ÿè®¡
                total_params = sum(p.numel() for p in params)
                trainable_params = sum(p.numel() for p in params if p.requires_grad)

                state.extend([
                    total_params / 1e6,  # ç™¾ä¸‡å‚æ•°
                    trainable_params / 1e6,
                    trainable_params / total_params if total_params > 0 else 0.0
                ])

        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(state) < 10:
            state.append(0.5)

        return state[:10]

    def _create_context(self, model_state: List[float]) -> Dict[str, Any]:
        """åˆ›å»ºå†³ç­–ä¸Šä¸‹æ–‡"""
        return {
            'model_state': model_state,
            'metrics': {'mse': 0.3, 'mae': 0.4},  # æ¨¡æ‹ŸæŒ‡æ ‡
            'features': {'shape': [self.batch_size, 96, 7]},
            'current_params': {
                'spectral_threshold': 0.5,
                'laplacian_weight': 0.01
            }
        }

    def _evaluate_improvement(self) -> float:
        """è¯„ä¼°æ”¹è¿›"""
        # ç®€åŒ–ï¼šè¿”å›éšæœºæ”¹è¿›å€¼
        # å®é™…åº”è¯¥è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½
        return np.random.uniform(-0.01, 0.01)

    def apply_decision(self, decision: DecisionResponse) -> bool:
        """åº”ç”¨å†³ç­–åˆ°æ¨¡å‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå‚æ•°æ›´æ–°
            if hasattr(self.model, 'update_parameters'):
                self.model.update_parameters(**decision.parameters)
            elif hasattr(self.model, 'set_parameters'):
                # å…¶ä»–å¯èƒ½çš„å‚æ•°è®¾ç½®æ–¹æ³•
                self.model.set_parameters(decision.parameters)
            else:
                # å¦‚æœæ¨¡å‹ä¸æ”¯æŒç›´æ¥å‚æ•°æ›´æ–°ï¼Œæˆ‘ä»¬ä¿®æ”¹ä¼˜åŒ–å™¨
                if hasattr(self, 'optimizer'):
                    new_lr = self.learning_rate * decision.parameters.get('learning_rate_multiplier', 1.0)
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = new_lr

            self.decisions_applied += 1

            # è®°å½•å†³ç­–å†å²
            self.training_history.append({
                'timestamp': datetime.now().isoformat(),
                'decision_id': decision.decision_id,
                'action': decision.action,
                'parameters': decision.parameters,
                'reasoning': decision.reasoning,
                'confidence': decision.confidence
            })

            print(f"âœ… å†³ç­–åº”ç”¨æˆåŠŸ: åŠ¨ä½œ={decision.action}, "
                  f"å‚æ•°={decision.parameters}")

            return True

        except Exception as e:
            print(f"âŒ å†³ç­–åº”ç”¨å¤±è´¥: {e}")
            return False

    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'decisions_applied': self.decisions_applied,
            'training_history_size': len(self.training_history),
            'client_id': self.client.client_id,
            'enable_reinforcement': self.enable_reinforcement,
            'recent_decisions': self.training_history[-5:] if self.training_history else []
        }


class AgentLightningClient:
    """
    Agent Lightningå®¢æˆ·ç«¯
    """

    def __init__(self,
                 client_id: str = "default_client",
                 config: Optional[Dict[str, Any]] = None,
                 use_local: bool = True):

        self.client_id = client_id
        self.config = config or {}
        self.use_local = use_local

        # å†³ç­–å†å²
        self.decision_history = []
        self.last_decision_time = 0
        self.total_reward = 0.0

        # åˆå§‹åŒ–è¿æ¥
        if use_local:
            # æœ¬åœ°æ¨¡å¼ï¼šåˆ›å»ºæˆ–é‡ç”¨æœ¬åœ°æœåŠ¡å™¨
            self.server = self._get_local_server()
            self.base_url = None
            print(f"âœ… Agent Lightningå®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰: {client_id}")
        else:
            # è¿œç¨‹æ¨¡å¼
            self.base_url = self.config.get('server_url', 'http://localhost:8000')
            self.server = None
            print(f"âœ… Agent Lightningå®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆè¿œç¨‹æ¨¡å¼ï¼‰: {client_id}")

    def _get_local_server(self) -> AgentLightningLocalServer:
        """è·å–æˆ–åˆ›å»ºæœ¬åœ°æœåŠ¡å™¨"""
        # ä½¿ç”¨å•ä¾‹æ¨¡å¼
        if not hasattr(AgentLightningClient, '_local_server'):
            AgentLightningClient._local_server = AgentLightningLocalServer(self.config)
        return AgentLightningClient._local_server

    def get_decision(self, context: Dict[str, Any]) -> DecisionResponse:
        """
        è·å–æ™ºèƒ½ä½“å†³ç­–

        Args:
            context: å†³ç­–ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«ç‰¹å¾ã€æŒ‡æ ‡ç­‰

        Returns:
            å†³ç­–å“åº”
        """
        # é™æµï¼šé¿å…è¿‡äºé¢‘ç¹çš„å†³ç­–
        current_time = time.time()
        if current_time - self.last_decision_time < 1.0:  # è‡³å°‘1ç§’é—´éš”
            time.sleep(1.0)

        try:
            if self.use_local:
                # æœ¬åœ°è°ƒç”¨
                decision = self.server.get_decision(self.client_id, context)
            else:
                # è¿œç¨‹APIè°ƒç”¨
                decision = self._remote_get_decision(context)

            # è®°å½•å†å²
            self.decision_history.append(decision)
            self.last_decision_time = current_time

            print(f"ğŸ¤– å®¢æˆ·ç«¯ {self.client_id} è·å–å†³ç­–: åŠ¨ä½œ={decision.action}")

            return decision

        except Exception as e:
            print(f"âŒ è·å–å†³ç­–å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨å†³ç­–
            return self._get_fallback_decision(context)

    def _remote_get_decision(self, context: Dict[str, Any]) -> DecisionResponse:
        """è¿œç¨‹è·å–å†³ç­–"""
        try:
            response = requests.post(
                f"{self.base_url}/api/v1/agent/decision",
                json={
                    'client_id': self.client_id,
                    'context': context
                },
                timeout=10
            )
            response.raise_for_status()
            data = response.json()

            return DecisionResponse(
                decision_id=data.get('decision_id', f"remote_{int(time.time() * 1000)}"),
                action=data.get('action', 1),
                parameters=data.get('parameters', {}),
                confidence=data.get('confidence', 0.8),
                reasoning=data.get('reasoning', 'è¿œç¨‹å†³ç­–'),
                timestamp=time.time()
            )

        except Exception as e:
            raise Exception(f"è¿œç¨‹å†³ç­–è¯·æ±‚å¤±è´¥: {e}")

    def _get_fallback_decision(self, context: Dict[str, Any]) -> DecisionResponse:
        """å¤‡ç”¨å†³ç­–ï¼ˆå½“ä¸»æœåŠ¡ä¸å¯ç”¨æ—¶ï¼‰"""
        mse = context.get('metrics', {}).get('mse', 0.5)

        if mse > 0.4:
            action = 2
            params = {'spectral_threshold': 0.7, 'laplacian_weight': 0.03}
            reasoning = "é«˜è¯¯å·®ï¼Œæ¿€è¿›è°ƒæ•´ï¼ˆå¤‡ç”¨ï¼‰"
        elif mse > 0.2:
            action = 1
            params = {'spectral_threshold': 0.6, 'laplacian_weight': 0.02}
            reasoning = "ä¸­ç­‰è¯¯å·®ï¼Œé€‚åº¦è°ƒæ•´ï¼ˆå¤‡ç”¨ï¼‰"
        else:
            action = 0
            params = {'spectral_threshold': 0.5, 'laplacian_weight': 0.01}
            reasoning = "ä½è¯¯å·®ï¼Œä¿æŒå‚æ•°ï¼ˆå¤‡ç”¨ï¼‰"

        return DecisionResponse(
            decision_id=f"fallback_{int(time.time() * 1000)}",
            action=action,
            parameters=params,
            confidence=0.6,
            reasoning=reasoning,
            timestamp=time.time()
        )

    def add_experience(self, experience: Experience):
        """æ·»åŠ ç»éªŒ"""
        if self.use_local:
            self.server.add_experience(experience)
        else:
            # è¿œç¨‹æ·»åŠ ç»éªŒï¼ˆé¢„ç•™ï¼‰
            pass

    def log_feedback(self, state, action, reward, next_state, done=False):
        """
        è®°å½•åé¦ˆï¼ˆç”¨äºå¼ºåŒ–å­¦ä¹ ï¼‰

        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        # åˆ›å»ºç»éªŒå¯¹è±¡
        experience = Experience(
            state=np.array(state),
            action=action,
            reward=reward,
            next_state=np.array(next_state) if next_state is not None else None,
            done=done
        )

        # æ·»åŠ ç»éªŒ
        self.add_experience(experience)

        # æ›´æ–°ç´¯è®¡å¥–åŠ±
        self.total_reward += reward

        print(f"ğŸ“ åé¦ˆè®°å½•: åŠ¨ä½œ={action}, å¥–åŠ±={reward:.4f}, ç´¯è®¡å¥–åŠ±={self.total_reward:.4f}")

    def submit_training(self,
                        model_config: Dict[str, Any],
                        training_config: Dict[str, Any]) -> str:
        """
        æäº¤è®­ç»ƒä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            model_config: æ¨¡å‹é…ç½®
            training_config: è®­ç»ƒé…ç½®

        Returns:
            ä»»åŠ¡ID
        """
        if self.use_local:
            task_id = self.server.submit_training_task(
                self.client_id, model_config, training_config
            )
        else:
            # è¿œç¨‹æäº¤
            task_id = f"remote_task_{int(time.time() * 1000)}"

        print(f"ğŸ“¤ æäº¤è®­ç»ƒä»»åŠ¡: {task_id}")
        return task_id

    def get_training_status(self, task_id: str) -> Dict[str, Any]:
        """è·å–è®­ç»ƒä»»åŠ¡çŠ¶æ€"""
        if self.use_local:
            return self.server.get_task_status(task_id)
        else:
            return {'status': 'unknown', 'task_id': task_id}

    def get_client_stats(self) -> Dict[str, Any]:
        """è·å–å®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'client_id': self.client_id,
            'decision_count': len(self.decision_history),
            'total_reward': self.total_reward,
            'last_decision_time': self.last_decision_time,
            'avg_decision_interval': self._calculate_avg_interval(),
            'use_local': self.use_local
        }

    def _calculate_avg_interval(self) -> float:
        """è®¡ç®—å¹³å‡å†³ç­–é—´éš”"""
        if len(self.decision_history) < 2:
            return 0.0

        intervals = []
        for i in range(1, len(self.decision_history)):
            interval = self.decision_history[i].timestamp - self.decision_history[i - 1].timestamp
            intervals.append(interval)

        return float(np.mean(intervals)) if intervals else 0.0

    def save_state(self, path: str):
        """ä¿å­˜å®¢æˆ·ç«¯çŠ¶æ€"""
        if self.use_local:
            self.server.save(path)

    def load_state(self, path: str):
        """åŠ è½½å®¢æˆ·ç«¯çŠ¶æ€"""
        if self.use_local:
            self.server.load(path)


def create_lightning_client(config: Dict[str, Any]) -> AgentLightningClient:
    """
    åˆ›å»ºAgent Lightningå®¢æˆ·ç«¯ï¼ˆå·¥å‚å‡½æ•°ï¼‰

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        AgentLightningClientå®ä¾‹
    """
    client_id = config.get('client_id', f'client_{int(time.time())}')

    client_config = {
        'check_interval': config.get('autogen', {}).get('check_interval', 50),
        'max_decisions': config.get('agent_lightning', {}).get('max_decisions', 1000),
        'server_url': config.get('agent_lightning', {}).get('server_url', None),
        'buffer_capacity': config.get('agent_lightning', {}).get('buffer_capacity', 5000),
        'policy_lr': config.get('agent_lightning', {}).get('policy_lr', 1e-3)
    }

    # å†³å®šä½¿ç”¨æœ¬åœ°è¿˜æ˜¯è¿œç¨‹æ¨¡å¼
    use_local = client_config['server_url'] is None

    return AgentLightningClient(
        client_id=client_id,
        config=client_config,
        use_local=use_local
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ”¬ æµ‹è¯•Agent Lightningå®¢æˆ·ç«¯...")

    # åˆ›å»ºå®¢æˆ·ç«¯
    config = {
        'client_id': 'test_client',
        'autogen': {'check_interval': 50},
        'agent_lightning': {
            'max_decisions': 100,
            'buffer_capacity': 1000,
            'policy_lr': 1e-3
        }
    }

    client = create_lightning_client(config)

    # æµ‹è¯•å†³ç­–
    context = {
        'features': {'shape': [32, 96, 7]},
        'metrics': {'mse': 0.35, 'mae': 0.45, 'r2': 0.75},
        'current_params': {'spectral_threshold': 0.5, 'laplacian_weight': 0.01}
    }

    print("\nğŸ§ª æµ‹è¯•å†³ç­–è·å–:")
    decision = client.get_decision(context)
    print(f"  å†³ç­–ID: {decision.decision_id}")
    print(f"  åŠ¨ä½œ: {decision.action}")
    print(f"  å‚æ•°: {decision.parameters}")
    print(f"  ç½®ä¿¡åº¦: {decision.confidence:.3f}")
    print(f"  ç†ç”±: {decision.reasoning}")

    # æµ‹è¯•ç»éªŒè®°å½•
    print("\nğŸ§ª æµ‹è¯•ç»éªŒè®°å½•:")
    experience = Experience(
        state=np.random.randn(10),
        action=1,
        reward=0.5,
        next_state=np.random.randn(10),
        done=False
    )
    client.add_experience(experience)

    # æµ‹è¯•åé¦ˆè®°å½•
    print("\nğŸ§ª æµ‹è¯•åé¦ˆè®°å½•:")
    client.log_feedback(
        state=[0.1, 0.2, 0.3, 0.4, 0.5],
        action=2,
        reward=0.8,
        next_state=[0.2, 0.3, 0.4, 0.5, 0.6],
        done=False
    )

    # æµ‹è¯•å®¢æˆ·ç«¯ç»Ÿè®¡
    print("\nğŸ§ª æµ‹è¯•å®¢æˆ·ç«¯ç»Ÿè®¡:")
    stats = client.get_client_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

    # æµ‹è¯•æœåŠ¡å™¨ç»Ÿè®¡ï¼ˆå¦‚æœæœ¬åœ°ï¼‰
    if hasattr(client, 'server'):
        server_stats = client.server.get_stats()
        print("\nğŸ§ª æœåŠ¡å™¨ç»Ÿè®¡:")
        for key, value in server_stats.items():
            print(f"  {key}: {value}")

    print("\nâœ… Agent Lightningæµ‹è¯•å®Œæˆ!")