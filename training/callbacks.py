"""
callbacks.py - çœŸå®è®­ç»ƒå›è°ƒç³»ç»Ÿ
æ”¯æŒæ™ºèƒ½ä½“äº¤äº’ã€æ¨¡å‹æ£€æŸ¥ç‚¹ã€æ—©åœç­‰
"""
import torch
import numpy as np
import time
import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field
import warnings

warnings.filterwarnings('ignore')


@dataclass
class CallbackState:
    """å›è°ƒçŠ¶æ€å®¹å™¨"""
    epoch: int = 0
    batch_idx: int = 0
    train_loss: float = 0.0
    val_loss: float = 0.0
    metrics: Dict[str, float] = field(default_factory=dict)
    model_state: Optional[Dict[str, Any]] = None
    optimizer_state: Optional[Dict[str, Any]] = None


class BaseCallback:
    """å›è°ƒåŸºç±»"""

    def __init__(self):
        self.name = self.__class__.__name__

    def on_train_begin(self, state: CallbackState):
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        pass

    def on_train_end(self, state: CallbackState):
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
        pass

    def on_epoch_begin(self, state: CallbackState):
        """æ¯ä¸ªepochå¼€å§‹æ—¶è°ƒç”¨"""
        pass

    def on_epoch_end(self, state: CallbackState):
        """æ¯ä¸ªepochç»“æŸæ—¶è°ƒç”¨"""
        pass

    def on_batch_begin(self, state: CallbackState):
        """æ¯ä¸ªbatchå¼€å§‹æ—¶è°ƒç”¨"""
        pass

    def on_batch_end(self, state: CallbackState):
        """æ¯ä¸ªbatchç»“æŸæ—¶è°ƒç”¨"""
        pass

    def on_validation_begin(self, state: CallbackState):
        """éªŒè¯å¼€å§‹æ—¶è°ƒç”¨"""
        pass

    def on_validation_end(self, state: CallbackState):
        """éªŒè¯ç»“æŸæ—¶è°ƒç”¨"""
        pass


class AgentInteractionCallback(BaseCallback):
    """
    æ™ºèƒ½ä½“äº¤äº’å›è°ƒ
    åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè§¦å‘AutoGenæ™ºèƒ½ä½“åˆ†æ
    """

    def __init__(self,
                 agent_client,
                 check_interval: int = 50,
                 min_epoch: int = 1):
        """
        åˆå§‹åŒ–

        Args:
            agent_client: Agent Lightningå®¢æˆ·ç«¯
            check_interval: è§¦å‘é—´éš”ï¼ˆæ‰¹æ¬¡æ•°ï¼‰
            min_epoch: æœ€å°epochæ•°ï¼ˆå‰å‡ ä¸ªepochä¸è§¦å‘ï¼‰
        """
        super().__init__()
        self.agent_client = agent_client
        self.check_interval = check_interval
        self.min_epoch = min_epoch
        self.interaction_count = 0
        self.last_interaction_batch = -1

        print(f"âœ… åˆå§‹åŒ–æ™ºèƒ½ä½“äº¤äº’å›è°ƒï¼Œé—´éš”={check_interval}")

    def on_batch_end(self, state: CallbackState):
        """åœ¨æ¯ä¸ªbatchç»“æŸæ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦æ™ºèƒ½ä½“äº¤äº’"""
        # æ£€æŸ¥æ¡ä»¶
        if state.epoch < self.min_epoch:
            return

        if state.batch_idx % self.check_interval != 0:
            return

        # é¿å…é‡å¤è§¦å‘
        if state.batch_idx == self.last_interaction_batch:
            return

        self.last_interaction_batch = state.batch_idx

        # è§¦å‘æ™ºèƒ½ä½“äº¤äº’
        self._trigger_agent_interaction(state)

    def _trigger_agent_interaction(self, state: CallbackState):
        """è§¦å‘æ™ºèƒ½ä½“äº¤äº’"""
        self.interaction_count += 1

        print(f"\nğŸ¤– æ™ºèƒ½ä½“äº¤äº’ #{self.interaction_count} "
              f"(Epoch {state.epoch}, Batch {state.batch_idx})")

        # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆè¿™é‡Œéœ€è¦æ¨¡å‹æä¾›ç‰¹å¾ï¼‰
        context = self._prepare_agent_context(state)

        # è·å–å†³ç­–
        decision = self.agent_client.get_decision(context)

        # è®°å½•å†³ç­–
        decision_record = {
            'epoch': state.epoch,
            'batch': state.batch_idx,
            'decision_id': decision.decision_id,
            'action': decision.action,
            'parameters': decision.parameters,
            'confidence': decision.confidence,
            'reasoning': decision.reasoning,
            'timestamp': time.time()
        }

        # ä¿å­˜å†³ç­–è®°å½•
        self._save_decision_record(decision_record)

        # è¿”å›å†³ç­–ä¿¡æ¯ï¼ˆä¾›å¤–éƒ¨ä½¿ç”¨ï¼‰
        state.metrics['agent_decision'] = decision.action
        state.metrics['agent_confidence'] = decision.confidence

    def _prepare_agent_context(self, state: CallbackState) -> Dict[str, Any]:
        """å‡†å¤‡æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡"""
        # è¿™é‡Œåº”è¯¥ä»æ¨¡å‹è·å–ç‰¹å¾ï¼Œç®€åŒ–å®ç°
        context = {
            'epoch': state.epoch,
            'batch_idx': state.batch_idx,
            'metrics': {
                'train_loss': state.train_loss,
                'val_loss': state.val_loss if state.val_loss else 0.0
            },
            'current_params': {
                'spectral_threshold': 0.5,  # åº”è¯¥ä»æ¨¡å‹è·å–
                'laplacian_weight': 0.01
            },
            'features': {
                'shape': [32, 96, 7],  # ç®€åŒ–
                'statistics': {'mean': 0.0, 'std': 1.0}
            }
        }

        return context

    def _save_decision_record(self, record: Dict[str, Any]):
        """ä¿å­˜å†³ç­–è®°å½•"""
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs('./logs/agent_decisions', exist_ok=True)

        # ä¿å­˜æ–‡ä»¶
        filename = f"./logs/agent_decisions/decision_{record['epoch']}_{record['batch']}.json"
        with open(filename, 'w') as f:
            json.dump(record, f, indent=2)

        print(f"ğŸ“ ä¿å­˜å†³ç­–è®°å½•: {filename}")


class ModelCheckpoint(BaseCallback):
    """æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ"""

    def __init__(self,
                 save_dir: str = './checkpoints',
                 save_best_only: bool = True,
                 monitor: str = 'val_loss',
                 mode: str = 'min'):
        """
        åˆå§‹åŒ–

        Args:
            save_dir: ä¿å­˜ç›®å½•
            save_best_only: æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹
            monitor: ç›‘æ§çš„æŒ‡æ ‡
            mode: 'min' æˆ– 'max'
        """
        super().__init__()
        self.save_dir = Path(save_dir)
        self.save_best_only = save_best_only
        self.monitor = monitor
        self.mode = mode

        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.best_value = float('inf') if mode == 'min' else float('-inf')
        self.best_epoch = 0

        print(f"âœ… åˆå§‹åŒ–æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œç›‘æ§æŒ‡æ ‡: {monitor}")

    def on_epoch_end(self, state: CallbackState):
        """æ¯ä¸ªepochç»“æŸæ—¶ä¿å­˜æ¨¡å‹"""
        if self.monitor not in state.metrics:
            print(f"âš ï¸  ç›‘æ§æŒ‡æ ‡ {self.monitor} ä¸å­˜åœ¨")
            return

        current_value = state.metrics[self.monitor]

        # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€ä½³å€¼
        is_best = False
        if self.mode == 'min':
            if current_value < self.best_value:
                self.best_value = current_value
                self.best_epoch = state.epoch
                is_best = True
        else:  # max
            if current_value > self.best_value:
                self.best_value = current_value
                self.best_epoch = state.epoch
                is_best = True

        # å†³å®šæ˜¯å¦ä¿å­˜
        should_save = not self.save_best_only or is_best

        if should_save:
            self._save_checkpoint(state, is_best)

    def _save_checkpoint(self, state: CallbackState, is_best: bool):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': state.epoch,
            'model_state_dict': state.model_state,
            'optimizer_state_dict': state.optimizer_state,
            'metrics': state.metrics,
            'best_value': self.best_value,
            'best_epoch': self.best_epoch
        }

        # åŸºç¡€æ–‡ä»¶å
        if is_best:
            filename = f"best_model_epoch{state.epoch}.pth"
        else:
            filename = f"checkpoint_epoch{state.epoch}.pth"

        # å®Œæ•´è·¯å¾„
        filepath = self.save_dir / filename

        # ä¿å­˜
        torch.save(checkpoint, filepath)

        print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {filepath}")

        # å¦‚æœæ˜¯æœ€å¥½æ¨¡å‹ï¼Œä¿å­˜é¢å¤–ä¿¡æ¯
        if is_best:
            info_file = self.save_dir / f"best_model_info.json"
            info = {
                'epoch': state.epoch,
                'value': self.best_value,
                'metrics': state.metrics,
                'timestamp': time.time()
            }
            with open(info_file, 'w') as f:
                json.dump(info, f, indent=2)


class EarlyStopping(BaseCallback):
    """æ—©åœå›è°ƒ"""

    def __init__(self,
                 patience: int = 20,
                 min_delta: float = 1e-4,
                 monitor: str = 'val_loss',
                 mode: str = 'min'):
        """
        åˆå§‹åŒ–

        Args:
            patience: è€å¿ƒå€¼ï¼ˆå¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„ï¼‰
            min_delta: æœ€å°æ”¹å–„å€¼
            monitor: ç›‘æ§æŒ‡æ ‡
            mode: 'min' æˆ– 'max'
        """
        super().__init__()
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.mode = mode

        self.counter = 0
        self.best_value = float('inf') if mode == 'min' else float('-inf')
        self.should_stop = False
        self.stopped_epoch = 0

        print(f"âœ… åˆå§‹åŒ–æ—©åœï¼Œè€å¿ƒå€¼={patience}")

    def on_epoch_end(self, state: CallbackState):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        if self.monitor not in state.metrics:
            return

        current_value = state.metrics[self.monitor]

        # æ£€æŸ¥æ˜¯å¦æ”¹å–„
        if self.mode == 'min':
            improvement = self.best_value - current_value
            if improvement > self.min_delta:
                self.best_value = current_value
                self.counter = 0
            else:
                self.counter += 1
        else:  # max
            improvement = current_value - self.best_value
            if improvement > self.min_delta:
                self.best_value = current_value
                self.counter = 0
            else:
                self.counter += 1

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
        if self.counter >= self.patience:
            self.should_stop = True
            self.stopped_epoch = state.epoch

            print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨epoch {state.epoch}åœæ­¢è®­ç»ƒ")
            print(f"   æœ€ä½³ {self.monitor}: {self.best_value:.6f}")

    def on_train_end(self, state: CallbackState):
        """è®­ç»ƒç»“æŸæ—¶è®°å½•æ—©åœä¿¡æ¯"""
        if self.should_stop:
            print(f"ğŸ è®­ç»ƒå› æ—©åœè€Œç»“æŸï¼Œæœ€ä½³epoch: {self.stopped_epoch - self.patience}")


class LearningRateScheduler(BaseCallback):
    """å­¦ä¹ ç‡è°ƒåº¦å›è°ƒ"""

    def __init__(self,
                 optimizer,
                 scheduler_type: str = 'plateau',
                 patience: int = 10,
                 factor: float = 0.5,
                 min_lr: float = 1e-6):
        """
        åˆå§‹åŒ–

        Args:
            optimizer: ä¼˜åŒ–å™¨
            scheduler_type: è°ƒåº¦å™¨ç±»å‹ ('plateau', 'step', 'cosine')
            patience: è€å¿ƒå€¼
            factor: è°ƒæ•´å› å­
            min_lr: æœ€å°å­¦ä¹ ç‡
        """
        super().__init__()
        self.optimizer = optimizer
        self.scheduler_type = scheduler_type
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr

        # åˆ›å»ºè°ƒåº¦å™¨
        if scheduler_type == 'plateau':
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', patience=patience, factor=factor, min_lr=min_lr
            )
        elif scheduler_type == 'step':
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer, step_size=patience, gamma=factor
            )
        elif scheduler_type == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=patience, eta_min=min_lr
            )
        else:
            raise ValueError(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")

        print(f"âœ… åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type}")

    def on_epoch_end(self, state: CallbackState):
        """æ›´æ–°å­¦ä¹ ç‡"""
        if self.scheduler_type == 'plateau':
            # ReduceLROnPlateauéœ€è¦éªŒè¯æŸå¤±
            if 'val_loss' in state.metrics:
                self.scheduler.step(state.metrics['val_loss'])
            else:
                self.scheduler.step(state.train_loss)
        else:
            # å…¶ä»–è°ƒåº¦å™¨
            self.scheduler.step()

        # è®°å½•å½“å‰å­¦ä¹ ç‡
        current_lr = self.optimizer.param_groups[0]['lr']
        state.metrics['learning_rate'] = current_lr

        # æ‰“å°å­¦ä¹ ç‡å˜åŒ–
        if current_lr < self.min_lr * 1.1:  # æ¥è¿‘æœ€å°å€¼
            print(f"ğŸ“‰ å­¦ä¹ ç‡æ¥è¿‘æœ€å°å€¼: {current_lr:.6f}")


class MetricsLogger(BaseCallback):
    """æŒ‡æ ‡è®°å½•å›è°ƒ"""

    def __init__(self,
                 log_dir: str = './logs',
                 log_interval: int = 10):
        """
        åˆå§‹åŒ–

        Args:
            log_dir: æ—¥å¿—ç›®å½•
            log_interval: è®°å½•é—´éš”ï¼ˆepochï¼‰
        """
        super().__init__()
        self.log_dir = Path(log_dir)
        self.log_interval = log_interval

        self.log_dir.mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—æ–‡ä»¶
        self.csv_file = self.log_dir / 'training_log.csv'
        self.json_file = self.log_dir / 'training_history.json'

        # åˆå§‹åŒ–CSVæ–‡ä»¶
        if not self.csv_file.exists():
            with open(self.csv_file, 'w') as f:
                f.write('epoch,train_loss,val_loss,learning_rate,timestamp\n')

        self.history = []

        print(f"âœ… åˆå§‹åŒ–æŒ‡æ ‡è®°å½•å™¨ï¼Œæ—¥å¿—ç›®å½•: {log_dir}")

    def on_epoch_end(self, state: CallbackState):
        """è®°å½•æŒ‡æ ‡"""
        # æ”¶é›†æŒ‡æ ‡
        log_entry = {
            'epoch': state.epoch,
            'train_loss': state.train_loss,
            'val_loss': state.val_loss,
            'timestamp': time.time()
        }

        # æ·»åŠ å…¶ä»–æŒ‡æ ‡
        for key, value in state.metrics.items():
            if key not in log_entry:
                log_entry[key] = value

        # æ·»åŠ åˆ°å†å²
        self.history.append(log_entry)

        # å®šæœŸä¿å­˜
        if state.epoch % self.log_interval == 0 or state.epoch == 1:
            self._save_logs()

    def on_train_end(self, state: CallbackState):
        """è®­ç»ƒç»“æŸæ—¶ä¿å­˜æ‰€æœ‰æ—¥å¿—"""
        self._save_logs()
        print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—ä¿å­˜å®Œæˆ: {self.json_file}")

    def _save_logs(self):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        # ä¿å­˜ä¸ºJSON
        with open(self.json_file, 'w') as f:
            json.dump(self.history, f, indent=2)

        # æ›´æ–°CSV
        with open(self.csv_file, 'w') as f:
            # å†™å…¥æ ‡é¢˜
            if self.history:
                headers = list(self.history[0].keys())
                f.write(','.join(headers) + '\n')

                # å†™å…¥æ•°æ®
                for entry in self.history:
                    row = [str(entry.get(h, '')) for h in headers]
                    f.write(','.join(row) + '\n')


class ProgressBar(BaseCallback):
    """è¿›åº¦æ¡å›è°ƒ"""

    def __init__(self, total_epochs: int):
        """
        åˆå§‹åŒ–

        Args:
            total_epochs: æ€»epochæ•°
        """
        super().__init__()
        self.total_epochs = total_epochs
        self.start_time = None
        self.current_epoch = 0

    def on_train_begin(self, state: CallbackState):
        """è®­ç»ƒå¼€å§‹æ—¶"""
        self.start_time = time.time()
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€»epochs: {self.total_epochs}")
        print("=" * 60)

    def on_epoch_begin(self, state: CallbackState):
        """epochå¼€å§‹æ—¶"""
        self.current_epoch = state.epoch
        print(f"\nEpoch {state.epoch}/{self.total_epochs}")
        print("-" * 40)

    def on_epoch_end(self, state: CallbackState):
        """epochç»“æŸæ—¶æ˜¾ç¤ºè¿›åº¦"""
        elapsed = time.time() - self.start_time
        epoch_time = elapsed / max(state.epoch, 1)
        remaining = (self.total_epochs - state.epoch) * epoch_time

        print(f"  è®­ç»ƒæŸå¤±: {state.train_loss:.6f}")
        if state.val_loss:
            print(f"  éªŒè¯æŸå¤±: {state.val_loss:.6f}")

        # æ˜¾ç¤ºå…¶ä»–é‡è¦æŒ‡æ ‡
        for key in ['mse', 'mae', 'learning_rate']:
            if key in state.metrics:
                print(f"  {key}: {state.metrics[key]:.6f}")

        print(f"  å·²ç”¨æ—¶: {elapsed:.1f}sï¼Œé¢„è®¡å‰©ä½™: {remaining:.1f}s")

    def on_train_end(self, state: CallbackState):
        """è®­ç»ƒç»“æŸæ—¶"""
        total_time = time.time() - self.start_time
        print("=" * 60)
        print(f"ğŸ è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}s")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {min([h.get('val_loss', float('inf')) for h in getattr(self, 'history', [])])}")
        print("=" * 60)


class CallbackHandler:
    """å›è°ƒå¤„ç†å™¨"""

    def __init__(self, callbacks: List[BaseCallback]):
        """
        åˆå§‹åŒ–

        Args:
            callbacks: å›è°ƒåˆ—è¡¨
        """
        self.callbacks = callbacks
        self.state = CallbackState()

        print(f"âœ… åˆå§‹åŒ–å›è°ƒå¤„ç†å™¨ï¼ŒåŒ…å« {len(callbacks)} ä¸ªå›è°ƒ")

    def set_model_optimizer(self, model, optimizer):
        """è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
        self.model = model
        self.optimizer = optimizer

    def on_train_begin(self, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨æ‰€æœ‰å›è°ƒ"""
        self.state = CallbackState()
        for callback in self.callbacks:
            callback.on_train_begin(self.state)

    def on_train_end(self, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨æ‰€æœ‰å›è°ƒ"""
        for callback in self.callbacks:
            callback.on_train_end(self.state)

    def on_epoch_begin(self, epoch: int, **kwargs):
        """epochå¼€å§‹æ—¶"""
        self.state.epoch = epoch
        for callback in self.callbacks:
            callback.on_epoch_begin(self.state)

    def on_epoch_end(self, train_loss: float, val_loss: float = None, metrics: Dict = None, **kwargs):
        """epochç»“æŸæ—¶"""
        self.state.train_loss = train_loss
        self.state.val_loss = val_loss
        self.state.metrics = metrics or {}

        # ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
        if hasattr(self, 'model'):
            self.state.model_state = self.model.state_dict()
        if hasattr(self, 'optimizer'):
            self.state.optimizer_state = self.optimizer.state_dict()

        for callback in self.callbacks:
            callback.on_epoch_end(self.state)

    def on_batch_begin(self, batch_idx: int, **kwargs):
        """batchå¼€å§‹æ—¶"""
        self.state.batch_idx = batch_idx
        for callback in self.callbacks:
            callback.on_batch_begin(self.state)

    def on_batch_end(self, loss: float = None, **kwargs):
        """batchç»“æŸæ—¶"""
        if loss is not None:
            self.state.train_loss = loss
        for callback in self.callbacks:
            callback.on_batch_end(self.state)

    def on_validation_begin(self, **kwargs):
        """éªŒè¯å¼€å§‹æ—¶"""
        for callback in self.callbacks:
            callback.on_validation_begin(self.state)

    def on_validation_end(self, val_loss: float, metrics: Dict = None, **kwargs):
        """éªŒè¯ç»“æŸæ—¶"""
        self.state.val_loss = val_loss
        self.state.metrics = metrics or {}
        for callback in self.callbacks:
            callback.on_validation_end(self.state)


def create_default_callbacks(config: Dict[str, Any],
                             model=None,
                             optimizer=None,
                             agent_client=None) -> CallbackHandler:
    """
    åˆ›å»ºé»˜è®¤å›è°ƒé›†åˆ

    Args:
        config: é…ç½®å­—å…¸
        model: æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        optimizer: ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
        agent_client: æ™ºèƒ½ä½“å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰

    Returns:
        å›è°ƒå¤„ç†å™¨
    """
    callbacks = []

    # è¿›åº¦æ¡
    total_epochs = config.get('training', {}).get('epochs', 100)
    callbacks.append(ProgressBar(total_epochs))

    # æŒ‡æ ‡è®°å½•å™¨
    log_dir = config.get('logging', {}).get('log_dir', './logs')
    callbacks.append(MetricsLogger(log_dir=log_dir))

    # æ¨¡å‹æ£€æŸ¥ç‚¹
    checkpoint_dir = config.get('logging', {}).get('checkpoint_dir', './checkpoints')
    monitor = config.get('training', {}).get('early_stopping', {}).get('monitor', 'val_loss')
    callbacks.append(ModelCheckpoint(
        save_dir=checkpoint_dir,
        monitor=monitor,
        mode='min'
    ))

    # æ—©åœ
    early_stop_config = config.get('training', {}).get('early_stopping', {})
    if early_stop_config.get('enabled', True):
        callbacks.append(EarlyStopping(
            patience=early_stop_config.get('patience', 20),
            min_delta=early_stop_config.get('min_delta', 1e-4),
            monitor=monitor,
            mode='min'
        ))

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆéœ€è¦ä¼˜åŒ–å™¨ï¼‰
    if optimizer is not None:
        scheduler_config = config.get('training', {}).get('scheduler', {})
        callbacks.append(LearningRateScheduler(
            optimizer=optimizer,
            scheduler_type=scheduler_config.get('type', 'plateau'),
            patience=scheduler_config.get('patience', 10),
            factor=scheduler_config.get('factor', 0.5),
            min_lr=1e-6
        ))

    # æ™ºèƒ½ä½“äº¤äº’å›è°ƒï¼ˆéœ€è¦æ™ºèƒ½ä½“å®¢æˆ·ç«¯ï¼‰
    if agent_client is not None:
        autogen_config = config.get('autogen', {})
        callbacks.append(AgentInteractionCallback(
            agent_client=agent_client,
            check_interval=autogen_config.get('check_interval', 50),
            min_epoch=1
        ))

    # åˆ›å»ºå¤„ç†å™¨
    handler = CallbackHandler(callbacks)

    # è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨
    if model is not None and optimizer is not None:
        handler.set_model_optimizer(model, optimizer)

    return handler


if __name__ == "__main__":
    # æµ‹è¯•å›è°ƒç³»ç»Ÿ
    print("æµ‹è¯•å›è°ƒç³»ç»Ÿ...")

    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'training': {
            'epochs': 5,
            'early_stopping': {'enabled': True, 'patience': 3},
            'scheduler': {'type': 'plateau', 'patience': 2}
        },
        'logging': {
            'log_dir': './test_logs',
            'checkpoint_dir': './test_checkpoints'
        },
        'autogen': {'check_interval': 2}
    }

    # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = torch.nn.Linear(10, 1)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # åˆ›å»ºå›è°ƒå¤„ç†å™¨
    handler = create_default_callbacks(config, model, optimizer, agent_client=None)

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    handler.on_train_begin()

    for epoch in range(1, 4):
        handler.on_epoch_begin(epoch)

        # æ¨¡æ‹Ÿbatchè®­ç»ƒ
        for batch in range(1, 4):
            handler.on_batch_begin(batch)

            # æ¨¡æ‹ŸæŸå¤±
            loss = 1.0 / (epoch * batch)
            handler.on_batch_end(loss)

        # epochç»“æŸ
        val_loss = 0.5 / epoch
        metrics = {'mse': 0.1 / epoch, 'mae': 0.2 / epoch}
        handler.on_epoch_end(train_loss=0.1 / epoch, val_loss=val_loss, metrics=metrics)

    handler.on_train_end()

    print("\nâœ… å›è°ƒç³»ç»Ÿæµ‹è¯•å®Œæˆ")