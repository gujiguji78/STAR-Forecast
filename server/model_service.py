"""
æ¨¡å‹è®­ç»ƒå’Œæ¨ç†APIæœåŠ¡
æä¾›æ¨¡å‹è®­ç»ƒã€é¢„æµ‹ã€ç®¡ç†çš„REST APIæ¥å£
"""
from fastapi import FastAPI, HTTPException, BackgroundTasks, File, UploadFile
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import uuid
import json
from datetime import datetime
import logging
import asyncio
from pathlib import Path
import shutil

import torch
import torch.nn as nn
import numpy as np

from ..models.istr import ISTRNetwork
from ..models.predictor import MultiHeadPredictor
from ..training.trainer import STARForecastTrainer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPIåº”ç”¨
app = FastAPI(
    title="æ¨¡å‹æœåŠ¡",
    version="1.0.0",
    description="æä¾›æ¨¡å‹è®­ç»ƒå’Œæ¨ç†APIæœåŠ¡"
)

# å…¨å±€æ¨¡å‹ç¼“å­˜
model_cache = {}
trainer_cache = {}


class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""
    model_id: str
    model_type: str
    status: str
    created_at: str
    last_used: str
    metrics: Dict[str, Any]


class TrainRequest(BaseModel):
    """è®­ç»ƒè¯·æ±‚"""
    client_id: str
    model_type: str = "istr"
    config: Optional[Dict[str, Any]] = None
    data_path: str = "./data/ETTh1.csv"
    epochs: int = 100
    batch_size: int = 32


class PredictRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚"""
    model_id: str
    input_data: List[List[float]]  # [seq_len, features]
    return_features: bool = False


class ModelConfigUpdate(BaseModel):
    """æ¨¡å‹é…ç½®æ›´æ–°"""
    model_id: str
    updates: Dict[str, Any]


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ æ¨¡å‹æœåŠ¡å¯åŠ¨")


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    logger.info("ğŸ›‘ æ¨¡å‹æœåŠ¡å…³é—­")

    # æ¸…ç†æ¨¡å‹ç¼“å­˜
    for model_id in list(model_cache.keys()):
        del model_cache[model_id]

    for trainer_id in list(trainer_cache.keys()):
        del trainer_cache[trainer_id]


@app.post("/api/v1/models/train", status_code=202)
async def train_model(request: TrainRequest, background_tasks: BackgroundTasks):
    """è®­ç»ƒæ¨¡å‹"""
    model_id = str(uuid.uuid4())

    # å­˜å‚¨è®­ç»ƒä»»åŠ¡
    trainer_cache[model_id] = {
        "status": "pending",
        "client_id": request.client_id,
        "created_at": datetime.now().isoformat(),
        "progress": 0.0,
        "metrics": {}
    }

    # åœ¨åå°æ‰§è¡Œè®­ç»ƒ
    background_tasks.add_task(
        execute_training,
        model_id,
        request.client_id,
        request.model_type,
        request.config or {},
        request.data_path,
        request.epochs,
        request.batch_size
    )

    return {
        "model_id": model_id,
        "status": "training_started",
        "message": "è®­ç»ƒä»»åŠ¡å·²æäº¤",
        "timestamp": datetime.now().isoformat()
    }


async def execute_training(model_id: str, client_id: str, model_type: str,
                           config: Dict[str, Any], data_path: str,
                           epochs: int, batch_size: int):
    """æ‰§è¡Œè®­ç»ƒä»»åŠ¡"""
    try:
        trainer_cache[model_id]["status"] = "training"
        trainer_cache[model_id]["started_at"] = datetime.now().isoformat()

        logger.info(f"ğŸ”§ å¼€å§‹è®­ç»ƒæ¨¡å‹ {model_id}")

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = STARForecastTrainer()

        # æ›´æ–°é…ç½®
        if config:
            trainer.config.update(config)

        trainer.config['training']['epochs'] = epochs
        trainer.config['data']['batch_size'] = batch_size

        # æ„å»ºæ¨¡å‹
        trainer.build_models()
        trainer.build_optimizer()

        # è®­ç»ƒ
        results = trainer.train(data_path)

        # ä¿å­˜æ¨¡å‹
        model_path = Path(f"./models/{model_id}")
        model_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ¨¡å‹çŠ¶æ€
        torch.save({
            'istr_state_dict': trainer.istr_model.state_dict(),
            'predictor_state_dict': trainer.predictor.state_dict(),
            'config': trainer.config
        }, model_path / "model.pth")

        # ä¿å­˜ç»“æœ
        with open(model_path / "results.json", 'w') as f:
            json.dump(results, f, indent=2)

        # æ›´æ–°ç¼“å­˜
        model_cache[model_id] = {
            "model_type": model_type,
            "model_path": str(model_path),
            "config": trainer.config,
            "results": results,
            "created_at": datetime.now().isoformat(),
            "last_used": datetime.now().isoformat()
        }

        trainer_cache[model_id].update({
            "status": "completed",
            "completed_at": datetime.now().isoformat(),
            "progress": 1.0,
            "metrics": results,
            "model_path": str(model_path)
        })

        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: {model_id}")

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")

        trainer_cache[model_id].update({
            "status": "failed",
            "completed_at": datetime.now().isoformat(),
            "error": str(e)
        })


@app.get("/api/v1/models/training/status/{model_id}")
async def get_training_status(model_id: str):
    """è·å–è®­ç»ƒçŠ¶æ€"""
    if model_id not in trainer_cache:
        raise HTTPException(status_code=404, detail="è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨")

    return trainer_cache[model_id]


@app.post("/api/v1/models/predict")
async def predict(request: PredictRequest):
    """æ¨¡å‹é¢„æµ‹"""
    if request.model_id not in model_cache:
        raise HTTPException(status_code=404, detail="æ¨¡å‹ä¸å­˜åœ¨")

    try:
        model_info = model_cache[request.model_id]

        # åŠ è½½æ¨¡å‹
        model_path = Path(model_info["model_path"])
        checkpoint = torch.load(model_path / "model.pth", map_location='cpu')

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        config = model_info["config"]

        istr_model = ISTRNetwork(config)
        predictor = MultiHeadPredictor(
            hidden_dim=config['istr']['hidden_dim'],
            pred_len=config['data']['pred_len'],
            heads=config['predictor']['heads']
        )

        # åŠ è½½æƒé‡
        istr_model.load_state_dict(checkpoint['istr_state_dict'])
        predictor.load_state_dict(checkpoint['predictor_state_dict'])

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        istr_model.eval()
        predictor.eval()

        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_tensor = torch.FloatTensor(request.input_data).unsqueeze(0)  # [1, seq_len, features]

        with torch.no_grad():
            # æå–ç‰¹å¾
            features = istr_model(input_tensor)

            # é¢„æµ‹
            predictions = predictor(features)

            # è½¬æ¢ä¸ºåˆ—è¡¨
            pred_list = predictions.squeeze(0).cpu().numpy().tolist()

            result = {
                "predictions": pred_list,
                "timestamp": datetime.now().isoformat()
            }

            # å¦‚æœéœ€è¦è¿”å›ç‰¹å¾
            if request.return_features:
                result["features"] = features.squeeze(0).cpu().numpy().tolist()

            # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
            model_cache[request.model_id]["last_used"] = datetime.now().isoformat()

            return result

    except Exception as e:
        logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/models")
async def list_models():
    """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
    models = []

    for model_id, model_info in model_cache.items():
        models.append({
            "model_id": model_id,
            "model_type": model_info["model_type"],
            "created_at": model_info["created_at"],
            "last_used": model_info["last_used"],
            "metrics": model_info.get("results", {}).get("test_metrics", {})
        })

    return {
        "models": models,
        "count": len(models)
    }


@app.get("/api/v1/models/{model_id}")
async def get_model_info(model_id: str):
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    if model_id not in model_cache:
        raise HTTPException(status_code=404, detail="æ¨¡å‹ä¸å­˜åœ¨")

    model_info = model_cache[model_id].copy()

    # æ·»åŠ è®­ç»ƒå†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    if model_id in trainer_cache:
        model_info["training_history"] = trainer_cache[model_id]

    return model_info


@app.delete("/api/v1/models/{model_id}")
async def delete_model(model_id: str):
    """åˆ é™¤æ¨¡å‹"""
    if model_id not in model_cache:
        raise HTTPException(status_code=404, detail="æ¨¡å‹ä¸å­˜åœ¨")

    try:
        # åˆ é™¤æ¨¡å‹æ–‡ä»¶
        model_path = Path(model_cache[model_id]["model_path"])
        if model_path.exists():
            shutil.rmtree(model_path)

        # ä»ç¼“å­˜ä¸­åˆ é™¤
        del model_cache[model_id]

        if model_id in trainer_cache:
            del trainer_cache[model_id]

        return {
            "status": "success",
            "message": f"æ¨¡å‹ {model_id} å·²åˆ é™¤",
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ åˆ é™¤æ¨¡å‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/models/{model_id}/config")
async def update_model_config(model_id: str, update: ModelConfigUpdate):
    """æ›´æ–°æ¨¡å‹é…ç½®"""
    if model_id not in model_cache:
        raise HTTPException(status_code=404, detail="æ¨¡å‹ä¸å­˜åœ¨")

    try:
        model_cache[model_id]["config"].update(update.updates)

        return {
            "status": "success",
            "message": "é…ç½®å·²æ›´æ–°",
            "model_id": model_id,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/models/upload")
async def upload_model(file: UploadFile = File(...)):
    """ä¸Šä¼ æ¨¡å‹"""
    try:
        # ç”Ÿæˆæ¨¡å‹ID
        model_id = str(uuid.uuid4())
        model_dir = Path(f"./models/uploaded_{model_id}")
        model_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        file_path = model_dir / file.filename
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹éªŒè¯é€»è¾‘
        # æš‚æ—¶å‡è®¾ä¸Šä¼ çš„æ˜¯æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶

        model_cache[model_id] = {
            "model_type": "uploaded",
            "model_path": str(model_dir),
            "filename": file.filename,
            "created_at": datetime.now().isoformat(),
            "last_used": datetime.now().isoformat()
        }

        return {
            "status": "success",
            "message": "æ¨¡å‹ä¸Šä¼ æˆåŠŸ",
            "model_id": model_id,
            "filename": file.filename,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "æ¨¡å‹æœåŠ¡",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat(),
        "models_cached": len(model_cache),
        "training_tasks": len(trainer_cache)
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8002,
        workers=2
    )