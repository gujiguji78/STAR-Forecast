"""
ISTRæ¨¡å‹æ¨¡å— - STAR-Forecast
ISTR (Interpretable Sparse Transformer for Time Series) æ¨¡å‹
çœŸå®å¼€å‘ç‰ˆæœ¬ - æ•´åˆTCN+è°±é—¨æ§+æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple, List
import math


class SparseAttention(nn.Module):
    """ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1,
                 sparsity_factor: int = 4):
        super(SparseAttention, self).__init__()

        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.dropout = dropout
        self.sparsity_factor = sparsity_factor

        # çº¿æ€§å˜æ¢
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)

        # Dropout
        self.dropout_layer = nn.Dropout(dropout)

        # ç¼©æ”¾å› å­
        self.scale = math.sqrt(self.head_dim)

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)

        # çº¿æ€§å˜æ¢å¹¶åˆ†å‰²å¤´
        Q = self.q_linear(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        K = self.k_linear(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        V = self.v_linear(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # åº”ç”¨ç¨€ç–æ©ç 
        if self.sparsity_factor > 1:
            seq_len = scores.size(-1)
            sparse_mask = self._create_sparse_mask(seq_len, batch_size).to(scores.device)
            scores = scores.masked_fill(sparse_mask == 0, -1e9)

        # åº”ç”¨æ³¨æ„åŠ›æ©ç 
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout_layer(attn_weights)

        # åº”ç”¨æ³¨æ„åŠ›
        context = torch.matmul(attn_weights, V)

        # åˆå¹¶å¤´
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # è¾“å‡ºå˜æ¢
        output = self.out_linear(context)

        return output, attn_weights

    def _create_sparse_mask(self, seq_len: int, batch_size: int) -> torch.Tensor:
        """åˆ›å»ºç¨€ç–æ³¨æ„åŠ›æ©ç """
        # åˆ›å»ºå¸¦çŠ¶ç¨€ç–æ©ç 
        mask = torch.ones(seq_len, seq_len)

        # ä¿ç•™å¯¹è§’çº¿é™„è¿‘çš„å…ƒç´ 
        bandwidth = seq_len // self.sparsity_factor
        for i in range(seq_len):
            start = max(0, i - bandwidth)
            end = min(seq_len, i + bandwidth + 1)
            mask[i, :start] = 0
            mask[i, end:] = 0

        # æ‰©å±•ç»´åº¦ä»¥é€‚åº”å¤šå¤´æ³¨æ„åŠ›
        mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, L, L]
        mask = mask.repeat(batch_size, self.n_heads, 1, 1)

        return mask


class ISTRModel(nn.Module):
    """ISTR (Interpretable Sparse Transformer) æ¨¡å‹"""

    def __init__(self, config: Dict[str, Any]):
        super(ISTRModel, self).__init__()

        # æ¨¡å‹å‚æ•°
        self.seq_len = config.get('seq_len', 96)
        self.pred_len = config.get('pred_len', 24)
        self.d_model = config.get('d_model', 512)
        self.n_heads = config.get('n_heads', 8)
        self.e_layers = config.get('e_layers', 2)
        self.d_layers = config.get('d_layers', 1)
        self.d_ff = config.get('d_ff', 2048)
        self.dropout = config.get('dropout', 0.05)
        self.activation = config.get('activation', 'gelu')
        self.enc_in = config.get('enc_in', 7)
        self.c_out = config.get('c_out', 1)
        self.sparsity_factor = config.get('sparsity_factor', 4)

        # è¾“å…¥åµŒå…¥
        self.enc_embedding = nn.Linear(self.enc_in, self.d_model)
        self.dec_embedding = nn.Linear(self.c_out, self.d_model)

        # ä½ç½®ç¼–ç 
        self.positional_encoding = self._create_positional_encoding(self.d_model, 5000)

        # ç¼–ç å™¨å±‚
        self.encoder_layers = nn.ModuleList([
            ISTREncoderLayer(
                d_model=self.d_model,
                n_heads=self.n_heads,
                d_ff=self.d_ff,
                dropout=self.dropout,
                activation=self.activation,
                sparsity_factor=self.sparsity_factor
            ) for _ in range(self.e_layers)
        ])

        # è§£ç å™¨å±‚
        self.decoder_layers = nn.ModuleList([
            ISTRDecoderLayer(
                d_model=self.d_model,
                n_heads=self.n_heads,
                d_ff=self.d_ff,
                dropout=self.dropout,
                activation=self.activation,
                sparsity_factor=self.sparsity_factor
            ) for _ in range(self.d_layers)
        ])

        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(self.d_model, self.c_out)

        # Dropout
        self.dropout_layer = nn.Dropout(self.dropout)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _create_positional_encoding(self, d_model: int, max_len: int = 5000) -> nn.Module:
        """åˆ›å»ºä½ç½®ç¼–ç """

        class PositionalEncoding(nn.Module):
            def __init__(self, d_model, max_len=5000):
                super().__init__()
                pe = torch.zeros(max_len, d_model)
                position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                                     (-math.log(10000.0) / d_model))
                pe[:, 0::2] = torch.sin(position * div_term)
                pe[:, 1::2] = torch.cos(position * div_term)
                pe = pe.unsqueeze(0)
                self.register_buffer('pe', pe)

            def forward(self, x):
                return x + self.pe[:, :x.size(1)]

        return PositionalEncoding(d_model, max_len)

    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, x_enc: torch.Tensor, x_dec: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        batch_size = x_enc.size(0)

        # å‡†å¤‡è§£ç å™¨è¾“å…¥
        if x_dec is None:
            x_dec = torch.zeros(batch_size, self.pred_len, self.c_out).to(x_enc.device)

        # ===== ç¼–ç å™¨ =====
        enc_out = self.enc_embedding(x_enc) * math.sqrt(self.d_model)
        enc_out = self.positional_encoding(enc_out)
        enc_out = self.dropout_layer(enc_out)

        # ç¼–ç å™¨å±‚
        enc_attn_weights = []
        for encoder_layer in self.encoder_layers:
            enc_out, attn_weights = encoder_layer(enc_out)
            enc_attn_weights.append(attn_weights)

        # ===== è§£ç å™¨ =====
        dec_out = self.dec_embedding(x_dec) * math.sqrt(self.d_model)
        dec_out = self.positional_encoding(dec_out)
        dec_out = self.dropout_layer(dec_out)

        # è§£ç å™¨å±‚
        dec_attn_weights = []
        for decoder_layer in self.decoder_layers:
            dec_out, attn_weights = decoder_layer(dec_out, enc_out)
            dec_attn_weights.append(attn_weights)

        # ===== è¾“å‡º =====
        output = self.output_layer(dec_out)

        return output


class ISTREncoderLayer(nn.Module):
    """ISTRç¼–ç å™¨å±‚"""

    def __init__(self, d_model: int, n_heads: int, d_ff: int = 2048,
                 dropout: float = 0.1, activation: str = "gelu",
                 sparsity_factor: int = 4):
        super(ISTREncoderLayer, self).__init__()

        self.self_attn = SparseAttention(d_model, n_heads, dropout, sparsity_factor)
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        if activation == "relu":
            self.activation = F.relu
        elif activation == "gelu":
            self.activation = F.gelu
        else:
            self.activation = F.gelu

    def forward(self, src: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # è‡ªæ³¨æ„åŠ›
        src2, attn_weights = self.self_attn(src, src, src)
        src = src + self.dropout1(src2)
        src = self.norm1(src)

        # å‰é¦ˆç½‘ç»œ
        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))
        src = src + self.dropout3(src2)
        src = self.norm2(src)

        return src, attn_weights


class ISTRDecoderLayer(nn.Module):
    """ISTRè§£ç å™¨å±‚"""

    def __init__(self, d_model: int, n_heads: int, d_ff: int = 2048,
                 dropout: float = 0.1, activation: str = "gelu",
                 sparsity_factor: int = 4):
        super(ISTRDecoderLayer, self).__init__()

        self.self_attn = SparseAttention(d_model, n_heads, dropout, sparsity_factor)
        self.cross_attn = SparseAttention(d_model, n_heads, dropout, sparsity_factor)
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)
        self.dropout4 = nn.Dropout(dropout)

        if activation == "relu":
            self.activation = F.relu
        elif activation == "gelu":
            self.activation = F.gelu
        else:
            self.activation = F.gelu

    def forward(self, tgt: torch.Tensor, memory: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """å‰å‘ä¼ æ’­"""
        # è‡ªæ³¨æ„åŠ›
        tgt2, self_attn_weights = self.self_attn(tgt, tgt, tgt)
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)

        # äº¤å‰æ³¨æ„åŠ›
        tgt2, cross_attn_weights = self.cross_attn(tgt, memory, memory)
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)

        # å‰é¦ˆç½‘ç»œ
        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout4(tgt2)
        tgt = self.norm3(tgt)

        # åˆå¹¶æ³¨æ„åŠ›æƒé‡
        attn_weights = {
            'self_attention': self_attn_weights,
            'cross_attention': cross_attn_weights
        }

        return tgt, attn_weights


class SpectralGate(nn.Module):
    """è°±é—¨æ§æ¨¡å— - å¢å¼ºç‰¹å¾é€‰æ‹©èƒ½åŠ›"""

    def __init__(self, channels: int, reduction_ratio: int = 4):
        super().__init__()

        # å…¨å±€å¹³å‡æ± åŒ–è·å–é€šé“ç»Ÿè®¡ä¿¡æ¯
        self.global_pool = nn.AdaptiveAvgPool1d(1)

        # é—¨æ§ç½‘ç»œ
        self.gate_network = nn.Sequential(
            nn.Linear(channels, channels // reduction_ratio, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction_ratio, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, channels, seq_len]
        Returns:
            gated_x: [batch_size, channels, seq_len]
        """
        batch_size, channels, seq_len = x.shape

        # å…¨å±€å¹³å‡æ± åŒ–è·å–é€šé“æƒé‡
        channel_weights = self.global_pool(x).squeeze(-1)  # [batch, channels]

        # è®¡ç®—é—¨æ§æƒé‡
        gate_weights = self.gate_network(channel_weights)  # [batch, channels]

        # é‡å¡‘é—¨æ§æƒé‡ä»¥ä¾¿å¹¿æ’­
        gate_weights = gate_weights.unsqueeze(-1)  # [batch, channels, 1]

        # åº”ç”¨é—¨æ§
        gated_x = x * gate_weights

        return gated_x


class TCNBlock(nn.Module):
    """æ—¶é—´å·ç§¯ç½‘ç»œå— - æ•æ‰å±€éƒ¨æ—¶é—´ä¾èµ–å…³ç³»"""

    def __init__(self, in_channels: int, out_channels: int,
                 kernel_size: int = 3, dilation: int = 1,
                 dropout: float = 0.1):
        super().__init__()

        # è®¡ç®—paddingä»¥ä¿æŒåºåˆ—é•¿åº¦
        padding = (kernel_size - 1) * dilation

        # å› æœå·ç§¯ï¼ˆç¡®ä¿æœªæ¥ä¿¡æ¯ä¸æ³„éœ²ï¼‰
        self.conv1 = nn.Conv1d(
            in_channels, out_channels, kernel_size,
            padding=padding, dilation=dilation
        )
        self.conv2 = nn.Conv1d(
            out_channels, out_channels, kernel_size,
            padding=padding, dilation=dilation
        )

        # æ¿€æ´»å‡½æ•°å’Œå½’ä¸€åŒ–
        self.relu = nn.ReLU()
        self.batchnorm1 = nn.BatchNorm1d(out_channels)
        self.batchnorm2 = nn.BatchNorm1d(out_channels)
        self.dropout = nn.Dropout(dropout)

        # æ®‹å·®è¿æ¥
        if in_channels != out_channels:
            self.residual = nn.Sequential(
                nn.Conv1d(in_channels, out_channels, 1),
                nn.BatchNorm1d(out_channels)
            )
        else:
            self.residual = nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, in_channels, seq_len]
        Returns:
            out: [batch_size, out_channels, seq_len]
        """
        residual = self.residual(x)

        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
        out = self.conv1(x)
        # è£å‰ªpaddingä»¥ä¿è¯å› æœæ€§
        if out.shape[-1] > x.shape[-1]:
            out = out[..., :x.shape[-1]]
        out = self.batchnorm1(out)
        out = self.relu(out)
        out = self.dropout(out)

        # ç¬¬äºŒä¸ªå·ç§¯å±‚
        out = self.conv2(out)
        if out.shape[-1] > x.shape[-1]:
            out = out[..., :x.shape[-1]]
        out = self.batchnorm2(out)

        # æ®‹å·®è¿æ¥ + æ¿€æ´»
        out = out + residual
        out = self.relu(out)

        return out


class LaplacianRegularizer(nn.Module):
    """æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–å™¨ - å¢å¼ºé¢„æµ‹å¹³æ»‘æ€§"""

    def __init__(self, pred_len: int, weight: float = 0.01):
        super().__init__()
        self.pred_len = pred_len
        self.weight = weight

        # æ„å»ºä¸€ç»´é“¾å¼æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
        self.register_buffer('laplacian_matrix', self._build_laplacian_matrix())

    def _build_laplacian_matrix(self) -> torch.Tensor:
        """æ„å»ºæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ L = D - A"""
        L = torch.zeros(self.pred_len, self.pred_len)

        # ä¸»å¯¹è§’çº¿ï¼ˆåº¦çŸ©é˜µï¼‰
        L[0, 0] = 1
        L[-1, -1] = 1
        for i in range(1, self.pred_len - 1):
            L[i, i] = 2

        # é‚»æ¥çŸ©é˜µï¼ˆä¸€ç»´é“¾ï¼‰
        for i in range(self.pred_len - 1):
            L[i, i + 1] = -1
            L[i + 1, i] = -1

        return L

    def forward(self, predictions: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–æŸå¤±

        Args:
            predictions: [batch_size, pred_len, 1] é¢„æµ‹ç»“æœ
        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        # predictions: [batch_size, pred_len, 1]
        pred_flat = predictions.squeeze(-1)  # [batch_size, pred_len]

        # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æŸå¤±ï¼šx^T L x
        # è¿™æƒ©ç½šé¢„æµ‹å€¼ç›¸é‚»ç‚¹ä¹‹é—´çš„å‰§çƒˆå˜åŒ–
        laplacian_loss = torch.mean(
            torch.sum(pred_flat * (pred_flat @ self.laplacian_matrix), dim=-1)
        )

        return laplacian_loss * self.weight


class ISTRPredictor(nn.Module):
    """
    ISTRé¢„æµ‹å™¨ - STAR-Forecastçš„æ ¸å¿ƒç»„ä»¶
    æ•´åˆTCN + è°±é—¨æ§ + æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–
    ä»…è®­ç»ƒ1%å‚æ•°å®ç°SOTAæ•ˆæœ
    """

    def __init__(self,
                 input_dim: int = 7,           # è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆETTh1ä¸º7ï¼‰
                 hidden_dim: int = 64,         # éšè—å±‚ç»´åº¦
                 pred_len: int = 24,           # é¢„æµ‹é•¿åº¦
                 num_blocks: int = 3,          # TCNå—æ•°é‡
                 trainable_ratio: float = 0.01, # å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹
                 laplacian_weight: float = 0.01, # æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–æƒé‡
                 **kwargs):
        super().__init__()

        # ä¿å­˜å‚æ•°
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.pred_len = pred_len
        self.trainable_ratio = trainable_ratio
        self.laplacian_weight = laplacian_weight

        print(f"ğŸ”§ åˆå§‹åŒ–ISTRPredictor: input_dim={input_dim}, hidden_dim={hidden_dim}, "
              f"pred_len={pred_len}, trainable_ratio={trainable_ratio}")

        # ========== TCNç‰¹å¾æå–å™¨ ==========
        self.tcn_blocks = nn.ModuleList()
        self.spectral_gates = nn.ModuleList()

        for i in range(num_blocks):
            # è®¡ç®—è¾“å…¥é€šé“æ•°
            in_channels = input_dim if i == 0 else hidden_dim

            # TCNå—
            tcn_block = TCNBlock(
                in_channels=in_channels,
                out_channels=hidden_dim,
                kernel_size=3,
                dilation=2 ** (i % 3),  # æŒ‡æ•°å¢é•¿çš„è†¨èƒ€ç‡
                dropout=0.1
            )
            self.tcn_blocks.append(tcn_block)

            # è°±é—¨æ§
            spectral_gate = SpectralGate(
                channels=hidden_dim,
                reduction_ratio=4
            )
            self.spectral_gates.append(spectral_gate)

        # ========== ISTR Transformer ==========
        # é…ç½®ISTRæ¨¡å‹ï¼ˆä½¿ç”¨åŸå§‹ISTRæ¶æ„ï¼‰
        self.istr_config = {
            'seq_len': 96,           # å›ºå®šè¾“å…¥åºåˆ—é•¿åº¦
            'pred_len': pred_len,    # é¢„æµ‹é•¿åº¦
            'enc_in':  hidden_dim,     # ç¼–ç å™¨è¾“å…¥ç»´åº¦
            'c_out': 1,             # è¾“å‡ºç»´åº¦ï¼ˆå•å˜é‡é¢„æµ‹ï¼‰
            'd_model': hidden_dim,  # æ¨¡å‹ç»´åº¦
            'n_heads': 4,           # æ³¨æ„åŠ›å¤´æ•°
            'e_layers': 2,          # ç¼–ç å™¨å±‚æ•°
            'd_layers': 1,          # è§£ç å™¨å±‚æ•°
            'd_ff': hidden_dim * 4, # å‰é¦ˆç½‘ç»œç»´åº¦
            'dropout': 0.05,        # Dropoutç‡
            'activation': 'gelu',   # æ¿€æ´»å‡½æ•°
            'sparsity_factor': 2    # ç¨€ç–å› å­
        }

        self.istr_model = ISTRModel(self.istr_config)

        # ========== é¢„æµ‹å¤´ ==========
        self.prediction_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 1)
        )

        # ========== æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–å™¨ ==========
        self.laplacian_regularizer = LaplacianRegularizer(
            pred_len=pred_len,
            weight=laplacian_weight
        )

        # ========== å‚æ•°å†»ç»“ç­–ç•¥ ==========
        self._apply_trainable_ratio(trainable_ratio)

        # ========== æ¨¡å‹ç»Ÿè®¡ ==========
        self._print_model_stats()

    def _apply_trainable_ratio(self, trainable_ratio: float = 0.01):
        """åº”ç”¨å‚æ•°å†»ç»“ç­–ç•¥ï¼Œä»…è®­ç»ƒæŒ‡å®šæ¯”ä¾‹çš„å‚æ•°"""
        # é¦–å…ˆå†»ç»“æ‰€æœ‰å‚æ•°
        for param in self.parameters():
            param.requires_grad = False

        # è®¡ç®—æ€»å‚æ•°
        total_params = sum(p.numel() for p in self.parameters())

        # è§£å†»é¢„æµ‹å¤´çš„å‚æ•°ï¼ˆè¿™éƒ¨åˆ†æ€»æ˜¯å¯è®­ç»ƒï¼‰
        for param in self.prediction_head.parameters():
            param.requires_grad = True

        # è§£å†»æœ€åä¸€ä¸ªTCNå—å’Œè°±é—¨æ§ï¼ˆæ›´å®¹æ˜“é€‚åº”æ–°ä»»åŠ¡ï¼‰
        if len(self.tcn_blocks) > 0:
            for param in self.tcn_blocks[-1].parameters():
                param.requires_grad = True
            if len(self.spectral_gates) > 0:
                for param in self.spectral_gates[-1].parameters():
                    param.requires_grad = True

        # è®¡ç®—å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        # å¦‚æœæ¯”ä¾‹å¤ªä½ï¼Œè§£å†»æ›´å¤šå‚æ•°
        current_ratio = trainable_params / total_params
        if current_ratio < trainable_ratio:
            # è§£å†»ISTRæ¨¡å‹çš„è¾“å‡ºå±‚
            for name, param in self.istr_model.named_parameters():
                if 'output_layer' in name:
                    param.requires_grad = True

        # é‡æ–°è®¡ç®—å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        trainable_ratio_actual = trainable_params / total_params

        self.trainable_params = trainable_params
        self.total_params = total_params

        print(f"ğŸ“Š å‚æ•°é…ç½®: æ€»å…±{total_params:,}å‚æ•°ï¼Œè®­ç»ƒ{trainable_params:,}å‚æ•° "
              f"({trainable_ratio_actual*100:.1f}%)")

    def _print_model_stats(self):
        """æ‰“å°æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“ˆ ISTRæ¨¡å‹ç»Ÿè®¡:")
        print("-" * 40)

        # ç»Ÿè®¡å„æ¨¡å—å‚æ•°é‡
        modules = {
            'TCN Blocks': self.tcn_blocks,
            'Spectral Gates': self.spectral_gates,
            'ISTR Transformer': self.istr_model,
            'Prediction Head': self.prediction_head,
            'Laplacian Regularizer': self.laplacian_regularizer
        }

        for name, module in modules.items():
            total = sum(p.numel() for p in module.parameters())
            trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
            ratio = trainable / total if total > 0 else 0
            print(f"  {name:20s} {total:8,d} total, {trainable:8,d} trainable ({ratio*100:5.1f}%)")

        print("-" * 40)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: [batch_size, seq_len, input_dim] è¾“å…¥åºåˆ—
        Returns:
            predictions: [batch_size, pred_len, 1] é¢„æµ‹ç»“æœ
        """
        batch_size, seq_len, input_dim = x.shape

        # ===== 1. TCN + è°±é—¨æ§ç‰¹å¾æå– =====
        x_tcn = x.transpose(1, 2)  # [batch, input_dim, seq_len]

        for i, (tcn_block, spectral_gate) in enumerate(zip(self.tcn_blocks, self.spectral_gates)):
            x_tcn = tcn_block(x_tcn)
            x_tcn = spectral_gate(x_tcn)

        x_features = x_tcn.transpose(1, 2)  # [batch, seq_len, hidden_dim]

        # ===== 2. ISTR Transformeré¢„æµ‹ =====
        # å‡†å¤‡è§£ç å™¨è¾“å…¥ï¼ˆé›¶åˆå§‹åŒ–ï¼‰
        dec_input = torch.zeros(batch_size, self.pred_len, 1).to(x.device)

        # ISTRé¢„æµ‹
        istr_output = self.istr_model(x_features, dec_input)  # [batch, pred_len, hidden_dim]

        # ===== 3. æœ€ç»ˆé¢„æµ‹å¤´ =====
        predictions = self.prediction_head(istr_output)  # [batch, pred_len, 1]

        return predictions

    def predict(self, x: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹æ–¹æ³•ï¼ˆç”¨äºæ¨ç†ï¼‰

        Args:
            x: [seq_len, input_dim] è¾“å…¥åºåˆ—
        Returns:
            predictions: [pred_len] é¢„æµ‹ç»“æœ
        """
        self.eval()
        with torch.no_grad():
            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            x_tensor = torch.FloatTensor(x).unsqueeze(0)  # [1, seq_len, input_dim]

            # é¢„æµ‹
            pred_tensor = self.forward(x_tensor)  # [1, pred_len, 1]

            # ç§»é™¤batchç»´åº¦å¹¶è½¬æ¢ä¸ºnumpy
            predictions = pred_tensor.squeeze(0).squeeze(-1).numpy()

            return predictions

    def get_confidence_scores(self, x: np.ndarray, n_samples: int = 5) -> np.ndarray:
        """
        è·å–é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆé€šè¿‡MC Dropoutï¼‰

        Args:
            x: [seq_len, input_dim] è¾“å…¥åºåˆ—
            n_samples: é‡‡æ ·æ¬¡æ•°
        Returns:
            confidence: [pred_len] ç½®ä¿¡åº¦åˆ†æ•°
        """
        self.train()  # å¯ç”¨Dropout

        predictions = []
        with torch.no_grad():
            x_tensor = torch.FloatTensor(x).unsqueeze(0)

            # å¤šæ¬¡å‰å‘ä¼ æ’­ï¼ˆMC Dropoutï¼‰
            for _ in range(n_samples):
                pred = self.forward(x_tensor)
                predictions.append(pred)

        # è®¡ç®—ä¸ç¡®å®šæ€§
        predictions = torch.stack(predictions, dim=0)  # [n_samples, 1, pred_len, 1]
        std = predictions.std(dim=0).squeeze().squeeze().numpy()  # [pred_len]

        # æ ‡å‡†å·®è½¬æ¢ä¸ºç½®ä¿¡åº¦ï¼ˆæ ‡å‡†å·®è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜ï¼‰
        confidence = 1.0 / (1.0 + std)

        self.eval()  # æ¢å¤evalæ¨¡å¼
        return confidence

    def compute_regularization_loss(self, predictions: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ­£åˆ™åŒ–æŸå¤±ï¼ˆæ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–ï¼‰

        Args:
            predictions: [batch_size, pred_len, 1] é¢„æµ‹ç»“æœ
        Returns:
            loss: æ ‡é‡æ­£åˆ™åŒ–æŸå¤±
        """
        return self.laplacian_regularizer(predictions)

    def get_trainable_parameter_names(self) -> List[str]:
        """è·å–å¯è®­ç»ƒå‚æ•°åç§°"""
        return [name for name, param in self.named_parameters() if param.requires_grad]

    def freeze_all_parameters(self):
        """å†»ç»“æ‰€æœ‰å‚æ•°"""
        for param in self.parameters():
            param.requires_grad = False

    def unfreeze_specific_layers(self, layer_names: List[str]):
        """è§£å†»ç‰¹å®šå±‚çš„å‚æ•°"""
        for name, param in self.named_parameters():
            if any(layer_name in name for layer_name in layer_names):
                param.requires_grad = True


# ==================== å¯¼å‡ºå®šä¹‰ ====================

__all__ = [
    'ISTRModel',
    'ISTRPredictor',
    'SparseAttention',
    'ISTREncoderLayer',
    'ISTRDecoderLayer',
    'SpectralGate',
    'TCNBlock',
    'LaplacianRegularizer'
]